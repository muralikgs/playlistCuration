{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorboard_logger import configure, log_value\n",
    "import os\n",
    "\n",
    "\n",
    "logdir = 'C:\\\\Users\\\\kkb6\\\\Desktop\\\\PlayListCurator\\\\model-training\\\\logs\\\\'\n",
    "checkpointdir = 'C:\\\\Users\\\\kkb6\\\\Desktop\\\\PlayListCurator\\\\model-training\\\\checkpoints\\\\run-1\\\\'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kkb6\\\\Desktop\\\\PlayListCurator\\\\model-training\\\\logs\\\\'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kkb6\\\\Desktop\\\\PlayListCurator\\\\model-training\\\\checkpoints\\\\run-1\\\\'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load(\"label_array.npy\")\n",
    "x = np.load(\"data_array.npy\")\n",
    "np.random.seed(5)\n",
    "index = np.random.permutation(x.shape[0])\n",
    "x = x[index]\n",
    "y = y[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of tuples\n",
    "X = []\n",
    "for feature, label in zip(x,y):\n",
    "    X.append((feature,label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_iterator(fs, seed=0, batch_size=4, num_fs=0, num_batch=0):\n",
    "\n",
    "    NORMALIZE = True\n",
    "    \n",
    "    fs_a = fs[:num_fs]\n",
    "    fs = fs[num_fs:]\n",
    "    num = batch_size - num_batch\n",
    "    num_itr = int(len(fs)/num)\n",
    "    num_itr_a = int(len(fs_a)/num_batch) if num_batch != 0 else 1\n",
    "    while True:\n",
    "        idxs = list(range(0, len(fs)))\n",
    "        idxs_a = list(range(0, len(fs_a)))\n",
    "        np.random.shuffle(idxs)\n",
    "        np.random.shuffle(idxs_a)\n",
    "\n",
    "        for batch_idx in range(0, num_itr):\n",
    "            current_indices = idxs[batch_idx*num:batch_idx*num + num]\n",
    "            batch_idx = batch_idx % num_itr_a\n",
    "            current_indices_a = idxs_a[batch_idx*num_batch:batch_idx*num_batch + num_batch]\n",
    "\n",
    "            features_batch = []\n",
    "            labels_batch = []\n",
    "            \n",
    "            for j in range(batch_size):\n",
    "                if j < num:\n",
    "                    i = current_indices[j]\n",
    "                    x, label = fs[i]\n",
    "                else:\n",
    "                    i = current_indices_a[j - num]\n",
    "                    x, label = fs_a[i]\n",
    "                                      \n",
    "                #Normalization - Subtrating by the mean and dividing by the standard deviation\n",
    "                if NORMALIZE:\n",
    "\n",
    "                    x = (x - x.mean())/ x.std()\n",
    "\n",
    "                    x = x/ (np.linalg.norm(x))\n",
    "                    \n",
    "                \n",
    "                features_batch.append(x)\n",
    "                labels_batch.append(label)\n",
    "\n",
    "            features_batch = np.array(features_batch)\n",
    "            labels_batch = np.array(labels_batch)\n",
    "\n",
    "            yield features_batch, labels_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlayListCurator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PlayListCurator, self).__init__()\n",
    "        self.fc1 = nn.Linear(445, 100)\n",
    "        #self.fc2 = nn.Linear(100, 84)\n",
    "        self.fc3 = nn.Linear(100, 14)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.sigmoid(self.fc1(x)) #hidden1\n",
    "        #x = F.relu(self.fc2(x)) #hidden2\n",
    "        x = F.log_softmax(self.fc3(x)) #output\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "date, tag = '20190418', 'run-PlayList-Curator'\n",
    "\n",
    "comments = \"\"\"First training\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "length = len(X)\n",
    "\n",
    "\n",
    "train_split = 0.80\n",
    "valid_split = 0.10\n",
    "test_split = 0.10\n",
    "\n",
    "\n",
    "indices1 = round(train_split * length)\n",
    "indices2 = indices1 + round(valid_split*length)\n",
    "\n",
    "train_data = X[:indices1]\n",
    "\n",
    "valid_data = X[indices1:indices2]\n",
    "\n",
    "test_data = X[indices2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_train = data_iterator(train_data,batch_size=batch_size)\n",
    "\n",
    "a_valid = data_iterator(valid_data,batch_size=batch_size)\n",
    "\n",
    "a_test = data_iterator(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.00224745, -0.00224853,  0.99887562, ..., -0.00225143,\n",
       "         -0.00225143, -0.00225143],\n",
       "        [-0.00224423, -0.0022429 ,  0.99887532, ..., -0.00225177,\n",
       "         -0.00225177, -0.00225177],\n",
       "        [-0.00224605, -0.00224579,  0.99887499, ..., -0.00225312,\n",
       "         -0.00225312, -0.00225312],\n",
       "        [-0.00225221, -0.00224832,  0.99887536, ..., -0.00225237,\n",
       "         -0.00225237, -0.00225237]]), array([7., 9., 9., 5.]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_train.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = PlayListCurator()\n",
    "net = net.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = net\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), weight_decay=8e-6, lr=0.0001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "i = 0\n",
    "best_valid_loss = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "configure(logdir + '\\\\run-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkb6\\AppData\\Local\\Continuum\\anaconda3\\envs\\texture\\lib\\site-packages\\torch\\nn\\functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\kkb6\\AppData\\Local\\Continuum\\anaconda3\\envs\\texture\\lib\\site-packages\\ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2, Loss:2.77 \n",
      "Iteration: 3, Loss:2.52 \n",
      "Iteration: 4, Loss:2.73 \n",
      "Iteration: 5, Loss:2.68 \n",
      "Iteration: 6, Loss:2.54 \n",
      "Iteration: 7, Loss:2.63 \n",
      "Iteration: 8, Loss:2.73 \n",
      "Iteration: 9, Loss:2.91 \n",
      "Iteration: 10, Loss:2.81 \n",
      "Iteration: 11, Loss:2.98 \n",
      "Iteration: 12, Loss:2.57 \n",
      "Iteration: 13, Loss:2.63 \n",
      "Iteration: 14, Loss:2.72 \n",
      "Iteration: 15, Loss:2.72 \n",
      "Iteration: 16, Loss:2.31 \n",
      "Iteration: 17, Loss:2.80 \n",
      "Iteration: 18, Loss:2.94 \n",
      "Iteration: 19, Loss:2.57 \n",
      "Iteration: 20, Loss:2.80 \n",
      "Iteration: 21, Loss:2.83 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_20.ckpt\n",
      "Iteration: 22, Loss:2.78 \n",
      "Iteration: 23, Loss:2.68 \n",
      "Iteration: 24, Loss:2.78 \n",
      "Iteration: 25, Loss:2.93 \n",
      "Iteration: 26, Loss:2.70 \n",
      "Iteration: 27, Loss:2.50 \n",
      "Iteration: 28, Loss:2.38 \n",
      "Iteration: 29, Loss:2.74 \n",
      "Iteration: 30, Loss:2.79 \n",
      "Iteration: 31, Loss:2.61 \n",
      "Iteration: 32, Loss:2.67 \n",
      "Iteration: 33, Loss:3.00 \n",
      "Iteration: 34, Loss:2.52 \n",
      "Iteration: 35, Loss:2.54 \n",
      "Iteration: 36, Loss:2.63 \n",
      "Iteration: 37, Loss:2.81 \n",
      "Iteration: 38, Loss:2.52 \n",
      "Iteration: 39, Loss:2.77 \n",
      "Iteration: 40, Loss:2.49 \n",
      "Iteration: 41, Loss:2.58 \n",
      "Iteration: 42, Loss:2.51 \n",
      "Iteration: 43, Loss:2.52 \n",
      "Iteration: 44, Loss:2.53 \n",
      "Iteration: 45, Loss:2.73 \n",
      "Iteration: 46, Loss:2.61 \n",
      "Iteration: 47, Loss:2.67 \n",
      "Iteration: 48, Loss:2.40 \n",
      "Iteration: 49, Loss:2.61 \n",
      "Iteration: 50, Loss:2.60 \n",
      "Iteration: 51, Loss:2.80 \n",
      "Iteration: 52, Loss:2.59 \n",
      "Iteration: 53, Loss:2.60 \n",
      "Iteration: 54, Loss:2.64 \n",
      "Iteration: 55, Loss:2.71 \n",
      "Iteration: 56, Loss:2.75 \n",
      "Iteration: 57, Loss:2.88 \n",
      "Iteration: 58, Loss:2.62 \n",
      "Iteration: 59, Loss:2.47 \n",
      "Iteration: 60, Loss:2.57 \n",
      "Iteration: 61, Loss:2.58 \n",
      "Iteration: 62, Loss:2.52 \n",
      "Iteration: 63, Loss:2.58 \n",
      "Iteration: 64, Loss:2.57 \n",
      "Iteration: 65, Loss:2.65 \n",
      "Iteration: 66, Loss:2.79 \n",
      "Iteration: 67, Loss:2.78 \n",
      "Iteration: 68, Loss:2.68 \n",
      "Iteration: 69, Loss:2.57 \n",
      "Iteration: 70, Loss:2.61 \n",
      "Iteration: 71, Loss:2.51 \n",
      "Iteration: 72, Loss:2.58 \n",
      "Iteration: 73, Loss:2.76 \n",
      "Iteration: 74, Loss:2.46 \n",
      "Iteration: 75, Loss:2.83 \n",
      "Iteration: 76, Loss:2.68 \n",
      "Iteration: 77, Loss:2.55 \n",
      "Iteration: 78, Loss:2.52 \n",
      "Iteration: 79, Loss:2.70 \n",
      "Iteration: 80, Loss:2.54 \n",
      "Iteration: 81, Loss:2.59 \n",
      "Iteration: 82, Loss:2.58 \n",
      "Iteration: 83, Loss:2.60 \n",
      "Iteration: 84, Loss:2.47 \n",
      "Iteration: 85, Loss:2.85 \n",
      "Iteration: 86, Loss:2.57 \n",
      "Iteration: 87, Loss:2.69 \n",
      "Iteration: 88, Loss:2.86 \n",
      "Iteration: 89, Loss:2.51 \n",
      "Iteration: 90, Loss:2.60 \n",
      "Iteration: 91, Loss:2.47 \n",
      "Iteration: 92, Loss:2.71 \n",
      "Iteration: 93, Loss:2.63 \n",
      "Iteration: 94, Loss:2.50 \n",
      "Iteration: 95, Loss:2.50 \n",
      "Iteration: 96, Loss:3.02 \n",
      "Iteration: 97, Loss:2.63 \n",
      "Iteration: 98, Loss:2.55 \n",
      "Iteration: 99, Loss:2.58 \n",
      "Iteration: 100, Loss:2.57 \n",
      "Iteration: 101, Loss:2.77 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_100.ckpt\n",
      "Iteration: 102, Loss:2.54 \n",
      "Iteration: 103, Loss:2.56 \n",
      "Iteration: 104, Loss:2.73 \n",
      "Iteration: 105, Loss:2.65 \n",
      "Iteration: 106, Loss:2.74 \n",
      "Iteration: 107, Loss:2.89 \n",
      "Iteration: 108, Loss:2.75 \n",
      "Iteration: 109, Loss:2.63 \n",
      "Iteration: 110, Loss:2.45 \n",
      "Iteration: 111, Loss:2.60 \n",
      "Iteration: 112, Loss:2.44 \n",
      "Iteration: 113, Loss:2.78 \n",
      "Iteration: 114, Loss:2.54 \n",
      "Iteration: 115, Loss:2.60 \n",
      "Iteration: 116, Loss:2.75 \n",
      "Iteration: 117, Loss:2.89 \n",
      "Iteration: 118, Loss:2.63 \n",
      "Iteration: 119, Loss:2.68 \n",
      "Iteration: 120, Loss:2.42 \n",
      "Iteration: 121, Loss:2.43 \n",
      "Iteration: 122, Loss:2.55 \n",
      "Iteration: 123, Loss:2.62 \n",
      "Iteration: 124, Loss:2.69 \n",
      "Iteration: 125, Loss:2.47 \n",
      "Iteration: 126, Loss:2.86 \n",
      "Iteration: 127, Loss:2.75 \n",
      "Iteration: 128, Loss:2.63 \n",
      "Iteration: 129, Loss:2.59 \n",
      "Iteration: 130, Loss:2.71 \n",
      "Iteration: 131, Loss:2.73 \n",
      "Iteration: 132, Loss:2.72 \n",
      "Iteration: 133, Loss:2.64 \n",
      "Iteration: 134, Loss:2.53 \n",
      "Iteration: 135, Loss:2.42 \n",
      "Iteration: 136, Loss:2.46 \n",
      "Iteration: 137, Loss:2.40 \n",
      "Iteration: 138, Loss:2.38 \n",
      "Iteration: 139, Loss:2.60 \n",
      "Iteration: 140, Loss:2.74 \n",
      "Iteration: 141, Loss:2.70 \n",
      "Iteration: 142, Loss:2.51 \n",
      "Iteration: 143, Loss:2.51 \n",
      "Iteration: 144, Loss:2.39 \n",
      "Iteration: 145, Loss:2.63 \n",
      "Iteration: 146, Loss:2.55 \n",
      "Iteration: 147, Loss:2.51 \n",
      "Iteration: 148, Loss:2.69 \n",
      "Iteration: 149, Loss:2.61 \n",
      "Iteration: 150, Loss:2.56 \n",
      "Iteration: 151, Loss:2.60 \n",
      "Iteration: 152, Loss:2.63 \n",
      "Iteration: 153, Loss:2.66 \n",
      "Iteration: 154, Loss:2.64 \n",
      "Iteration: 155, Loss:2.80 \n",
      "Iteration: 156, Loss:2.54 \n",
      "Iteration: 157, Loss:2.60 \n",
      "Iteration: 158, Loss:2.64 \n",
      "Iteration: 159, Loss:2.52 \n",
      "Iteration: 160, Loss:2.76 \n",
      "Iteration: 161, Loss:2.46 \n",
      "Iteration: 162, Loss:2.51 \n",
      "Iteration: 163, Loss:2.47 \n",
      "Iteration: 164, Loss:2.75 \n",
      "Iteration: 165, Loss:2.74 \n",
      "Iteration: 166, Loss:2.75 \n",
      "Iteration: 167, Loss:2.61 \n",
      "Iteration: 168, Loss:2.89 \n",
      "Iteration: 169, Loss:2.50 \n",
      "Iteration: 170, Loss:2.73 \n",
      "Iteration: 171, Loss:2.38 \n",
      "Iteration: 172, Loss:2.87 \n",
      "Iteration: 173, Loss:2.77 \n",
      "Iteration: 174, Loss:2.80 \n",
      "Iteration: 175, Loss:2.58 \n",
      "Iteration: 176, Loss:2.48 \n",
      "Iteration: 177, Loss:2.44 \n",
      "Iteration: 178, Loss:2.51 \n",
      "Iteration: 179, Loss:2.71 \n",
      "Iteration: 180, Loss:2.71 \n",
      "Iteration: 181, Loss:2.80 \n",
      "Iteration: 182, Loss:2.59 \n",
      "Iteration: 183, Loss:2.64 \n",
      "Iteration: 184, Loss:2.62 \n",
      "Iteration: 185, Loss:2.70 \n",
      "Iteration: 186, Loss:2.63 \n",
      "Iteration: 187, Loss:2.72 \n",
      "Iteration: 188, Loss:2.41 \n",
      "Iteration: 189, Loss:2.68 \n",
      "Iteration: 190, Loss:2.73 \n",
      "Iteration: 191, Loss:2.58 \n",
      "Iteration: 192, Loss:2.50 \n",
      "Iteration: 193, Loss:2.64 \n",
      "Iteration: 194, Loss:2.54 \n",
      "Iteration: 195, Loss:2.64 \n",
      "Iteration: 196, Loss:2.53 \n",
      "Iteration: 197, Loss:2.48 \n",
      "Iteration: 198, Loss:2.75 \n",
      "Iteration: 199, Loss:2.52 \n",
      "Iteration: 200, Loss:2.55 \n",
      "Iteration: 201, Loss:2.59 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_200.ckpt\n",
      "Iteration: 202, Loss:2.45 \n",
      "Iteration: 203, Loss:2.51 \n",
      "Iteration: 204, Loss:2.41 \n",
      "Iteration: 205, Loss:2.50 \n",
      "Iteration: 206, Loss:2.59 \n",
      "Iteration: 207, Loss:2.64 \n",
      "Iteration: 208, Loss:2.52 \n",
      "Iteration: 209, Loss:2.74 \n",
      "Iteration: 210, Loss:2.60 \n",
      "Iteration: 211, Loss:2.67 \n",
      "Iteration: 212, Loss:2.44 \n",
      "Iteration: 213, Loss:2.63 \n",
      "Iteration: 214, Loss:2.42 \n",
      "Iteration: 215, Loss:2.57 \n",
      "Iteration: 216, Loss:2.65 \n",
      "Iteration: 217, Loss:2.50 \n",
      "Iteration: 218, Loss:2.59 \n",
      "Iteration: 219, Loss:2.42 \n",
      "Iteration: 220, Loss:2.70 \n",
      "Iteration: 221, Loss:2.66 \n",
      "Iteration: 222, Loss:2.49 \n",
      "Iteration: 223, Loss:2.76 \n",
      "Iteration: 224, Loss:2.58 \n",
      "Iteration: 225, Loss:2.55 \n",
      "Iteration: 226, Loss:2.63 \n",
      "Iteration: 227, Loss:2.78 \n",
      "Iteration: 228, Loss:2.73 \n",
      "Iteration: 229, Loss:2.55 \n",
      "Iteration: 230, Loss:2.58 \n",
      "Iteration: 231, Loss:2.57 \n",
      "Iteration: 232, Loss:2.60 \n",
      "Iteration: 233, Loss:2.79 \n",
      "Iteration: 234, Loss:2.52 \n",
      "Iteration: 235, Loss:2.47 \n",
      "Iteration: 236, Loss:2.65 \n",
      "Iteration: 237, Loss:2.65 \n",
      "Iteration: 238, Loss:2.41 \n",
      "Iteration: 239, Loss:2.69 \n",
      "Iteration: 240, Loss:2.75 \n",
      "Iteration: 241, Loss:2.49 \n",
      "Iteration: 242, Loss:2.48 \n",
      "Iteration: 243, Loss:2.67 \n",
      "Iteration: 244, Loss:2.60 \n",
      "Iteration: 245, Loss:2.52 \n",
      "Iteration: 246, Loss:2.63 \n",
      "Iteration: 247, Loss:2.71 \n",
      "Iteration: 248, Loss:2.45 \n",
      "Iteration: 249, Loss:2.66 \n",
      "Iteration: 250, Loss:2.34 \n",
      "Iteration: 251, Loss:2.57 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_250.ckpt\n",
      "Iteration: 252, Loss:2.61 \n",
      "Iteration: 253, Loss:2.38 \n",
      "Iteration: 254, Loss:2.60 \n",
      "Iteration: 255, Loss:2.71 \n",
      "Iteration: 256, Loss:2.55 \n",
      "Iteration: 257, Loss:2.45 \n",
      "Iteration: 258, Loss:2.37 \n",
      "Iteration: 259, Loss:2.76 \n",
      "Iteration: 260, Loss:2.77 \n",
      "Iteration: 261, Loss:2.69 \n",
      "Iteration: 262, Loss:2.38 \n",
      "Iteration: 263, Loss:2.59 \n",
      "Iteration: 264, Loss:2.44 \n",
      "Iteration: 265, Loss:2.58 \n",
      "Iteration: 266, Loss:2.75 \n",
      "Iteration: 267, Loss:2.61 \n",
      "Iteration: 268, Loss:2.57 \n",
      "Iteration: 269, Loss:2.78 \n",
      "Iteration: 270, Loss:2.75 \n",
      "Iteration: 271, Loss:2.68 \n",
      "Iteration: 272, Loss:2.36 \n",
      "Iteration: 273, Loss:2.62 \n",
      "Iteration: 274, Loss:2.38 \n",
      "Iteration: 275, Loss:2.62 \n",
      "Iteration: 276, Loss:2.66 \n",
      "Iteration: 277, Loss:2.76 \n",
      "Iteration: 278, Loss:2.67 \n",
      "Iteration: 279, Loss:2.55 \n",
      "Iteration: 280, Loss:2.69 \n",
      "Iteration: 281, Loss:2.57 \n",
      "Iteration: 282, Loss:2.47 \n",
      "Iteration: 283, Loss:2.43 \n",
      "Iteration: 284, Loss:2.47 \n",
      "Iteration: 285, Loss:2.47 \n",
      "Iteration: 286, Loss:2.58 \n",
      "Iteration: 287, Loss:2.73 \n",
      "Iteration: 288, Loss:2.44 \n",
      "Iteration: 289, Loss:2.47 \n",
      "Iteration: 290, Loss:2.61 \n",
      "Iteration: 291, Loss:2.41 \n",
      "Iteration: 292, Loss:2.54 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 293, Loss:2.46 \n",
      "Iteration: 294, Loss:2.52 \n",
      "Iteration: 295, Loss:2.71 \n",
      "Iteration: 296, Loss:2.62 \n",
      "Iteration: 297, Loss:2.57 \n",
      "Iteration: 298, Loss:2.74 \n",
      "Iteration: 299, Loss:2.42 \n",
      "Iteration: 300, Loss:2.52 \n",
      "Iteration: 301, Loss:2.77 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_300.ckpt\n",
      "Iteration: 302, Loss:2.63 \n",
      "Iteration: 303, Loss:2.63 \n",
      "Iteration: 304, Loss:2.80 \n",
      "Iteration: 305, Loss:2.64 \n",
      "Iteration: 306, Loss:2.68 \n",
      "Iteration: 307, Loss:2.57 \n",
      "Iteration: 308, Loss:2.68 \n",
      "Iteration: 309, Loss:2.39 \n",
      "Iteration: 310, Loss:2.30 \n",
      "Iteration: 311, Loss:2.19 \n",
      "Iteration: 312, Loss:2.64 \n",
      "Iteration: 313, Loss:2.67 \n",
      "Iteration: 314, Loss:2.64 \n",
      "Iteration: 315, Loss:2.76 \n",
      "Iteration: 316, Loss:2.22 \n",
      "Iteration: 317, Loss:2.56 \n",
      "Iteration: 318, Loss:2.68 \n",
      "Iteration: 319, Loss:2.47 \n",
      "Iteration: 320, Loss:2.24 \n",
      "Iteration: 321, Loss:2.59 \n",
      "Iteration: 322, Loss:2.52 \n",
      "Iteration: 323, Loss:2.51 \n",
      "Iteration: 324, Loss:2.67 \n",
      "Iteration: 325, Loss:2.33 \n",
      "Iteration: 326, Loss:2.67 \n",
      "Iteration: 327, Loss:2.64 \n",
      "Iteration: 328, Loss:2.52 \n",
      "Iteration: 329, Loss:2.69 \n",
      "Iteration: 330, Loss:2.82 \n",
      "Iteration: 331, Loss:2.59 \n",
      "Iteration: 332, Loss:2.64 \n",
      "Iteration: 333, Loss:2.46 \n",
      "Iteration: 334, Loss:2.50 \n",
      "Iteration: 335, Loss:2.71 \n",
      "Iteration: 336, Loss:2.66 \n",
      "Iteration: 337, Loss:2.52 \n",
      "Iteration: 338, Loss:2.20 \n",
      "Iteration: 339, Loss:2.35 \n",
      "Iteration: 340, Loss:2.54 \n",
      "Iteration: 341, Loss:2.59 \n",
      "Iteration: 342, Loss:2.53 \n",
      "Iteration: 343, Loss:2.68 \n",
      "Iteration: 344, Loss:2.53 \n",
      "Iteration: 345, Loss:2.51 \n",
      "Iteration: 346, Loss:2.45 \n",
      "Iteration: 347, Loss:2.26 \n",
      "Iteration: 348, Loss:2.71 \n",
      "Iteration: 349, Loss:2.66 \n",
      "Iteration: 350, Loss:2.28 \n",
      "Iteration: 351, Loss:2.54 \n",
      "Iteration: 352, Loss:2.36 \n",
      "Iteration: 353, Loss:2.78 \n",
      "Iteration: 354, Loss:2.56 \n",
      "Iteration: 355, Loss:2.68 \n",
      "Iteration: 356, Loss:2.30 \n",
      "Iteration: 357, Loss:2.38 \n",
      "Iteration: 358, Loss:2.75 \n",
      "Iteration: 359, Loss:2.41 \n",
      "Iteration: 360, Loss:2.94 \n",
      "Iteration: 361, Loss:2.71 \n",
      "Iteration: 362, Loss:2.71 \n",
      "Iteration: 363, Loss:2.46 \n",
      "Iteration: 364, Loss:2.26 \n",
      "Iteration: 365, Loss:2.66 \n",
      "Iteration: 366, Loss:2.40 \n",
      "Iteration: 367, Loss:2.49 \n",
      "Iteration: 368, Loss:2.25 \n",
      "Iteration: 369, Loss:2.25 \n",
      "Iteration: 370, Loss:2.63 \n",
      "Iteration: 371, Loss:2.59 \n",
      "Iteration: 372, Loss:2.93 \n",
      "Iteration: 373, Loss:2.34 \n",
      "Iteration: 374, Loss:2.70 \n",
      "Iteration: 375, Loss:2.60 \n",
      "Iteration: 376, Loss:2.80 \n",
      "Iteration: 377, Loss:2.65 \n",
      "Iteration: 378, Loss:2.74 \n",
      "Iteration: 379, Loss:2.58 \n",
      "Iteration: 380, Loss:2.66 \n",
      "Iteration: 381, Loss:2.26 \n",
      "Iteration: 382, Loss:2.53 \n",
      "Iteration: 383, Loss:2.87 \n",
      "Iteration: 384, Loss:2.45 \n",
      "Iteration: 385, Loss:2.30 \n",
      "Iteration: 386, Loss:2.61 \n",
      "Iteration: 387, Loss:2.48 \n",
      "Iteration: 388, Loss:2.64 \n",
      "Iteration: 389, Loss:2.63 \n",
      "Iteration: 390, Loss:2.33 \n",
      "Iteration: 391, Loss:2.76 \n",
      "Iteration: 392, Loss:2.42 \n",
      "Iteration: 393, Loss:2.30 \n",
      "Iteration: 394, Loss:2.05 \n",
      "Iteration: 395, Loss:2.55 \n",
      "Iteration: 396, Loss:2.41 \n",
      "Iteration: 397, Loss:2.74 \n",
      "Iteration: 398, Loss:2.44 \n",
      "Iteration: 399, Loss:2.56 \n",
      "Iteration: 400, Loss:2.86 \n",
      "Iteration: 401, Loss:2.57 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_400.ckpt\n",
      "Iteration: 402, Loss:2.49 \n",
      "Iteration: 403, Loss:2.38 \n",
      "Iteration: 404, Loss:2.85 \n",
      "Iteration: 405, Loss:2.67 \n",
      "Iteration: 406, Loss:2.29 \n",
      "Iteration: 407, Loss:2.45 \n",
      "Iteration: 408, Loss:2.62 \n",
      "Iteration: 409, Loss:2.70 \n",
      "Iteration: 410, Loss:2.45 \n",
      "Iteration: 411, Loss:2.73 \n",
      "Iteration: 412, Loss:2.48 \n",
      "Iteration: 413, Loss:2.65 \n",
      "Iteration: 414, Loss:2.43 \n",
      "Iteration: 415, Loss:2.86 \n",
      "Iteration: 416, Loss:2.58 \n",
      "Iteration: 417, Loss:2.47 \n",
      "Iteration: 418, Loss:2.84 \n",
      "Iteration: 419, Loss:2.70 \n",
      "Iteration: 420, Loss:2.41 \n",
      "Iteration: 421, Loss:2.43 \n",
      "Iteration: 422, Loss:2.72 \n",
      "Iteration: 423, Loss:2.64 \n",
      "Iteration: 424, Loss:2.37 \n",
      "Iteration: 425, Loss:2.50 \n",
      "Iteration: 426, Loss:2.90 \n",
      "Iteration: 427, Loss:2.89 \n",
      "Iteration: 428, Loss:2.43 \n",
      "Iteration: 429, Loss:2.32 \n",
      "Iteration: 430, Loss:2.61 \n",
      "Iteration: 431, Loss:2.77 \n",
      "Iteration: 432, Loss:2.61 \n",
      "Iteration: 433, Loss:2.59 \n",
      "Iteration: 434, Loss:2.04 \n",
      "Iteration: 435, Loss:2.55 \n",
      "Iteration: 436, Loss:2.69 \n",
      "Iteration: 437, Loss:2.37 \n",
      "Iteration: 438, Loss:2.55 \n",
      "Iteration: 439, Loss:2.47 \n",
      "Iteration: 440, Loss:2.69 \n",
      "Iteration: 441, Loss:2.64 \n",
      "Iteration: 442, Loss:2.71 \n",
      "Iteration: 443, Loss:2.71 \n",
      "Iteration: 444, Loss:2.68 \n",
      "Iteration: 445, Loss:2.68 \n",
      "Iteration: 446, Loss:2.62 \n",
      "Iteration: 447, Loss:2.83 \n",
      "Iteration: 448, Loss:2.33 \n",
      "Iteration: 449, Loss:2.50 \n",
      "Iteration: 450, Loss:2.47 \n",
      "Iteration: 451, Loss:2.69 \n",
      "Iteration: 452, Loss:2.38 \n",
      "Iteration: 453, Loss:2.69 \n",
      "Iteration: 454, Loss:2.43 \n",
      "Iteration: 455, Loss:2.22 \n",
      "Iteration: 456, Loss:2.44 \n",
      "Iteration: 457, Loss:2.44 \n",
      "Iteration: 458, Loss:2.52 \n",
      "Iteration: 459, Loss:2.51 \n",
      "Iteration: 460, Loss:2.35 \n",
      "Iteration: 461, Loss:2.49 \n",
      "Iteration: 462, Loss:2.33 \n",
      "Iteration: 463, Loss:2.28 \n",
      "Iteration: 464, Loss:2.53 \n",
      "Iteration: 465, Loss:2.62 \n",
      "Iteration: 466, Loss:2.50 \n",
      "Iteration: 467, Loss:2.64 \n",
      "Iteration: 468, Loss:2.69 \n",
      "Iteration: 469, Loss:2.67 \n",
      "Iteration: 470, Loss:2.62 \n",
      "Iteration: 471, Loss:2.48 \n",
      "Iteration: 472, Loss:2.38 \n",
      "Iteration: 473, Loss:2.76 \n",
      "Iteration: 474, Loss:2.87 \n",
      "Iteration: 475, Loss:2.25 \n",
      "Iteration: 476, Loss:2.64 \n",
      "Iteration: 477, Loss:2.40 \n",
      "Iteration: 478, Loss:2.85 \n",
      "Iteration: 479, Loss:2.19 \n",
      "Iteration: 480, Loss:2.79 \n",
      "Iteration: 481, Loss:2.64 \n",
      "Iteration: 482, Loss:2.62 \n",
      "Iteration: 483, Loss:2.81 \n",
      "Iteration: 484, Loss:3.01 \n",
      "Iteration: 485, Loss:2.68 \n",
      "Iteration: 486, Loss:2.54 \n",
      "Iteration: 487, Loss:2.27 \n",
      "Iteration: 488, Loss:2.41 \n",
      "Iteration: 489, Loss:2.42 \n",
      "Iteration: 490, Loss:2.40 \n",
      "Iteration: 491, Loss:2.62 \n",
      "Iteration: 492, Loss:2.51 \n",
      "Iteration: 493, Loss:2.38 \n",
      "Iteration: 494, Loss:2.56 \n",
      "Iteration: 495, Loss:2.42 \n",
      "Iteration: 496, Loss:2.64 \n",
      "Iteration: 497, Loss:2.81 \n",
      "Iteration: 498, Loss:2.62 \n",
      "Iteration: 499, Loss:2.12 \n",
      "Iteration: 500, Loss:2.82 \n",
      "Iteration: 501, Loss:2.52 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_500.ckpt\n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_500.ckpt\n",
      "Iteration: 502, Loss:2.66 \n",
      "Iteration: 503, Loss:2.10 \n",
      "Iteration: 504, Loss:2.69 \n",
      "Iteration: 505, Loss:2.48 \n",
      "Iteration: 506, Loss:2.43 \n",
      "Iteration: 507, Loss:2.66 \n",
      "Iteration: 508, Loss:2.80 \n",
      "Iteration: 509, Loss:2.49 \n",
      "Iteration: 510, Loss:2.53 \n",
      "Iteration: 511, Loss:2.73 \n",
      "Iteration: 512, Loss:2.60 \n",
      "Iteration: 513, Loss:2.79 \n",
      "Iteration: 514, Loss:2.47 \n",
      "Iteration: 515, Loss:2.57 \n",
      "Iteration: 516, Loss:2.61 \n",
      "Iteration: 517, Loss:2.49 \n",
      "Iteration: 518, Loss:2.76 \n",
      "Iteration: 519, Loss:2.24 \n",
      "Iteration: 520, Loss:2.29 \n",
      "Iteration: 521, Loss:2.69 \n",
      "Iteration: 522, Loss:2.16 \n",
      "Iteration: 523, Loss:2.90 \n",
      "Iteration: 524, Loss:2.40 \n",
      "Iteration: 525, Loss:2.49 \n",
      "Iteration: 526, Loss:2.44 \n",
      "Iteration: 527, Loss:2.66 \n",
      "Iteration: 528, Loss:2.52 \n",
      "Iteration: 529, Loss:2.76 \n",
      "Iteration: 530, Loss:2.58 \n",
      "Iteration: 531, Loss:2.10 \n",
      "Iteration: 532, Loss:2.24 \n",
      "Iteration: 533, Loss:2.85 \n",
      "Iteration: 534, Loss:2.80 \n",
      "Iteration: 535, Loss:2.93 \n",
      "Iteration: 536, Loss:2.66 \n",
      "Iteration: 537, Loss:2.48 \n",
      "Iteration: 538, Loss:2.22 \n",
      "Iteration: 539, Loss:2.35 \n",
      "Iteration: 540, Loss:2.41 \n",
      "Iteration: 541, Loss:2.55 \n",
      "Iteration: 542, Loss:2.62 \n",
      "Iteration: 543, Loss:2.76 \n",
      "Iteration: 544, Loss:2.60 \n",
      "Iteration: 545, Loss:2.39 \n",
      "Iteration: 546, Loss:2.54 \n",
      "Iteration: 547, Loss:2.48 \n",
      "Iteration: 548, Loss:2.64 \n",
      "Iteration: 549, Loss:2.60 \n",
      "Iteration: 550, Loss:2.67 \n",
      "Iteration: 551, Loss:2.41 \n",
      "Iteration: 552, Loss:2.44 \n",
      "Iteration: 553, Loss:2.41 \n",
      "Iteration: 554, Loss:2.16 \n",
      "Iteration: 555, Loss:2.43 \n",
      "Iteration: 556, Loss:2.18 \n",
      "Iteration: 557, Loss:2.57 \n",
      "Iteration: 558, Loss:2.73 \n",
      "Iteration: 559, Loss:2.31 \n",
      "Iteration: 560, Loss:2.46 \n",
      "Iteration: 561, Loss:2.36 \n",
      "Iteration: 562, Loss:2.28 \n",
      "Iteration: 563, Loss:2.67 \n",
      "Iteration: 564, Loss:2.54 \n",
      "Iteration: 565, Loss:2.50 \n",
      "Iteration: 566, Loss:2.85 \n",
      "Iteration: 567, Loss:2.79 \n",
      "Iteration: 568, Loss:2.86 \n",
      "Iteration: 569, Loss:2.43 \n",
      "Iteration: 570, Loss:2.53 \n",
      "Iteration: 571, Loss:2.48 \n",
      "Iteration: 572, Loss:2.79 \n",
      "Iteration: 573, Loss:2.47 \n",
      "Iteration: 574, Loss:2.55 \n",
      "Iteration: 575, Loss:2.40 \n",
      "Iteration: 576, Loss:2.48 \n",
      "Iteration: 577, Loss:2.68 \n",
      "Iteration: 578, Loss:2.63 \n",
      "Iteration: 579, Loss:2.85 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 580, Loss:2.67 \n",
      "Iteration: 581, Loss:2.69 \n",
      "Iteration: 582, Loss:2.70 \n",
      "Iteration: 583, Loss:2.37 \n",
      "Iteration: 584, Loss:2.73 \n",
      "Iteration: 585, Loss:2.50 \n",
      "Iteration: 586, Loss:2.30 \n",
      "Iteration: 587, Loss:2.77 \n",
      "Iteration: 588, Loss:2.71 \n",
      "Iteration: 589, Loss:2.50 \n",
      "Iteration: 590, Loss:2.35 \n",
      "Iteration: 591, Loss:2.67 \n",
      "Iteration: 592, Loss:2.54 \n",
      "Iteration: 593, Loss:2.45 \n",
      "Iteration: 594, Loss:2.59 \n",
      "Iteration: 595, Loss:2.32 \n",
      "Iteration: 596, Loss:2.21 \n",
      "Iteration: 597, Loss:2.82 \n",
      "Iteration: 598, Loss:2.60 \n",
      "Iteration: 599, Loss:2.60 \n",
      "Iteration: 600, Loss:2.48 \n",
      "Iteration: 601, Loss:2.55 \n",
      "Iteration: 602, Loss:2.65 \n",
      "Iteration: 603, Loss:2.34 \n",
      "Iteration: 604, Loss:2.63 \n",
      "Iteration: 605, Loss:2.66 \n",
      "Iteration: 606, Loss:2.77 \n",
      "Iteration: 607, Loss:2.83 \n",
      "Iteration: 608, Loss:2.64 \n",
      "Iteration: 609, Loss:2.49 \n",
      "Iteration: 610, Loss:2.47 \n",
      "Iteration: 611, Loss:2.81 \n",
      "Iteration: 612, Loss:2.61 \n",
      "Iteration: 613, Loss:2.79 \n",
      "Iteration: 614, Loss:2.47 \n",
      "Iteration: 615, Loss:2.51 \n",
      "Iteration: 616, Loss:2.48 \n",
      "Iteration: 617, Loss:2.65 \n",
      "Iteration: 618, Loss:2.88 \n",
      "Iteration: 619, Loss:2.72 \n",
      "Iteration: 620, Loss:2.26 \n",
      "Iteration: 621, Loss:2.92 \n",
      "Iteration: 622, Loss:2.36 \n",
      "Iteration: 623, Loss:2.44 \n",
      "Iteration: 624, Loss:2.58 \n",
      "Iteration: 625, Loss:2.18 \n",
      "Iteration: 626, Loss:2.67 \n",
      "Iteration: 627, Loss:2.74 \n",
      "Iteration: 628, Loss:2.21 \n",
      "Iteration: 629, Loss:2.53 \n",
      "Iteration: 630, Loss:2.20 \n",
      "Iteration: 631, Loss:2.29 \n",
      "Iteration: 632, Loss:2.47 \n",
      "Iteration: 633, Loss:2.45 \n",
      "Iteration: 634, Loss:2.49 \n",
      "Iteration: 635, Loss:2.53 \n",
      "Iteration: 636, Loss:2.52 \n",
      "Iteration: 637, Loss:2.39 \n",
      "Iteration: 638, Loss:2.57 \n",
      "Iteration: 639, Loss:2.54 \n",
      "Iteration: 640, Loss:2.42 \n",
      "Iteration: 641, Loss:2.89 \n",
      "Iteration: 642, Loss:2.38 \n",
      "Iteration: 643, Loss:2.47 \n",
      "Iteration: 644, Loss:2.65 \n",
      "Iteration: 645, Loss:2.46 \n",
      "Iteration: 646, Loss:2.45 \n",
      "Iteration: 647, Loss:2.64 \n",
      "Iteration: 648, Loss:2.42 \n",
      "Iteration: 649, Loss:2.23 \n",
      "Iteration: 650, Loss:2.39 \n",
      "Iteration: 651, Loss:2.77 \n",
      "Iteration: 652, Loss:2.30 \n",
      "Iteration: 653, Loss:2.58 \n",
      "Iteration: 654, Loss:2.40 \n",
      "Iteration: 655, Loss:2.64 \n",
      "Iteration: 656, Loss:2.58 \n",
      "Iteration: 657, Loss:2.87 \n",
      "Iteration: 658, Loss:2.71 \n",
      "Iteration: 659, Loss:2.65 \n",
      "Iteration: 660, Loss:2.82 \n",
      "Iteration: 661, Loss:2.83 \n",
      "Iteration: 662, Loss:2.46 \n",
      "Iteration: 663, Loss:2.24 \n",
      "Iteration: 664, Loss:2.58 \n",
      "Iteration: 665, Loss:2.58 \n",
      "Iteration: 666, Loss:2.07 \n",
      "Iteration: 667, Loss:2.33 \n",
      "Iteration: 668, Loss:2.19 \n",
      "Iteration: 669, Loss:2.16 \n",
      "Iteration: 670, Loss:2.66 \n",
      "Iteration: 671, Loss:2.49 \n",
      "Iteration: 672, Loss:2.68 \n",
      "Iteration: 673, Loss:2.11 \n",
      "Iteration: 674, Loss:2.61 \n",
      "Iteration: 675, Loss:2.73 \n",
      "Iteration: 676, Loss:2.68 \n",
      "Iteration: 677, Loss:2.59 \n",
      "Iteration: 678, Loss:2.43 \n",
      "Iteration: 679, Loss:2.73 \n",
      "Iteration: 680, Loss:2.31 \n",
      "Iteration: 681, Loss:2.06 \n",
      "Iteration: 682, Loss:2.98 \n",
      "Iteration: 683, Loss:2.55 \n",
      "Iteration: 684, Loss:2.81 \n",
      "Iteration: 685, Loss:2.51 \n",
      "Iteration: 686, Loss:2.32 \n",
      "Iteration: 687, Loss:2.38 \n",
      "Iteration: 688, Loss:2.79 \n",
      "Iteration: 689, Loss:2.58 \n",
      "Iteration: 690, Loss:2.33 \n",
      "Iteration: 691, Loss:2.48 \n",
      "Iteration: 692, Loss:2.66 \n",
      "Iteration: 693, Loss:2.72 \n",
      "Iteration: 694, Loss:2.67 \n",
      "Iteration: 695, Loss:2.40 \n",
      "Iteration: 696, Loss:2.48 \n",
      "Iteration: 697, Loss:2.66 \n",
      "Iteration: 698, Loss:2.98 \n",
      "Iteration: 699, Loss:2.41 \n",
      "Iteration: 700, Loss:2.62 \n",
      "Iteration: 701, Loss:2.15 \n",
      "Iteration: 702, Loss:2.30 \n",
      "Iteration: 703, Loss:2.84 \n",
      "Iteration: 704, Loss:2.42 \n",
      "Iteration: 705, Loss:2.31 \n",
      "Iteration: 706, Loss:2.76 \n",
      "Iteration: 707, Loss:2.71 \n",
      "Iteration: 708, Loss:2.09 \n",
      "Iteration: 709, Loss:2.67 \n",
      "Iteration: 710, Loss:2.51 \n",
      "Iteration: 711, Loss:2.68 \n",
      "Iteration: 712, Loss:2.39 \n",
      "Iteration: 713, Loss:2.01 \n",
      "Iteration: 714, Loss:2.83 \n",
      "Iteration: 715, Loss:2.30 \n",
      "Iteration: 716, Loss:2.68 \n",
      "Iteration: 717, Loss:2.76 \n",
      "Iteration: 718, Loss:2.57 \n",
      "Iteration: 719, Loss:2.42 \n",
      "Iteration: 720, Loss:2.25 \n",
      "Iteration: 721, Loss:2.58 \n",
      "Iteration: 722, Loss:2.45 \n",
      "Iteration: 723, Loss:2.67 \n",
      "Iteration: 724, Loss:2.52 \n",
      "Iteration: 725, Loss:2.46 \n",
      "Iteration: 726, Loss:2.75 \n",
      "Iteration: 727, Loss:2.73 \n",
      "Iteration: 728, Loss:2.44 \n",
      "Iteration: 729, Loss:2.51 \n",
      "Iteration: 730, Loss:2.76 \n",
      "Iteration: 731, Loss:2.40 \n",
      "Iteration: 732, Loss:2.64 \n",
      "Iteration: 733, Loss:2.56 \n",
      "Iteration: 734, Loss:2.23 \n",
      "Iteration: 735, Loss:2.45 \n",
      "Iteration: 736, Loss:2.83 \n",
      "Iteration: 737, Loss:2.89 \n",
      "Iteration: 738, Loss:2.44 \n",
      "Iteration: 739, Loss:2.66 \n",
      "Iteration: 740, Loss:2.82 \n",
      "Iteration: 741, Loss:2.12 \n",
      "Iteration: 742, Loss:2.66 \n",
      "Iteration: 743, Loss:2.47 \n",
      "Iteration: 744, Loss:2.57 \n",
      "Iteration: 745, Loss:2.70 \n",
      "Iteration: 746, Loss:2.52 \n",
      "Iteration: 747, Loss:2.67 \n",
      "Iteration: 748, Loss:2.34 \n",
      "Iteration: 749, Loss:2.73 \n",
      "Iteration: 750, Loss:2.56 \n",
      "Iteration: 751, Loss:2.54 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_750.ckpt\n",
      "Iteration: 752, Loss:2.49 \n",
      "Iteration: 753, Loss:2.33 \n",
      "Iteration: 754, Loss:2.49 \n",
      "Iteration: 755, Loss:2.36 \n",
      "Iteration: 756, Loss:2.55 \n",
      "Iteration: 757, Loss:2.24 \n",
      "Iteration: 758, Loss:2.51 \n",
      "Iteration: 759, Loss:2.69 \n",
      "Iteration: 760, Loss:3.01 \n",
      "Iteration: 761, Loss:2.73 \n",
      "Iteration: 762, Loss:2.42 \n",
      "Iteration: 763, Loss:2.46 \n",
      "Iteration: 764, Loss:2.23 \n",
      "Iteration: 765, Loss:2.61 \n",
      "Iteration: 766, Loss:2.85 \n",
      "Iteration: 767, Loss:2.56 \n",
      "Iteration: 768, Loss:2.39 \n",
      "Iteration: 769, Loss:2.14 \n",
      "Iteration: 770, Loss:2.58 \n",
      "Iteration: 771, Loss:2.45 \n",
      "Iteration: 772, Loss:2.57 \n",
      "Iteration: 773, Loss:2.70 \n",
      "Iteration: 774, Loss:2.39 \n",
      "Iteration: 775, Loss:2.08 \n",
      "Iteration: 776, Loss:1.73 \n",
      "Iteration: 777, Loss:2.40 \n",
      "Iteration: 778, Loss:2.67 \n",
      "Iteration: 779, Loss:2.46 \n",
      "Iteration: 780, Loss:2.94 \n",
      "Iteration: 781, Loss:2.79 \n",
      "Iteration: 782, Loss:2.53 \n",
      "Iteration: 783, Loss:2.75 \n",
      "Iteration: 784, Loss:2.78 \n",
      "Iteration: 785, Loss:2.50 \n",
      "Iteration: 786, Loss:2.76 \n",
      "Iteration: 787, Loss:2.61 \n",
      "Iteration: 788, Loss:2.69 \n",
      "Iteration: 789, Loss:2.70 \n",
      "Iteration: 790, Loss:2.51 \n",
      "Iteration: 791, Loss:2.84 \n",
      "Iteration: 792, Loss:2.24 \n",
      "Iteration: 793, Loss:2.32 \n",
      "Iteration: 794, Loss:2.72 \n",
      "Iteration: 795, Loss:2.71 \n",
      "Iteration: 796, Loss:2.69 \n",
      "Iteration: 797, Loss:2.84 \n",
      "Iteration: 798, Loss:2.49 \n",
      "Iteration: 799, Loss:2.72 \n",
      "Iteration: 800, Loss:2.58 \n",
      "Iteration: 801, Loss:2.43 \n",
      "Iteration: 802, Loss:2.38 \n",
      "Iteration: 803, Loss:2.86 \n",
      "Iteration: 804, Loss:2.18 \n",
      "Iteration: 805, Loss:2.67 \n",
      "Iteration: 806, Loss:2.11 \n",
      "Iteration: 807, Loss:2.54 \n",
      "Iteration: 808, Loss:2.52 \n",
      "Iteration: 809, Loss:2.66 \n",
      "Iteration: 810, Loss:2.76 \n",
      "Iteration: 811, Loss:2.83 \n",
      "Iteration: 812, Loss:2.60 \n",
      "Iteration: 813, Loss:2.63 \n",
      "Iteration: 814, Loss:2.59 \n",
      "Iteration: 815, Loss:2.52 \n",
      "Iteration: 816, Loss:2.20 \n",
      "Iteration: 817, Loss:2.57 \n",
      "Iteration: 818, Loss:2.62 \n",
      "Iteration: 819, Loss:2.45 \n",
      "Iteration: 820, Loss:2.95 \n",
      "Iteration: 821, Loss:2.43 \n",
      "Iteration: 822, Loss:2.59 \n",
      "Iteration: 823, Loss:2.86 \n",
      "Iteration: 824, Loss:2.48 \n",
      "Iteration: 825, Loss:2.83 \n",
      "Iteration: 826, Loss:2.55 \n",
      "Iteration: 827, Loss:2.41 \n",
      "Iteration: 828, Loss:2.78 \n",
      "Iteration: 829, Loss:2.34 \n",
      "Iteration: 830, Loss:2.66 \n",
      "Iteration: 831, Loss:2.51 \n",
      "Iteration: 832, Loss:2.60 \n",
      "Iteration: 833, Loss:2.33 \n",
      "Iteration: 834, Loss:2.60 \n",
      "Iteration: 835, Loss:2.60 \n",
      "Iteration: 836, Loss:2.29 \n",
      "Iteration: 837, Loss:2.34 \n",
      "Iteration: 838, Loss:2.29 \n",
      "Iteration: 839, Loss:2.96 \n",
      "Iteration: 840, Loss:2.43 \n",
      "Iteration: 841, Loss:2.81 \n",
      "Iteration: 842, Loss:2.58 \n",
      "Iteration: 843, Loss:2.26 \n",
      "Iteration: 844, Loss:2.12 \n",
      "Iteration: 845, Loss:2.30 \n",
      "Iteration: 846, Loss:2.42 \n",
      "Iteration: 847, Loss:2.63 \n",
      "Iteration: 848, Loss:2.82 \n",
      "Iteration: 849, Loss:2.53 \n",
      "Iteration: 850, Loss:2.52 \n",
      "Iteration: 851, Loss:2.23 \n",
      "Iteration: 852, Loss:2.40 \n",
      "Iteration: 853, Loss:2.82 \n",
      "Iteration: 854, Loss:2.73 \n",
      "Iteration: 855, Loss:2.55 \n",
      "Iteration: 856, Loss:2.65 \n",
      "Iteration: 857, Loss:2.67 \n",
      "Iteration: 858, Loss:1.96 \n",
      "Iteration: 859, Loss:2.24 \n",
      "Iteration: 860, Loss:2.50 \n",
      "Iteration: 861, Loss:2.30 \n",
      "Iteration: 862, Loss:2.97 \n",
      "Iteration: 863, Loss:2.63 \n",
      "Iteration: 864, Loss:2.78 \n",
      "Iteration: 865, Loss:2.62 \n",
      "Iteration: 866, Loss:2.93 \n",
      "Iteration: 867, Loss:2.03 \n",
      "Iteration: 868, Loss:2.66 \n",
      "Iteration: 869, Loss:2.92 \n",
      "Iteration: 870, Loss:2.54 \n",
      "Iteration: 871, Loss:2.67 \n",
      "Iteration: 872, Loss:2.56 \n",
      "Iteration: 873, Loss:2.56 \n",
      "Iteration: 874, Loss:2.69 \n",
      "Iteration: 875, Loss:2.61 \n",
      "Iteration: 876, Loss:2.80 \n",
      "Iteration: 877, Loss:2.65 \n",
      "Iteration: 878, Loss:2.83 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 879, Loss:2.90 \n",
      "Iteration: 880, Loss:2.65 \n",
      "Iteration: 881, Loss:2.63 \n",
      "Iteration: 882, Loss:2.74 \n",
      "Iteration: 883, Loss:2.45 \n",
      "Iteration: 884, Loss:2.77 \n",
      "Iteration: 885, Loss:2.62 \n",
      "Iteration: 886, Loss:2.77 \n",
      "Iteration: 887, Loss:2.59 \n",
      "Iteration: 888, Loss:2.41 \n",
      "Iteration: 889, Loss:2.62 \n",
      "Iteration: 890, Loss:2.60 \n",
      "Iteration: 891, Loss:2.41 \n",
      "Iteration: 892, Loss:2.70 \n",
      "Iteration: 893, Loss:2.61 \n",
      "Iteration: 894, Loss:2.52 \n",
      "Iteration: 895, Loss:2.63 \n",
      "Iteration: 896, Loss:2.28 \n",
      "Iteration: 897, Loss:2.96 \n",
      "Iteration: 898, Loss:2.29 \n",
      "Iteration: 899, Loss:2.74 \n",
      "Iteration: 900, Loss:2.72 \n",
      "Iteration: 901, Loss:2.39 \n",
      "Iteration: 902, Loss:2.40 \n",
      "Iteration: 903, Loss:2.87 \n",
      "Iteration: 904, Loss:2.37 \n",
      "Iteration: 905, Loss:2.32 \n",
      "Iteration: 906, Loss:2.40 \n",
      "Iteration: 907, Loss:2.68 \n",
      "Iteration: 908, Loss:2.60 \n",
      "Iteration: 909, Loss:2.54 \n",
      "Iteration: 910, Loss:2.56 \n",
      "Iteration: 911, Loss:2.60 \n",
      "Iteration: 912, Loss:2.22 \n",
      "Iteration: 913, Loss:2.35 \n",
      "Iteration: 914, Loss:2.86 \n",
      "Iteration: 915, Loss:2.50 \n",
      "Iteration: 916, Loss:2.78 \n",
      "Iteration: 917, Loss:2.77 \n",
      "Iteration: 918, Loss:2.91 \n",
      "Iteration: 919, Loss:2.38 \n",
      "Iteration: 920, Loss:2.43 \n",
      "Iteration: 921, Loss:2.40 \n",
      "Iteration: 922, Loss:2.71 \n",
      "Iteration: 923, Loss:2.25 \n",
      "Iteration: 924, Loss:2.75 \n",
      "Iteration: 925, Loss:2.28 \n",
      "Iteration: 926, Loss:2.13 \n",
      "Iteration: 927, Loss:2.92 \n",
      "Iteration: 928, Loss:2.64 \n",
      "Iteration: 929, Loss:2.74 \n",
      "Iteration: 930, Loss:2.40 \n",
      "Iteration: 931, Loss:2.48 \n",
      "Iteration: 932, Loss:2.66 \n",
      "Iteration: 933, Loss:2.86 \n",
      "Iteration: 934, Loss:2.69 \n",
      "Iteration: 935, Loss:2.36 \n",
      "Iteration: 936, Loss:2.58 \n",
      "Iteration: 937, Loss:2.48 \n",
      "Iteration: 938, Loss:2.60 \n",
      "Iteration: 939, Loss:2.72 \n",
      "Iteration: 940, Loss:2.57 \n",
      "Iteration: 941, Loss:2.41 \n",
      "Iteration: 942, Loss:2.65 \n",
      "Iteration: 943, Loss:2.63 \n",
      "Iteration: 944, Loss:2.55 \n",
      "Iteration: 945, Loss:2.54 \n",
      "Iteration: 946, Loss:2.55 \n",
      "Iteration: 947, Loss:2.60 \n",
      "Iteration: 948, Loss:2.33 \n",
      "Iteration: 949, Loss:2.92 \n",
      "Iteration: 950, Loss:2.43 \n",
      "Iteration: 951, Loss:2.43 \n",
      "Iteration: 952, Loss:2.48 \n",
      "Iteration: 953, Loss:2.76 \n",
      "Iteration: 954, Loss:2.63 \n",
      "Iteration: 955, Loss:2.94 \n",
      "Iteration: 956, Loss:2.16 \n",
      "Iteration: 957, Loss:2.68 \n",
      "Iteration: 958, Loss:2.23 \n",
      "Iteration: 959, Loss:2.92 \n",
      "Iteration: 960, Loss:2.64 \n",
      "Iteration: 961, Loss:2.25 \n",
      "Iteration: 962, Loss:2.20 \n",
      "Iteration: 963, Loss:2.45 \n",
      "Iteration: 964, Loss:2.34 \n",
      "Iteration: 965, Loss:2.45 \n",
      "Iteration: 966, Loss:2.38 \n",
      "Iteration: 967, Loss:2.67 \n",
      "Iteration: 968, Loss:2.51 \n",
      "Iteration: 969, Loss:2.63 \n",
      "Iteration: 970, Loss:2.86 \n",
      "Iteration: 971, Loss:2.39 \n",
      "Iteration: 972, Loss:2.37 \n",
      "Iteration: 973, Loss:2.37 \n",
      "Iteration: 974, Loss:2.38 \n",
      "Iteration: 975, Loss:1.91 \n",
      "Iteration: 976, Loss:2.33 \n",
      "Iteration: 977, Loss:2.28 \n",
      "Iteration: 978, Loss:2.46 \n",
      "Iteration: 979, Loss:2.39 \n",
      "Iteration: 980, Loss:2.82 \n",
      "Iteration: 981, Loss:2.46 \n",
      "Iteration: 982, Loss:2.76 \n",
      "Iteration: 983, Loss:2.35 \n",
      "Iteration: 984, Loss:2.31 \n",
      "Iteration: 985, Loss:2.26 \n",
      "Iteration: 986, Loss:2.92 \n",
      "Iteration: 987, Loss:2.75 \n",
      "Iteration: 988, Loss:2.60 \n",
      "Iteration: 989, Loss:2.36 \n",
      "Iteration: 990, Loss:2.88 \n",
      "Iteration: 991, Loss:2.30 \n",
      "Iteration: 992, Loss:2.56 \n",
      "Iteration: 993, Loss:2.45 \n",
      "Iteration: 994, Loss:2.83 \n",
      "Iteration: 995, Loss:2.56 \n",
      "Iteration: 996, Loss:2.45 \n",
      "Iteration: 997, Loss:2.35 \n",
      "Iteration: 998, Loss:2.47 \n",
      "Iteration: 999, Loss:2.47 \n",
      "Iteration: 1000, Loss:2.73 \n",
      "Iteration: 1001, Loss:2.73 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_1000.ckpt\n",
      "Iteration: 1002, Loss:2.66 \n",
      "Iteration: 1003, Loss:2.01 \n",
      "Iteration: 1004, Loss:2.84 \n",
      "Iteration: 1005, Loss:2.70 \n",
      "Iteration: 1006, Loss:2.62 \n",
      "Iteration: 1007, Loss:2.55 \n",
      "Iteration: 1008, Loss:2.55 \n",
      "Iteration: 1009, Loss:2.57 \n",
      "Iteration: 1010, Loss:2.20 \n",
      "Iteration: 1011, Loss:2.53 \n",
      "Iteration: 1012, Loss:2.17 \n",
      "Iteration: 1013, Loss:2.73 \n",
      "Iteration: 1014, Loss:2.41 \n",
      "Iteration: 1015, Loss:2.50 \n",
      "Iteration: 1016, Loss:2.21 \n",
      "Iteration: 1017, Loss:2.51 \n",
      "Iteration: 1018, Loss:2.64 \n",
      "Iteration: 1019, Loss:2.72 \n",
      "Iteration: 1020, Loss:2.50 \n",
      "Iteration: 1021, Loss:3.02 \n",
      "Iteration: 1022, Loss:2.65 \n",
      "Iteration: 1023, Loss:2.27 \n",
      "Iteration: 1024, Loss:2.63 \n",
      "Iteration: 1025, Loss:2.42 \n",
      "Iteration: 1026, Loss:2.47 \n",
      "Iteration: 1027, Loss:2.38 \n",
      "Iteration: 1028, Loss:1.86 \n",
      "Iteration: 1029, Loss:2.40 \n",
      "Iteration: 1030, Loss:2.47 \n",
      "Iteration: 1031, Loss:2.55 \n",
      "Iteration: 1032, Loss:2.79 \n",
      "Iteration: 1033, Loss:2.66 \n",
      "Iteration: 1034, Loss:2.50 \n",
      "Iteration: 1035, Loss:2.65 \n",
      "Iteration: 1036, Loss:2.50 \n",
      "Iteration: 1037, Loss:2.51 \n",
      "Iteration: 1038, Loss:2.84 \n",
      "Iteration: 1039, Loss:2.37 \n",
      "Iteration: 1040, Loss:2.49 \n",
      "Iteration: 1041, Loss:2.93 \n",
      "Iteration: 1042, Loss:2.47 \n",
      "Iteration: 1043, Loss:2.69 \n",
      "Iteration: 1044, Loss:2.70 \n",
      "Iteration: 1045, Loss:2.50 \n",
      "Iteration: 1046, Loss:2.68 \n",
      "Iteration: 1047, Loss:2.76 \n",
      "Iteration: 1048, Loss:2.47 \n",
      "Iteration: 1049, Loss:2.70 \n",
      "Iteration: 1050, Loss:2.38 \n",
      "Iteration: 1051, Loss:2.72 \n",
      "Iteration: 1052, Loss:3.01 \n",
      "Iteration: 1053, Loss:2.43 \n",
      "Iteration: 1054, Loss:2.84 \n",
      "Iteration: 1055, Loss:2.71 \n",
      "Iteration: 1056, Loss:2.15 \n",
      "Iteration: 1057, Loss:2.31 \n",
      "Iteration: 1058, Loss:2.14 \n",
      "Iteration: 1059, Loss:3.00 \n",
      "Iteration: 1060, Loss:2.22 \n",
      "Iteration: 1061, Loss:2.50 \n",
      "Iteration: 1062, Loss:2.66 \n",
      "Iteration: 1063, Loss:2.62 \n",
      "Iteration: 1064, Loss:2.59 \n",
      "Iteration: 1065, Loss:2.87 \n",
      "Iteration: 1066, Loss:2.79 \n",
      "Iteration: 1067, Loss:2.47 \n",
      "Iteration: 1068, Loss:2.36 \n",
      "Iteration: 1069, Loss:2.48 \n",
      "Iteration: 1070, Loss:2.78 \n",
      "Iteration: 1071, Loss:2.89 \n",
      "Iteration: 1072, Loss:2.45 \n",
      "Iteration: 1073, Loss:2.71 \n",
      "Iteration: 1074, Loss:2.69 \n",
      "Iteration: 1075, Loss:2.53 \n",
      "Iteration: 1076, Loss:2.59 \n",
      "Iteration: 1077, Loss:2.78 \n",
      "Iteration: 1078, Loss:2.28 \n",
      "Iteration: 1079, Loss:2.05 \n",
      "Iteration: 1080, Loss:2.81 \n",
      "Iteration: 1081, Loss:2.15 \n",
      "Iteration: 1082, Loss:2.88 \n",
      "Iteration: 1083, Loss:2.21 \n",
      "Iteration: 1084, Loss:2.41 \n",
      "Iteration: 1085, Loss:2.30 \n",
      "Iteration: 1086, Loss:2.50 \n",
      "Iteration: 1087, Loss:2.49 \n",
      "Iteration: 1088, Loss:3.07 \n",
      "Iteration: 1089, Loss:1.87 \n",
      "Iteration: 1090, Loss:2.82 \n",
      "Iteration: 1091, Loss:2.74 \n",
      "Iteration: 1092, Loss:2.70 \n",
      "Iteration: 1093, Loss:2.38 \n",
      "Iteration: 1094, Loss:2.54 \n",
      "Iteration: 1095, Loss:2.14 \n",
      "Iteration: 1096, Loss:2.41 \n",
      "Iteration: 1097, Loss:2.59 \n",
      "Iteration: 1098, Loss:2.78 \n",
      "Iteration: 1099, Loss:2.41 \n",
      "Iteration: 1100, Loss:2.49 \n",
      "Iteration: 1101, Loss:2.76 \n",
      "Iteration: 1102, Loss:2.76 \n",
      "Iteration: 1103, Loss:2.54 \n",
      "Iteration: 1104, Loss:2.85 \n",
      "Iteration: 1105, Loss:2.17 \n",
      "Iteration: 1106, Loss:2.35 \n",
      "Iteration: 1107, Loss:2.30 \n",
      "Iteration: 1108, Loss:2.44 \n",
      "Iteration: 1109, Loss:2.33 \n",
      "Iteration: 1110, Loss:2.44 \n",
      "Iteration: 1111, Loss:2.44 \n",
      "Iteration: 1112, Loss:2.40 \n",
      "Iteration: 1113, Loss:2.30 \n",
      "Iteration: 1114, Loss:2.55 \n",
      "Iteration: 1115, Loss:2.10 \n",
      "Iteration: 1116, Loss:2.92 \n",
      "Iteration: 1117, Loss:2.36 \n",
      "Iteration: 1118, Loss:2.43 \n",
      "Iteration: 1119, Loss:2.30 \n",
      "Iteration: 1120, Loss:2.52 \n",
      "Iteration: 1121, Loss:2.10 \n",
      "Iteration: 1122, Loss:2.15 \n",
      "Iteration: 1123, Loss:2.33 \n",
      "Iteration: 1124, Loss:1.92 \n",
      "Iteration: 1125, Loss:2.70 \n",
      "Iteration: 1126, Loss:2.58 \n",
      "Iteration: 1127, Loss:2.57 \n",
      "Iteration: 1128, Loss:2.36 \n",
      "Iteration: 1129, Loss:2.83 \n",
      "Iteration: 1130, Loss:2.99 \n",
      "Iteration: 1131, Loss:2.35 \n",
      "Iteration: 1132, Loss:2.69 \n",
      "Iteration: 1133, Loss:2.84 \n",
      "Iteration: 1134, Loss:2.73 \n",
      "Iteration: 1135, Loss:2.65 \n",
      "Iteration: 1136, Loss:2.63 \n",
      "Iteration: 1137, Loss:2.71 \n",
      "Iteration: 1138, Loss:2.70 \n",
      "Iteration: 1139, Loss:2.34 \n",
      "Iteration: 1140, Loss:2.55 \n",
      "Iteration: 1141, Loss:2.89 \n",
      "Iteration: 1142, Loss:2.37 \n",
      "Iteration: 1143, Loss:2.73 \n",
      "Iteration: 1144, Loss:2.72 \n",
      "Iteration: 1145, Loss:2.74 \n",
      "Iteration: 1146, Loss:2.94 \n",
      "Iteration: 1147, Loss:2.65 \n",
      "Iteration: 1148, Loss:2.79 \n",
      "Iteration: 1149, Loss:2.34 \n",
      "Iteration: 1150, Loss:2.68 \n",
      "Iteration: 1151, Loss:2.44 \n",
      "Iteration: 1152, Loss:2.55 \n",
      "Iteration: 1153, Loss:2.61 \n",
      "Iteration: 1154, Loss:1.99 \n",
      "Iteration: 1155, Loss:2.65 \n",
      "Iteration: 1156, Loss:2.37 \n",
      "Iteration: 1157, Loss:2.38 \n",
      "Iteration: 1158, Loss:2.40 \n",
      "Iteration: 1159, Loss:2.19 \n",
      "Iteration: 1160, Loss:2.53 \n",
      "Iteration: 1161, Loss:2.27 \n",
      "Iteration: 1162, Loss:2.37 \n",
      "Iteration: 1163, Loss:2.17 \n",
      "Iteration: 1164, Loss:2.68 \n",
      "Iteration: 1165, Loss:2.46 \n",
      "Iteration: 1166, Loss:2.55 \n",
      "Iteration: 1167, Loss:2.67 \n",
      "Iteration: 1168, Loss:2.49 \n",
      "Iteration: 1169, Loss:2.55 \n",
      "Iteration: 1170, Loss:2.90 \n",
      "Iteration: 1171, Loss:2.42 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1172, Loss:2.89 \n",
      "Iteration: 1173, Loss:2.10 \n",
      "Iteration: 1174, Loss:2.53 \n",
      "Iteration: 1175, Loss:2.66 \n",
      "Iteration: 1176, Loss:2.42 \n",
      "Iteration: 1177, Loss:2.38 \n",
      "Iteration: 1178, Loss:2.49 \n",
      "Iteration: 1179, Loss:2.25 \n",
      "Iteration: 1180, Loss:2.22 \n",
      "Iteration: 1181, Loss:2.69 \n",
      "Iteration: 1182, Loss:2.42 \n",
      "Iteration: 1183, Loss:2.19 \n",
      "Iteration: 1184, Loss:2.72 \n",
      "Iteration: 1185, Loss:2.65 \n",
      "Iteration: 1186, Loss:2.08 \n",
      "Iteration: 1187, Loss:2.83 \n",
      "Iteration: 1188, Loss:2.59 \n",
      "Iteration: 1189, Loss:2.19 \n",
      "Iteration: 1190, Loss:2.67 \n",
      "Iteration: 1191, Loss:2.59 \n",
      "Iteration: 1192, Loss:2.77 \n",
      "Iteration: 1193, Loss:2.63 \n",
      "Iteration: 1194, Loss:2.94 \n",
      "Iteration: 1195, Loss:2.49 \n",
      "Iteration: 1196, Loss:2.62 \n",
      "Iteration: 1197, Loss:2.13 \n",
      "Iteration: 1198, Loss:2.63 \n",
      "Iteration: 1199, Loss:2.46 \n",
      "Iteration: 1200, Loss:2.53 \n",
      "Iteration: 1201, Loss:2.92 \n",
      "Iteration: 1202, Loss:2.71 \n",
      "Iteration: 1203, Loss:2.96 \n",
      "Iteration: 1204, Loss:2.58 \n",
      "Iteration: 1205, Loss:2.60 \n",
      "Iteration: 1206, Loss:2.69 \n",
      "Iteration: 1207, Loss:2.43 \n",
      "Iteration: 1208, Loss:2.39 \n",
      "Iteration: 1209, Loss:2.49 \n",
      "Iteration: 1210, Loss:2.14 \n",
      "Iteration: 1211, Loss:2.48 \n",
      "Iteration: 1212, Loss:2.49 \n",
      "Iteration: 1213, Loss:2.32 \n",
      "Iteration: 1214, Loss:2.45 \n",
      "Iteration: 1215, Loss:2.66 \n",
      "Iteration: 1216, Loss:2.36 \n",
      "Iteration: 1217, Loss:2.76 \n",
      "Iteration: 1218, Loss:2.78 \n",
      "Iteration: 1219, Loss:2.03 \n",
      "Iteration: 1220, Loss:2.86 \n",
      "Iteration: 1221, Loss:2.57 \n",
      "Iteration: 1222, Loss:2.76 \n",
      "Iteration: 1223, Loss:2.61 \n",
      "Iteration: 1224, Loss:2.35 \n",
      "Iteration: 1225, Loss:2.61 \n",
      "Iteration: 1226, Loss:2.77 \n",
      "Iteration: 1227, Loss:2.92 \n",
      "Iteration: 1228, Loss:2.33 \n",
      "Iteration: 1229, Loss:2.28 \n",
      "Iteration: 1230, Loss:2.82 \n",
      "Iteration: 1231, Loss:2.24 \n",
      "Iteration: 1232, Loss:2.79 \n",
      "Iteration: 1233, Loss:2.82 \n",
      "Iteration: 1234, Loss:2.62 \n",
      "Iteration: 1235, Loss:2.42 \n",
      "Iteration: 1236, Loss:2.86 \n",
      "Iteration: 1237, Loss:2.81 \n",
      "Iteration: 1238, Loss:2.41 \n",
      "Iteration: 1239, Loss:2.68 \n",
      "Iteration: 1240, Loss:2.20 \n",
      "Iteration: 1241, Loss:2.36 \n",
      "Iteration: 1242, Loss:2.73 \n",
      "Iteration: 1243, Loss:2.07 \n",
      "Iteration: 1244, Loss:2.33 \n",
      "Iteration: 1245, Loss:2.43 \n",
      "Iteration: 1246, Loss:2.33 \n",
      "Iteration: 1247, Loss:2.25 \n",
      "Iteration: 1248, Loss:2.63 \n",
      "Iteration: 1249, Loss:2.50 \n",
      "Iteration: 1250, Loss:2.34 \n",
      "Iteration: 1251, Loss:2.70 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_1250.ckpt\n",
      "Iteration: 1252, Loss:2.42 \n",
      "Iteration: 1253, Loss:2.79 \n",
      "Iteration: 1254, Loss:2.84 \n",
      "Iteration: 1255, Loss:2.35 \n",
      "Iteration: 1256, Loss:2.42 \n",
      "Iteration: 1257, Loss:2.48 \n",
      "Iteration: 1258, Loss:2.31 \n",
      "Iteration: 1259, Loss:2.64 \n",
      "Iteration: 1260, Loss:2.21 \n",
      "Iteration: 1261, Loss:2.19 \n",
      "Iteration: 1262, Loss:2.44 \n",
      "Iteration: 1263, Loss:2.72 \n",
      "Iteration: 1264, Loss:2.65 \n",
      "Iteration: 1265, Loss:2.77 \n",
      "Iteration: 1266, Loss:2.41 \n",
      "Iteration: 1267, Loss:2.67 \n",
      "Iteration: 1268, Loss:2.13 \n",
      "Iteration: 1269, Loss:2.37 \n",
      "Iteration: 1270, Loss:3.02 \n",
      "Iteration: 1271, Loss:2.84 \n",
      "Iteration: 1272, Loss:2.85 \n",
      "Iteration: 1273, Loss:2.42 \n",
      "Iteration: 1274, Loss:2.80 \n",
      "Iteration: 1275, Loss:2.23 \n",
      "Iteration: 1276, Loss:2.79 \n",
      "Iteration: 1277, Loss:2.66 \n",
      "Iteration: 1278, Loss:2.87 \n",
      "Iteration: 1279, Loss:2.72 \n",
      "Iteration: 1280, Loss:2.89 \n",
      "Iteration: 1281, Loss:2.47 \n",
      "Iteration: 1282, Loss:2.76 \n",
      "Iteration: 1283, Loss:2.33 \n",
      "Iteration: 1284, Loss:2.64 \n",
      "Iteration: 1285, Loss:2.80 \n",
      "Iteration: 1286, Loss:2.81 \n",
      "Iteration: 1287, Loss:2.64 \n",
      "Iteration: 1288, Loss:2.65 \n",
      "Iteration: 1289, Loss:2.27 \n",
      "Iteration: 1290, Loss:1.90 \n",
      "Iteration: 1291, Loss:2.65 \n",
      "Iteration: 1292, Loss:2.24 \n",
      "Iteration: 1293, Loss:2.50 \n",
      "Iteration: 1294, Loss:2.59 \n",
      "Iteration: 1295, Loss:2.21 \n",
      "Iteration: 1296, Loss:2.79 \n",
      "Iteration: 1297, Loss:3.03 \n",
      "Iteration: 1298, Loss:2.54 \n",
      "Iteration: 1299, Loss:2.79 \n",
      "Iteration: 1300, Loss:2.30 \n",
      "Iteration: 1301, Loss:2.58 \n",
      "Iteration: 1302, Loss:2.69 \n",
      "Iteration: 1303, Loss:2.54 \n",
      "Iteration: 1304, Loss:2.96 \n",
      "Iteration: 1305, Loss:2.71 \n",
      "Iteration: 1306, Loss:2.78 \n",
      "Iteration: 1307, Loss:2.65 \n",
      "Iteration: 1308, Loss:2.10 \n",
      "Iteration: 1309, Loss:3.06 \n",
      "Iteration: 1310, Loss:2.57 \n",
      "Iteration: 1311, Loss:2.79 \n",
      "Iteration: 1312, Loss:3.05 \n",
      "Iteration: 1313, Loss:2.27 \n",
      "Iteration: 1314, Loss:2.92 \n",
      "Iteration: 1315, Loss:2.70 \n",
      "Iteration: 1316, Loss:2.40 \n",
      "Iteration: 1317, Loss:2.10 \n",
      "Iteration: 1318, Loss:2.79 \n",
      "Iteration: 1319, Loss:2.38 \n",
      "Iteration: 1320, Loss:2.53 \n",
      "Iteration: 1321, Loss:2.55 \n",
      "Iteration: 1322, Loss:2.75 \n",
      "Iteration: 1323, Loss:2.48 \n",
      "Iteration: 1324, Loss:2.08 \n",
      "Iteration: 1325, Loss:2.39 \n",
      "Iteration: 1326, Loss:2.44 \n",
      "Iteration: 1327, Loss:2.76 \n",
      "Iteration: 1328, Loss:2.48 \n",
      "Iteration: 1329, Loss:2.86 \n",
      "Iteration: 1330, Loss:2.63 \n",
      "Iteration: 1331, Loss:2.67 \n",
      "Iteration: 1332, Loss:2.82 \n",
      "Iteration: 1333, Loss:2.93 \n",
      "Iteration: 1334, Loss:2.42 \n",
      "Iteration: 1335, Loss:2.22 \n",
      "Iteration: 1336, Loss:2.36 \n",
      "Iteration: 1337, Loss:2.38 \n",
      "Iteration: 1338, Loss:2.58 \n",
      "Iteration: 1339, Loss:2.81 \n",
      "Iteration: 1340, Loss:2.44 \n",
      "Iteration: 1341, Loss:2.38 \n",
      "Iteration: 1342, Loss:2.68 \n",
      "Iteration: 1343, Loss:2.67 \n",
      "Iteration: 1344, Loss:2.33 \n",
      "Iteration: 1345, Loss:2.37 \n",
      "Iteration: 1346, Loss:2.55 \n",
      "Iteration: 1347, Loss:2.57 \n",
      "Iteration: 1348, Loss:2.98 \n",
      "Iteration: 1349, Loss:2.85 \n",
      "Iteration: 1350, Loss:2.90 \n",
      "Iteration: 1351, Loss:2.42 \n",
      "Iteration: 1352, Loss:1.95 \n",
      "Iteration: 1353, Loss:3.06 \n",
      "Iteration: 1354, Loss:2.52 \n",
      "Iteration: 1355, Loss:2.30 \n",
      "Iteration: 1356, Loss:2.49 \n",
      "Iteration: 1357, Loss:2.34 \n",
      "Iteration: 1358, Loss:2.61 \n",
      "Iteration: 1359, Loss:2.71 \n",
      "Iteration: 1360, Loss:2.50 \n",
      "Iteration: 1361, Loss:2.25 \n",
      "Iteration: 1362, Loss:2.39 \n",
      "Iteration: 1363, Loss:2.58 \n",
      "Iteration: 1364, Loss:2.75 \n",
      "Iteration: 1365, Loss:2.69 \n",
      "Iteration: 1366, Loss:2.54 \n",
      "Iteration: 1367, Loss:2.97 \n",
      "Iteration: 1368, Loss:2.09 \n",
      "Iteration: 1369, Loss:2.52 \n",
      "Iteration: 1370, Loss:2.62 \n",
      "Iteration: 1371, Loss:2.63 \n",
      "Iteration: 1372, Loss:2.70 \n",
      "Iteration: 1373, Loss:2.67 \n",
      "Iteration: 1374, Loss:2.52 \n",
      "Iteration: 1375, Loss:2.83 \n",
      "Iteration: 1376, Loss:2.58 \n",
      "Iteration: 1377, Loss:2.58 \n",
      "Iteration: 1378, Loss:2.55 \n",
      "Iteration: 1379, Loss:2.80 \n",
      "Iteration: 1380, Loss:2.37 \n",
      "Iteration: 1381, Loss:2.71 \n",
      "Iteration: 1382, Loss:2.68 \n",
      "Iteration: 1383, Loss:2.40 \n",
      "Iteration: 1384, Loss:2.50 \n",
      "Iteration: 1385, Loss:2.51 \n",
      "Iteration: 1386, Loss:2.51 \n",
      "Iteration: 1387, Loss:2.41 \n",
      "Iteration: 1388, Loss:2.39 \n",
      "Iteration: 1389, Loss:2.44 \n",
      "Iteration: 1390, Loss:2.55 \n",
      "Iteration: 1391, Loss:2.59 \n",
      "Iteration: 1392, Loss:2.47 \n",
      "Iteration: 1393, Loss:2.40 \n",
      "Iteration: 1394, Loss:2.36 \n",
      "Iteration: 1395, Loss:2.72 \n",
      "Iteration: 1396, Loss:2.58 \n",
      "Iteration: 1397, Loss:2.56 \n",
      "Iteration: 1398, Loss:2.53 \n",
      "Iteration: 1399, Loss:2.64 \n",
      "Iteration: 1400, Loss:2.48 \n",
      "Iteration: 1401, Loss:2.39 \n",
      "Iteration: 1402, Loss:2.66 \n",
      "Iteration: 1403, Loss:2.39 \n",
      "Iteration: 1404, Loss:2.29 \n",
      "Iteration: 1405, Loss:2.00 \n",
      "Iteration: 1406, Loss:2.39 \n",
      "Iteration: 1407, Loss:2.86 \n",
      "Iteration: 1408, Loss:3.00 \n",
      "Iteration: 1409, Loss:2.24 \n",
      "Iteration: 1410, Loss:2.69 \n",
      "Iteration: 1411, Loss:2.54 \n",
      "Iteration: 1412, Loss:2.66 \n",
      "Iteration: 1413, Loss:2.66 \n",
      "Iteration: 1414, Loss:2.81 \n",
      "Iteration: 1415, Loss:2.75 \n",
      "Iteration: 1416, Loss:2.70 \n",
      "Iteration: 1417, Loss:2.64 \n",
      "Iteration: 1418, Loss:2.79 \n",
      "Iteration: 1419, Loss:2.40 \n",
      "Iteration: 1420, Loss:2.52 \n",
      "Iteration: 1421, Loss:2.67 \n",
      "Iteration: 1422, Loss:2.60 \n",
      "Iteration: 1423, Loss:2.45 \n",
      "Iteration: 1424, Loss:2.78 \n",
      "Iteration: 1425, Loss:2.49 \n",
      "Iteration: 1426, Loss:2.78 \n",
      "Iteration: 1427, Loss:2.40 \n",
      "Iteration: 1428, Loss:2.73 \n",
      "Iteration: 1429, Loss:2.49 \n",
      "Iteration: 1430, Loss:2.36 \n",
      "Iteration: 1431, Loss:2.55 \n",
      "Iteration: 1432, Loss:2.50 \n",
      "Iteration: 1433, Loss:2.72 \n",
      "Iteration: 1434, Loss:2.50 \n",
      "Iteration: 1435, Loss:2.57 \n",
      "Iteration: 1436, Loss:2.69 \n",
      "Iteration: 1437, Loss:2.58 \n",
      "Iteration: 1438, Loss:2.59 \n",
      "Iteration: 1439, Loss:2.73 \n",
      "Iteration: 1440, Loss:2.40 \n",
      "Iteration: 1441, Loss:2.31 \n",
      "Iteration: 1442, Loss:2.23 \n",
      "Iteration: 1443, Loss:2.85 \n",
      "Iteration: 1444, Loss:2.94 \n",
      "Iteration: 1445, Loss:2.87 \n",
      "Iteration: 1446, Loss:2.50 \n",
      "Iteration: 1447, Loss:2.77 \n",
      "Iteration: 1448, Loss:2.51 \n",
      "Iteration: 1449, Loss:2.73 \n",
      "Iteration: 1450, Loss:2.49 \n",
      "Iteration: 1451, Loss:2.46 \n",
      "Iteration: 1452, Loss:3.00 \n",
      "Iteration: 1453, Loss:2.59 \n",
      "Iteration: 1454, Loss:2.32 \n",
      "Iteration: 1455, Loss:2.63 \n",
      "Iteration: 1456, Loss:2.68 \n",
      "Iteration: 1457, Loss:2.31 \n",
      "Iteration: 1458, Loss:2.42 \n",
      "Iteration: 1459, Loss:1.76 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1460, Loss:2.34 \n",
      "Iteration: 1461, Loss:2.70 \n",
      "Iteration: 1462, Loss:2.53 \n",
      "Iteration: 1463, Loss:2.90 \n",
      "Iteration: 1464, Loss:2.77 \n",
      "Iteration: 1465, Loss:2.36 \n",
      "Iteration: 1466, Loss:2.76 \n",
      "Iteration: 1467, Loss:2.96 \n",
      "Iteration: 1468, Loss:2.56 \n",
      "Iteration: 1469, Loss:2.70 \n",
      "Iteration: 1470, Loss:1.95 \n",
      "Iteration: 1471, Loss:2.71 \n",
      "Iteration: 1472, Loss:2.49 \n",
      "Iteration: 1473, Loss:2.35 \n",
      "Iteration: 1474, Loss:2.23 \n",
      "Iteration: 1475, Loss:2.68 \n",
      "Iteration: 1476, Loss:2.56 \n",
      "Iteration: 1477, Loss:2.57 \n",
      "Iteration: 1478, Loss:2.54 \n",
      "Iteration: 1479, Loss:2.14 \n",
      "Iteration: 1480, Loss:3.06 \n",
      "Iteration: 1481, Loss:2.67 \n",
      "Iteration: 1482, Loss:2.18 \n",
      "Iteration: 1483, Loss:2.12 \n",
      "Iteration: 1484, Loss:2.51 \n",
      "Iteration: 1485, Loss:2.04 \n",
      "Iteration: 1486, Loss:2.71 \n",
      "Iteration: 1487, Loss:2.12 \n",
      "Iteration: 1488, Loss:2.79 \n",
      "Iteration: 1489, Loss:2.17 \n",
      "Iteration: 1490, Loss:2.62 \n",
      "Iteration: 1491, Loss:2.38 \n",
      "Iteration: 1492, Loss:2.24 \n",
      "Iteration: 1493, Loss:2.36 \n",
      "Iteration: 1494, Loss:2.66 \n",
      "Iteration: 1495, Loss:2.16 \n",
      "Iteration: 1496, Loss:2.79 \n",
      "Iteration: 1497, Loss:2.30 \n",
      "Iteration: 1498, Loss:2.71 \n",
      "Iteration: 1499, Loss:2.40 \n",
      "Iteration: 1500, Loss:2.60 \n",
      "Iteration: 1501, Loss:2.56 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_1500.ckpt\n",
      "Iteration: 1502, Loss:2.22 \n",
      "Iteration: 1503, Loss:2.77 \n",
      "Iteration: 1504, Loss:2.78 \n",
      "Iteration: 1505, Loss:2.51 \n",
      "Iteration: 1506, Loss:2.88 \n",
      "Iteration: 1507, Loss:2.59 \n",
      "Iteration: 1508, Loss:2.41 \n",
      "Iteration: 1509, Loss:2.75 \n",
      "Iteration: 1510, Loss:2.81 \n",
      "Iteration: 1511, Loss:2.42 \n",
      "Iteration: 1512, Loss:2.66 \n",
      "Iteration: 1513, Loss:2.08 \n",
      "Iteration: 1514, Loss:2.61 \n",
      "Iteration: 1515, Loss:2.78 \n",
      "Iteration: 1516, Loss:2.42 \n",
      "Iteration: 1517, Loss:2.55 \n",
      "Iteration: 1518, Loss:2.31 \n",
      "Iteration: 1519, Loss:2.78 \n",
      "Iteration: 1520, Loss:2.44 \n",
      "Iteration: 1521, Loss:2.20 \n",
      "Iteration: 1522, Loss:2.56 \n",
      "Iteration: 1523, Loss:2.43 \n",
      "Iteration: 1524, Loss:2.72 \n",
      "Iteration: 1525, Loss:2.05 \n",
      "Iteration: 1526, Loss:2.29 \n",
      "Iteration: 1527, Loss:2.40 \n",
      "Iteration: 1528, Loss:2.31 \n",
      "Iteration: 1529, Loss:2.44 \n",
      "Iteration: 1530, Loss:2.51 \n",
      "Iteration: 1531, Loss:2.43 \n",
      "Iteration: 1532, Loss:2.73 \n",
      "Iteration: 1533, Loss:2.45 \n",
      "Iteration: 1534, Loss:2.61 \n",
      "Iteration: 1535, Loss:2.86 \n",
      "Iteration: 1536, Loss:2.78 \n",
      "Iteration: 1537, Loss:2.72 \n",
      "Iteration: 1538, Loss:2.61 \n",
      "Iteration: 1539, Loss:2.38 \n",
      "Iteration: 1540, Loss:2.56 \n",
      "Iteration: 1541, Loss:2.55 \n",
      "Iteration: 1542, Loss:2.41 \n",
      "Iteration: 1543, Loss:2.85 \n",
      "Iteration: 1544, Loss:2.31 \n",
      "Iteration: 1545, Loss:2.68 \n",
      "Iteration: 1546, Loss:2.15 \n",
      "Iteration: 1547, Loss:2.58 \n",
      "Iteration: 1548, Loss:2.65 \n",
      "Iteration: 1549, Loss:2.56 \n",
      "Iteration: 1550, Loss:3.00 \n",
      "Iteration: 1551, Loss:2.72 \n",
      "Iteration: 1552, Loss:2.50 \n",
      "Iteration: 1553, Loss:2.69 \n",
      "Iteration: 1554, Loss:2.73 \n",
      "Iteration: 1555, Loss:2.82 \n",
      "Iteration: 1556, Loss:2.72 \n",
      "Iteration: 1557, Loss:2.42 \n",
      "Iteration: 1558, Loss:1.86 \n",
      "Iteration: 1559, Loss:2.59 \n",
      "Iteration: 1560, Loss:2.76 \n",
      "Iteration: 1561, Loss:2.77 \n",
      "Iteration: 1562, Loss:2.53 \n",
      "Iteration: 1563, Loss:2.62 \n",
      "Iteration: 1564, Loss:2.37 \n",
      "Iteration: 1565, Loss:2.71 \n",
      "Iteration: 1566, Loss:2.70 \n",
      "Iteration: 1567, Loss:2.53 \n",
      "Iteration: 1568, Loss:2.68 \n",
      "Iteration: 1569, Loss:2.14 \n",
      "Iteration: 1570, Loss:2.52 \n",
      "Iteration: 1571, Loss:2.15 \n",
      "Iteration: 1572, Loss:2.74 \n",
      "Iteration: 1573, Loss:2.60 \n",
      "Iteration: 1574, Loss:1.99 \n",
      "Iteration: 1575, Loss:2.52 \n",
      "Iteration: 1576, Loss:3.05 \n",
      "Iteration: 1577, Loss:2.69 \n",
      "Iteration: 1578, Loss:2.61 \n",
      "Iteration: 1579, Loss:2.44 \n",
      "Iteration: 1580, Loss:2.60 \n",
      "Iteration: 1581, Loss:2.47 \n",
      "Iteration: 1582, Loss:2.58 \n",
      "Iteration: 1583, Loss:2.59 \n",
      "Iteration: 1584, Loss:2.74 \n",
      "Iteration: 1585, Loss:2.24 \n",
      "Iteration: 1586, Loss:2.59 \n",
      "Iteration: 1587, Loss:2.40 \n",
      "Iteration: 1588, Loss:2.46 \n",
      "Iteration: 1589, Loss:2.70 \n",
      "Iteration: 1590, Loss:1.93 \n",
      "Iteration: 1591, Loss:2.53 \n",
      "Iteration: 1592, Loss:2.02 \n",
      "Iteration: 1593, Loss:2.75 \n",
      "Iteration: 1594, Loss:2.53 \n",
      "Iteration: 1595, Loss:2.00 \n",
      "Iteration: 1596, Loss:2.80 \n",
      "Iteration: 1597, Loss:2.90 \n",
      "Iteration: 1598, Loss:2.82 \n",
      "Iteration: 1599, Loss:2.16 \n",
      "Iteration: 1600, Loss:2.59 \n",
      "Iteration: 1601, Loss:2.87 \n",
      "Iteration: 1602, Loss:2.43 \n",
      "Iteration: 1603, Loss:2.33 \n",
      "Iteration: 1604, Loss:2.34 \n",
      "Iteration: 1605, Loss:2.27 \n",
      "Iteration: 1606, Loss:2.12 \n",
      "Iteration: 1607, Loss:2.48 \n",
      "Iteration: 1608, Loss:2.56 \n",
      "Iteration: 1609, Loss:2.67 \n",
      "Iteration: 1610, Loss:2.59 \n",
      "Iteration: 1611, Loss:2.26 \n",
      "Iteration: 1612, Loss:2.35 \n",
      "Iteration: 1613, Loss:2.83 \n",
      "Iteration: 1614, Loss:2.28 \n",
      "Iteration: 1615, Loss:2.22 \n",
      "Iteration: 1616, Loss:2.70 \n",
      "Iteration: 1617, Loss:2.90 \n",
      "Iteration: 1618, Loss:2.63 \n",
      "Iteration: 1619, Loss:2.69 \n",
      "Iteration: 1620, Loss:2.33 \n",
      "Iteration: 1621, Loss:2.70 \n",
      "Iteration: 1622, Loss:2.15 \n",
      "Iteration: 1623, Loss:2.96 \n",
      "Iteration: 1624, Loss:1.99 \n",
      "Iteration: 1625, Loss:2.54 \n",
      "Iteration: 1626, Loss:2.63 \n",
      "Iteration: 1627, Loss:2.83 \n",
      "Iteration: 1628, Loss:1.98 \n",
      "Iteration: 1629, Loss:2.84 \n",
      "Iteration: 1630, Loss:2.52 \n",
      "Iteration: 1631, Loss:2.72 \n",
      "Iteration: 1632, Loss:2.90 \n",
      "Iteration: 1633, Loss:2.89 \n",
      "Iteration: 1634, Loss:2.27 \n",
      "Iteration: 1635, Loss:2.88 \n",
      "Iteration: 1636, Loss:2.50 \n",
      "Iteration: 1637, Loss:2.45 \n",
      "Iteration: 1638, Loss:2.48 \n",
      "Iteration: 1639, Loss:2.30 \n",
      "Iteration: 1640, Loss:2.44 \n",
      "Iteration: 1641, Loss:2.91 \n",
      "Iteration: 1642, Loss:2.65 \n",
      "Iteration: 1643, Loss:2.96 \n",
      "Iteration: 1644, Loss:2.59 \n",
      "Iteration: 1645, Loss:2.21 \n",
      "Iteration: 1646, Loss:2.30 \n",
      "Iteration: 1647, Loss:2.64 \n",
      "Iteration: 1648, Loss:2.58 \n",
      "Iteration: 1649, Loss:2.27 \n",
      "Iteration: 1650, Loss:2.07 \n",
      "Iteration: 1651, Loss:2.72 \n",
      "Iteration: 1652, Loss:2.46 \n",
      "Iteration: 1653, Loss:3.00 \n",
      "Iteration: 1654, Loss:2.45 \n",
      "Iteration: 1655, Loss:2.58 \n",
      "Iteration: 1656, Loss:2.63 \n",
      "Iteration: 1657, Loss:2.37 \n",
      "Iteration: 1658, Loss:2.11 \n",
      "Iteration: 1659, Loss:2.67 \n",
      "Iteration: 1660, Loss:2.42 \n",
      "Iteration: 1661, Loss:2.30 \n",
      "Iteration: 1662, Loss:2.69 \n",
      "Iteration: 1663, Loss:2.69 \n",
      "Iteration: 1664, Loss:2.59 \n",
      "Iteration: 1665, Loss:2.85 \n",
      "Iteration: 1666, Loss:2.56 \n",
      "Iteration: 1667, Loss:2.65 \n",
      "Iteration: 1668, Loss:2.59 \n",
      "Iteration: 1669, Loss:2.58 \n",
      "Iteration: 1670, Loss:2.55 \n",
      "Iteration: 1671, Loss:2.66 \n",
      "Iteration: 1672, Loss:2.33 \n",
      "Iteration: 1673, Loss:2.47 \n",
      "Iteration: 1674, Loss:2.41 \n",
      "Iteration: 1675, Loss:2.77 \n",
      "Iteration: 1676, Loss:2.77 \n",
      "Iteration: 1677, Loss:2.41 \n",
      "Iteration: 1678, Loss:2.88 \n",
      "Iteration: 1679, Loss:2.32 \n",
      "Iteration: 1680, Loss:2.57 \n",
      "Iteration: 1681, Loss:2.45 \n",
      "Iteration: 1682, Loss:2.44 \n",
      "Iteration: 1683, Loss:2.39 \n",
      "Iteration: 1684, Loss:2.42 \n",
      "Iteration: 1685, Loss:2.81 \n",
      "Iteration: 1686, Loss:2.60 \n",
      "Iteration: 1687, Loss:3.06 \n",
      "Iteration: 1688, Loss:2.45 \n",
      "Iteration: 1689, Loss:2.34 \n",
      "Iteration: 1690, Loss:2.82 \n",
      "Iteration: 1691, Loss:2.80 \n",
      "Iteration: 1692, Loss:2.42 \n",
      "Iteration: 1693, Loss:2.61 \n",
      "Iteration: 1694, Loss:2.56 \n",
      "Iteration: 1695, Loss:2.28 \n",
      "Iteration: 1696, Loss:2.46 \n",
      "Iteration: 1697, Loss:3.01 \n",
      "Iteration: 1698, Loss:2.39 \n",
      "Iteration: 1699, Loss:2.46 \n",
      "Iteration: 1700, Loss:2.38 \n",
      "Iteration: 1701, Loss:2.37 \n",
      "Iteration: 1702, Loss:2.39 \n",
      "Iteration: 1703, Loss:2.54 \n",
      "Iteration: 1704, Loss:2.46 \n",
      "Iteration: 1705, Loss:2.63 \n",
      "Iteration: 1706, Loss:2.48 \n",
      "Iteration: 1707, Loss:2.46 \n",
      "Iteration: 1708, Loss:2.75 \n",
      "Iteration: 1709, Loss:2.63 \n",
      "Iteration: 1710, Loss:2.26 \n",
      "Iteration: 1711, Loss:2.83 \n",
      "Iteration: 1712, Loss:2.62 \n",
      "Iteration: 1713, Loss:2.35 \n",
      "Iteration: 1714, Loss:2.55 \n",
      "Iteration: 1715, Loss:2.81 \n",
      "Iteration: 1716, Loss:2.61 \n",
      "Iteration: 1717, Loss:2.68 \n",
      "Iteration: 1718, Loss:2.70 \n",
      "Iteration: 1719, Loss:3.06 \n",
      "Iteration: 1720, Loss:2.57 \n",
      "Iteration: 1721, Loss:2.48 \n",
      "Iteration: 1722, Loss:2.30 \n",
      "Iteration: 1723, Loss:2.55 \n",
      "Iteration: 1724, Loss:2.88 \n",
      "Iteration: 1725, Loss:2.56 \n",
      "Iteration: 1726, Loss:2.38 \n",
      "Iteration: 1727, Loss:2.87 \n",
      "Iteration: 1728, Loss:2.55 \n",
      "Iteration: 1729, Loss:2.47 \n",
      "Iteration: 1730, Loss:2.47 \n",
      "Iteration: 1731, Loss:2.60 \n",
      "Iteration: 1732, Loss:2.45 \n",
      "Iteration: 1733, Loss:2.74 \n",
      "Iteration: 1734, Loss:2.10 \n",
      "Iteration: 1735, Loss:2.31 \n",
      "Iteration: 1736, Loss:2.84 \n",
      "Iteration: 1737, Loss:2.65 \n",
      "Iteration: 1738, Loss:2.37 \n",
      "Iteration: 1739, Loss:2.14 \n",
      "Iteration: 1740, Loss:2.58 \n",
      "Iteration: 1741, Loss:2.79 \n",
      "Iteration: 1742, Loss:2.62 \n",
      "Iteration: 1743, Loss:2.90 \n",
      "Iteration: 1744, Loss:2.74 \n",
      "Iteration: 1745, Loss:2.17 \n",
      "Iteration: 1746, Loss:2.54 \n",
      "Iteration: 1747, Loss:2.69 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1748, Loss:2.23 \n",
      "Iteration: 1749, Loss:2.58 \n",
      "Iteration: 1750, Loss:2.24 \n",
      "Iteration: 1751, Loss:2.34 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_1750.ckpt\n",
      "Iteration: 1752, Loss:2.38 \n",
      "Iteration: 1753, Loss:2.00 \n",
      "Iteration: 1754, Loss:2.77 \n",
      "Iteration: 1755, Loss:2.51 \n",
      "Iteration: 1756, Loss:2.51 \n",
      "Iteration: 1757, Loss:2.90 \n",
      "Iteration: 1758, Loss:2.81 \n",
      "Iteration: 1759, Loss:2.59 \n",
      "Iteration: 1760, Loss:2.61 \n",
      "Iteration: 1761, Loss:2.01 \n",
      "Iteration: 1762, Loss:2.30 \n",
      "Iteration: 1763, Loss:2.40 \n",
      "Iteration: 1764, Loss:2.28 \n",
      "Iteration: 1765, Loss:2.54 \n",
      "Iteration: 1766, Loss:2.59 \n",
      "Iteration: 1767, Loss:2.65 \n",
      "Iteration: 1768, Loss:2.64 \n",
      "Iteration: 1769, Loss:2.64 \n",
      "Iteration: 1770, Loss:2.83 \n",
      "Iteration: 1771, Loss:2.74 \n",
      "Iteration: 1772, Loss:2.75 \n",
      "Iteration: 1773, Loss:2.59 \n",
      "Iteration: 1774, Loss:2.63 \n",
      "Iteration: 1775, Loss:2.31 \n",
      "Iteration: 1776, Loss:2.08 \n",
      "Iteration: 1777, Loss:2.44 \n",
      "Iteration: 1778, Loss:2.88 \n",
      "Iteration: 1779, Loss:2.61 \n",
      "Iteration: 1780, Loss:2.35 \n",
      "Iteration: 1781, Loss:2.82 \n",
      "Iteration: 1782, Loss:2.81 \n",
      "Iteration: 1783, Loss:2.65 \n",
      "Iteration: 1784, Loss:2.68 \n",
      "Iteration: 1785, Loss:2.41 \n",
      "Iteration: 1786, Loss:2.95 \n",
      "Iteration: 1787, Loss:2.10 \n",
      "Iteration: 1788, Loss:2.76 \n",
      "Iteration: 1789, Loss:2.77 \n",
      "Iteration: 1790, Loss:2.41 \n",
      "Iteration: 1791, Loss:2.73 \n",
      "Iteration: 1792, Loss:2.44 \n",
      "Iteration: 1793, Loss:2.47 \n",
      "Iteration: 1794, Loss:2.44 \n",
      "Iteration: 1795, Loss:2.63 \n",
      "Iteration: 1796, Loss:2.68 \n",
      "Iteration: 1797, Loss:2.09 \n",
      "Iteration: 1798, Loss:2.84 \n",
      "Iteration: 1799, Loss:2.45 \n",
      "Iteration: 1800, Loss:2.71 \n",
      "Iteration: 1801, Loss:2.83 \n",
      "Iteration: 1802, Loss:2.48 \n",
      "Iteration: 1803, Loss:2.99 \n",
      "Iteration: 1804, Loss:2.75 \n",
      "Iteration: 1805, Loss:2.51 \n",
      "Iteration: 1806, Loss:2.54 \n",
      "Iteration: 1807, Loss:2.51 \n",
      "Iteration: 1808, Loss:1.86 \n",
      "Iteration: 1809, Loss:2.50 \n",
      "Iteration: 1810, Loss:2.59 \n",
      "Iteration: 1811, Loss:2.38 \n",
      "Iteration: 1812, Loss:2.49 \n",
      "Iteration: 1813, Loss:2.79 \n",
      "Iteration: 1814, Loss:2.44 \n",
      "Iteration: 1815, Loss:2.95 \n",
      "Iteration: 1816, Loss:2.40 \n",
      "Iteration: 1817, Loss:2.75 \n",
      "Iteration: 1818, Loss:2.37 \n",
      "Iteration: 1819, Loss:2.71 \n",
      "Iteration: 1820, Loss:2.59 \n",
      "Iteration: 1821, Loss:2.41 \n",
      "Iteration: 1822, Loss:2.23 \n",
      "Iteration: 1823, Loss:2.27 \n",
      "Iteration: 1824, Loss:2.34 \n",
      "Iteration: 1825, Loss:2.74 \n",
      "Iteration: 1826, Loss:2.44 \n",
      "Iteration: 1827, Loss:2.62 \n",
      "Iteration: 1828, Loss:3.16 \n",
      "Iteration: 1829, Loss:2.68 \n",
      "Iteration: 1830, Loss:2.25 \n",
      "Iteration: 1831, Loss:2.83 \n",
      "Iteration: 1832, Loss:2.58 \n",
      "Iteration: 1833, Loss:2.52 \n",
      "Iteration: 1834, Loss:2.63 \n",
      "Iteration: 1835, Loss:2.65 \n",
      "Iteration: 1836, Loss:2.66 \n",
      "Iteration: 1837, Loss:2.86 \n",
      "Iteration: 1838, Loss:2.67 \n",
      "Iteration: 1839, Loss:2.61 \n",
      "Iteration: 1840, Loss:2.07 \n",
      "Iteration: 1841, Loss:2.49 \n",
      "Iteration: 1842, Loss:2.68 \n",
      "Iteration: 1843, Loss:2.84 \n",
      "Iteration: 1844, Loss:2.77 \n",
      "Iteration: 1845, Loss:2.27 \n",
      "Iteration: 1846, Loss:2.84 \n",
      "Iteration: 1847, Loss:2.14 \n",
      "Iteration: 1848, Loss:2.69 \n",
      "Iteration: 1849, Loss:2.85 \n",
      "Iteration: 1850, Loss:2.85 \n",
      "Iteration: 1851, Loss:2.57 \n",
      "Iteration: 1852, Loss:2.65 \n",
      "Iteration: 1853, Loss:2.39 \n",
      "Iteration: 1854, Loss:2.35 \n",
      "Iteration: 1855, Loss:2.59 \n",
      "Iteration: 1856, Loss:2.21 \n",
      "Iteration: 1857, Loss:2.20 \n",
      "Iteration: 1858, Loss:2.44 \n",
      "Iteration: 1859, Loss:2.60 \n",
      "Iteration: 1860, Loss:2.71 \n",
      "Iteration: 1861, Loss:2.53 \n",
      "Iteration: 1862, Loss:2.21 \n",
      "Iteration: 1863, Loss:2.70 \n",
      "Iteration: 1864, Loss:2.44 \n",
      "Iteration: 1865, Loss:2.42 \n",
      "Iteration: 1866, Loss:2.41 \n",
      "Iteration: 1867, Loss:2.56 \n",
      "Iteration: 1868, Loss:2.53 \n",
      "Iteration: 1869, Loss:2.23 \n",
      "Iteration: 1870, Loss:2.56 \n",
      "Iteration: 1871, Loss:2.38 \n",
      "Iteration: 1872, Loss:2.41 \n",
      "Iteration: 1873, Loss:2.75 \n",
      "Iteration: 1874, Loss:2.64 \n",
      "Iteration: 1875, Loss:2.93 \n",
      "Iteration: 1876, Loss:2.40 \n",
      "Iteration: 1877, Loss:2.19 \n",
      "Iteration: 1878, Loss:2.43 \n",
      "Iteration: 1879, Loss:2.38 \n",
      "Iteration: 1880, Loss:2.58 \n",
      "Iteration: 1881, Loss:2.17 \n",
      "Iteration: 1882, Loss:3.05 \n",
      "Iteration: 1883, Loss:2.09 \n",
      "Iteration: 1884, Loss:2.20 \n",
      "Iteration: 1885, Loss:2.52 \n",
      "Iteration: 1886, Loss:2.57 \n",
      "Iteration: 1887, Loss:2.80 \n",
      "Iteration: 1888, Loss:2.46 \n",
      "Iteration: 1889, Loss:2.41 \n",
      "Iteration: 1890, Loss:2.41 \n",
      "Iteration: 1891, Loss:2.47 \n",
      "Iteration: 1892, Loss:2.28 \n",
      "Iteration: 1893, Loss:2.81 \n",
      "Iteration: 1894, Loss:2.15 \n",
      "Iteration: 1895, Loss:2.64 \n",
      "Iteration: 1896, Loss:2.42 \n",
      "Iteration: 1897, Loss:2.49 \n",
      "Iteration: 1898, Loss:2.46 \n",
      "Iteration: 1899, Loss:2.68 \n",
      "Iteration: 1900, Loss:2.69 \n",
      "Iteration: 1901, Loss:2.59 \n",
      "Iteration: 1902, Loss:2.59 \n",
      "Iteration: 1903, Loss:3.00 \n",
      "Iteration: 1904, Loss:2.52 \n",
      "Iteration: 1905, Loss:2.51 \n",
      "Iteration: 1906, Loss:2.76 \n",
      "Iteration: 1907, Loss:2.91 \n",
      "Iteration: 1908, Loss:2.72 \n",
      "Iteration: 1909, Loss:2.77 \n",
      "Iteration: 1910, Loss:2.74 \n",
      "Iteration: 1911, Loss:2.53 \n",
      "Iteration: 1912, Loss:2.88 \n",
      "Iteration: 1913, Loss:2.54 \n",
      "Iteration: 1914, Loss:2.87 \n",
      "Iteration: 1915, Loss:2.43 \n",
      "Iteration: 1916, Loss:2.33 \n",
      "Iteration: 1917, Loss:2.86 \n",
      "Iteration: 1918, Loss:2.30 \n",
      "Iteration: 1919, Loss:2.15 \n",
      "Iteration: 1920, Loss:2.55 \n",
      "Iteration: 1921, Loss:2.55 \n",
      "Iteration: 1922, Loss:1.99 \n",
      "Iteration: 1923, Loss:2.52 \n",
      "Iteration: 1924, Loss:2.60 \n",
      "Iteration: 1925, Loss:2.71 \n",
      "Iteration: 1926, Loss:2.69 \n",
      "Iteration: 1927, Loss:2.29 \n",
      "Iteration: 1928, Loss:2.70 \n",
      "Iteration: 1929, Loss:2.76 \n",
      "Iteration: 1930, Loss:2.58 \n",
      "Iteration: 1931, Loss:2.44 \n",
      "Iteration: 1932, Loss:2.45 \n",
      "Iteration: 1933, Loss:2.31 \n",
      "Iteration: 1934, Loss:2.78 \n",
      "Iteration: 1935, Loss:2.55 \n",
      "Iteration: 1936, Loss:2.34 \n",
      "Iteration: 1937, Loss:2.67 \n",
      "Iteration: 1938, Loss:2.38 \n",
      "Iteration: 1939, Loss:2.55 \n",
      "Iteration: 1940, Loss:1.89 \n",
      "Iteration: 1941, Loss:2.54 \n",
      "Iteration: 1942, Loss:2.56 \n",
      "Iteration: 1943, Loss:2.94 \n",
      "Iteration: 1944, Loss:2.62 \n",
      "Iteration: 1945, Loss:2.04 \n",
      "Iteration: 1946, Loss:2.69 \n",
      "Iteration: 1947, Loss:2.22 \n",
      "Iteration: 1948, Loss:2.52 \n",
      "Iteration: 1949, Loss:2.50 \n",
      "Iteration: 1950, Loss:2.99 \n",
      "Iteration: 1951, Loss:2.64 \n",
      "Iteration: 1952, Loss:2.40 \n",
      "Iteration: 1953, Loss:2.66 \n",
      "Iteration: 1954, Loss:2.39 \n",
      "Iteration: 1955, Loss:2.72 \n",
      "Iteration: 1956, Loss:2.53 \n",
      "Iteration: 1957, Loss:2.61 \n",
      "Iteration: 1958, Loss:2.68 \n",
      "Iteration: 1959, Loss:2.51 \n",
      "Iteration: 1960, Loss:2.22 \n",
      "Iteration: 1961, Loss:2.44 \n",
      "Iteration: 1962, Loss:2.35 \n",
      "Iteration: 1963, Loss:2.61 \n",
      "Iteration: 1964, Loss:2.38 \n",
      "Iteration: 1965, Loss:2.78 \n",
      "Iteration: 1966, Loss:3.03 \n",
      "Iteration: 1967, Loss:2.50 \n",
      "Iteration: 1968, Loss:2.41 \n",
      "Iteration: 1969, Loss:2.48 \n",
      "Iteration: 1970, Loss:2.17 \n",
      "Iteration: 1971, Loss:2.51 \n",
      "Iteration: 1972, Loss:2.62 \n",
      "Iteration: 1973, Loss:2.25 \n",
      "Iteration: 1974, Loss:2.38 \n",
      "Iteration: 1975, Loss:2.52 \n",
      "Iteration: 1976, Loss:2.37 \n",
      "Iteration: 1977, Loss:2.90 \n",
      "Iteration: 1978, Loss:2.39 \n",
      "Iteration: 1979, Loss:2.55 \n",
      "Iteration: 1980, Loss:2.73 \n",
      "Iteration: 1981, Loss:2.94 \n",
      "Iteration: 1982, Loss:2.41 \n",
      "Iteration: 1983, Loss:2.67 \n",
      "Iteration: 1984, Loss:2.97 \n",
      "Iteration: 1985, Loss:2.51 \n",
      "Iteration: 1986, Loss:2.22 \n",
      "Iteration: 1987, Loss:2.44 \n",
      "Iteration: 1988, Loss:2.93 \n",
      "Iteration: 1989, Loss:2.68 \n",
      "Iteration: 1990, Loss:2.53 \n",
      "Iteration: 1991, Loss:2.41 \n",
      "Iteration: 1992, Loss:2.95 \n",
      "Iteration: 1993, Loss:2.74 \n",
      "Iteration: 1994, Loss:2.28 \n",
      "Iteration: 1995, Loss:2.58 \n",
      "Iteration: 1996, Loss:2.54 \n",
      "Iteration: 1997, Loss:2.43 \n",
      "Iteration: 1998, Loss:2.48 \n",
      "Iteration: 1999, Loss:2.28 \n",
      "Iteration: 2000, Loss:2.28 \n",
      "Iteration: 2001, Loss:2.80 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_2000.ckpt\n",
      "Iteration: 2002, Loss:2.19 \n",
      "Iteration: 2003, Loss:2.45 \n",
      "Iteration: 2004, Loss:2.80 \n",
      "Iteration: 2005, Loss:2.48 \n",
      "Iteration: 2006, Loss:2.38 \n",
      "Iteration: 2007, Loss:2.28 \n",
      "Iteration: 2008, Loss:2.76 \n",
      "Iteration: 2009, Loss:2.37 \n",
      "Iteration: 2010, Loss:2.51 \n",
      "Iteration: 2011, Loss:2.00 \n",
      "Iteration: 2012, Loss:2.24 \n",
      "Iteration: 2013, Loss:2.49 \n",
      "Iteration: 2014, Loss:2.52 \n",
      "Iteration: 2015, Loss:2.30 \n",
      "Iteration: 2016, Loss:2.67 \n",
      "Iteration: 2017, Loss:2.97 \n",
      "Iteration: 2018, Loss:2.59 \n",
      "Iteration: 2019, Loss:2.35 \n",
      "Iteration: 2020, Loss:2.52 \n",
      "Iteration: 2021, Loss:2.38 \n",
      "Iteration: 2022, Loss:2.82 \n",
      "Iteration: 2023, Loss:2.86 \n",
      "Iteration: 2024, Loss:2.48 \n",
      "Iteration: 2025, Loss:2.71 \n",
      "Iteration: 2026, Loss:3.01 \n",
      "Iteration: 2027, Loss:2.53 \n",
      "Iteration: 2028, Loss:2.43 \n",
      "Iteration: 2029, Loss:2.86 \n",
      "Iteration: 2030, Loss:2.81 \n",
      "Iteration: 2031, Loss:2.76 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2032, Loss:2.25 \n",
      "Iteration: 2033, Loss:2.35 \n",
      "Iteration: 2034, Loss:2.57 \n",
      "Iteration: 2035, Loss:2.66 \n",
      "Iteration: 2036, Loss:2.50 \n",
      "Iteration: 2037, Loss:2.47 \n",
      "Iteration: 2038, Loss:2.55 \n",
      "Iteration: 2039, Loss:2.53 \n",
      "Iteration: 2040, Loss:2.39 \n",
      "Iteration: 2041, Loss:2.74 \n",
      "Iteration: 2042, Loss:1.99 \n",
      "Iteration: 2043, Loss:2.44 \n",
      "Iteration: 2044, Loss:2.97 \n",
      "Iteration: 2045, Loss:2.56 \n",
      "Iteration: 2046, Loss:2.14 \n",
      "Iteration: 2047, Loss:2.48 \n",
      "Iteration: 2048, Loss:2.79 \n",
      "Iteration: 2049, Loss:2.51 \n",
      "Iteration: 2050, Loss:2.31 \n",
      "Iteration: 2051, Loss:2.29 \n",
      "Iteration: 2052, Loss:2.29 \n",
      "Iteration: 2053, Loss:2.93 \n",
      "Iteration: 2054, Loss:2.72 \n",
      "Iteration: 2055, Loss:2.39 \n",
      "Iteration: 2056, Loss:2.33 \n",
      "Iteration: 2057, Loss:2.78 \n",
      "Iteration: 2058, Loss:2.58 \n",
      "Iteration: 2059, Loss:2.89 \n",
      "Iteration: 2060, Loss:2.30 \n",
      "Iteration: 2061, Loss:2.57 \n",
      "Iteration: 2062, Loss:2.48 \n",
      "Iteration: 2063, Loss:2.56 \n",
      "Iteration: 2064, Loss:2.78 \n",
      "Iteration: 2065, Loss:2.31 \n",
      "Iteration: 2066, Loss:2.59 \n",
      "Iteration: 2067, Loss:2.47 \n",
      "Iteration: 2068, Loss:2.13 \n",
      "Iteration: 2069, Loss:2.64 \n",
      "Iteration: 2070, Loss:2.64 \n",
      "Iteration: 2071, Loss:2.65 \n",
      "Iteration: 2072, Loss:2.81 \n",
      "Iteration: 2073, Loss:2.50 \n",
      "Iteration: 2074, Loss:2.71 \n",
      "Iteration: 2075, Loss:2.39 \n",
      "Iteration: 2076, Loss:2.95 \n",
      "Iteration: 2077, Loss:2.96 \n",
      "Iteration: 2078, Loss:2.63 \n",
      "Iteration: 2079, Loss:2.45 \n",
      "Iteration: 2080, Loss:2.79 \n",
      "Iteration: 2081, Loss:2.33 \n",
      "Iteration: 2082, Loss:2.60 \n",
      "Iteration: 2083, Loss:2.56 \n",
      "Iteration: 2084, Loss:2.52 \n",
      "Iteration: 2085, Loss:2.27 \n",
      "Iteration: 2086, Loss:2.44 \n",
      "Iteration: 2087, Loss:2.24 \n",
      "Iteration: 2088, Loss:2.64 \n",
      "Iteration: 2089, Loss:2.59 \n",
      "Iteration: 2090, Loss:2.94 \n",
      "Iteration: 2091, Loss:2.50 \n",
      "Iteration: 2092, Loss:2.61 \n",
      "Iteration: 2093, Loss:2.40 \n",
      "Iteration: 2094, Loss:2.16 \n",
      "Iteration: 2095, Loss:2.84 \n",
      "Iteration: 2096, Loss:2.74 \n",
      "Iteration: 2097, Loss:2.56 \n",
      "Iteration: 2098, Loss:2.43 \n",
      "Iteration: 2099, Loss:2.64 \n",
      "Iteration: 2100, Loss:2.63 \n",
      "Iteration: 2101, Loss:2.59 \n",
      "Iteration: 2102, Loss:2.48 \n",
      "Iteration: 2103, Loss:2.23 \n",
      "Iteration: 2104, Loss:2.76 \n",
      "Iteration: 2105, Loss:2.39 \n",
      "Iteration: 2106, Loss:2.52 \n",
      "Iteration: 2107, Loss:2.79 \n",
      "Iteration: 2108, Loss:2.31 \n",
      "Iteration: 2109, Loss:2.70 \n",
      "Iteration: 2110, Loss:2.71 \n",
      "Iteration: 2111, Loss:2.48 \n",
      "Iteration: 2112, Loss:2.55 \n",
      "Iteration: 2113, Loss:2.80 \n",
      "Iteration: 2114, Loss:2.42 \n",
      "Iteration: 2115, Loss:2.17 \n",
      "Iteration: 2116, Loss:2.12 \n",
      "Iteration: 2117, Loss:2.59 \n",
      "Iteration: 2118, Loss:2.86 \n",
      "Iteration: 2119, Loss:2.73 \n",
      "Iteration: 2120, Loss:2.03 \n",
      "Iteration: 2121, Loss:2.20 \n",
      "Iteration: 2122, Loss:2.69 \n",
      "Iteration: 2123, Loss:2.50 \n",
      "Iteration: 2124, Loss:2.70 \n",
      "Iteration: 2125, Loss:2.62 \n",
      "Iteration: 2126, Loss:2.54 \n",
      "Iteration: 2127, Loss:2.33 \n",
      "Iteration: 2128, Loss:2.64 \n",
      "Iteration: 2129, Loss:2.65 \n",
      "Iteration: 2130, Loss:2.73 \n",
      "Iteration: 2131, Loss:2.36 \n",
      "Iteration: 2132, Loss:2.86 \n",
      "Iteration: 2133, Loss:2.24 \n",
      "Iteration: 2134, Loss:2.85 \n",
      "Iteration: 2135, Loss:2.93 \n",
      "Iteration: 2136, Loss:2.19 \n",
      "Iteration: 2137, Loss:2.61 \n",
      "Iteration: 2138, Loss:2.43 \n",
      "Iteration: 2139, Loss:2.41 \n",
      "Iteration: 2140, Loss:2.50 \n",
      "Iteration: 2141, Loss:2.43 \n",
      "Iteration: 2142, Loss:2.63 \n",
      "Iteration: 2143, Loss:3.04 \n",
      "Iteration: 2144, Loss:2.26 \n",
      "Iteration: 2145, Loss:2.36 \n",
      "Iteration: 2146, Loss:2.77 \n",
      "Iteration: 2147, Loss:2.39 \n",
      "Iteration: 2148, Loss:2.42 \n",
      "Iteration: 2149, Loss:2.42 \n",
      "Iteration: 2150, Loss:2.60 \n",
      "Iteration: 2151, Loss:2.36 \n",
      "Iteration: 2152, Loss:2.65 \n",
      "Iteration: 2153, Loss:2.94 \n",
      "Iteration: 2154, Loss:2.64 \n",
      "Iteration: 2155, Loss:2.32 \n",
      "Iteration: 2156, Loss:2.81 \n",
      "Iteration: 2157, Loss:2.36 \n",
      "Iteration: 2158, Loss:2.35 \n",
      "Iteration: 2159, Loss:2.26 \n",
      "Iteration: 2160, Loss:2.07 \n",
      "Iteration: 2161, Loss:2.98 \n",
      "Iteration: 2162, Loss:2.67 \n",
      "Iteration: 2163, Loss:2.49 \n",
      "Iteration: 2164, Loss:2.69 \n",
      "Iteration: 2165, Loss:2.34 \n",
      "Iteration: 2166, Loss:2.28 \n",
      "Iteration: 2167, Loss:2.55 \n",
      "Iteration: 2168, Loss:2.65 \n",
      "Iteration: 2169, Loss:2.50 \n",
      "Iteration: 2170, Loss:2.31 \n",
      "Iteration: 2171, Loss:2.68 \n",
      "Iteration: 2172, Loss:2.29 \n",
      "Iteration: 2173, Loss:2.75 \n",
      "Iteration: 2174, Loss:2.69 \n",
      "Iteration: 2175, Loss:2.50 \n",
      "Iteration: 2176, Loss:2.52 \n",
      "Iteration: 2177, Loss:2.65 \n",
      "Iteration: 2178, Loss:2.19 \n",
      "Iteration: 2179, Loss:2.34 \n",
      "Iteration: 2180, Loss:2.79 \n",
      "Iteration: 2181, Loss:2.57 \n",
      "Iteration: 2182, Loss:2.32 \n",
      "Iteration: 2183, Loss:3.06 \n",
      "Iteration: 2184, Loss:2.14 \n",
      "Iteration: 2185, Loss:2.34 \n",
      "Iteration: 2186, Loss:2.88 \n",
      "Iteration: 2187, Loss:2.51 \n",
      "Iteration: 2188, Loss:2.24 \n",
      "Iteration: 2189, Loss:2.47 \n",
      "Iteration: 2190, Loss:2.30 \n",
      "Iteration: 2191, Loss:2.01 \n",
      "Iteration: 2192, Loss:2.72 \n",
      "Iteration: 2193, Loss:3.09 \n",
      "Iteration: 2194, Loss:2.47 \n",
      "Iteration: 2195, Loss:2.81 \n",
      "Iteration: 2196, Loss:2.80 \n",
      "Iteration: 2197, Loss:2.49 \n",
      "Iteration: 2198, Loss:2.60 \n",
      "Iteration: 2199, Loss:2.85 \n",
      "Iteration: 2200, Loss:2.73 \n",
      "Iteration: 2201, Loss:2.62 \n",
      "Iteration: 2202, Loss:2.54 \n",
      "Iteration: 2203, Loss:1.85 \n",
      "Iteration: 2204, Loss:2.78 \n",
      "Iteration: 2205, Loss:2.24 \n",
      "Iteration: 2206, Loss:2.65 \n",
      "Iteration: 2207, Loss:2.82 \n",
      "Iteration: 2208, Loss:2.44 \n",
      "Iteration: 2209, Loss:2.88 \n",
      "Iteration: 2210, Loss:2.27 \n",
      "Iteration: 2211, Loss:2.31 \n",
      "Iteration: 2212, Loss:2.37 \n",
      "Iteration: 2213, Loss:2.35 \n",
      "Iteration: 2214, Loss:2.37 \n",
      "Iteration: 2215, Loss:2.34 \n",
      "Iteration: 2216, Loss:2.43 \n",
      "Iteration: 2217, Loss:2.37 \n",
      "Iteration: 2218, Loss:2.72 \n",
      "Iteration: 2219, Loss:2.33 \n",
      "Iteration: 2220, Loss:2.77 \n",
      "Iteration: 2221, Loss:2.51 \n",
      "Iteration: 2222, Loss:2.65 \n",
      "Iteration: 2223, Loss:2.68 \n",
      "Iteration: 2224, Loss:2.27 \n",
      "Iteration: 2225, Loss:2.55 \n",
      "Iteration: 2226, Loss:1.98 \n",
      "Iteration: 2227, Loss:2.40 \n",
      "Iteration: 2228, Loss:2.90 \n",
      "Iteration: 2229, Loss:2.94 \n",
      "Iteration: 2230, Loss:2.33 \n",
      "Iteration: 2231, Loss:2.42 \n",
      "Iteration: 2232, Loss:2.88 \n",
      "Iteration: 2233, Loss:2.76 \n",
      "Iteration: 2234, Loss:2.62 \n",
      "Iteration: 2235, Loss:2.78 \n",
      "Iteration: 2236, Loss:2.33 \n",
      "Iteration: 2237, Loss:2.71 \n",
      "Iteration: 2238, Loss:2.35 \n",
      "Iteration: 2239, Loss:2.65 \n",
      "Iteration: 2240, Loss:2.33 \n",
      "Iteration: 2241, Loss:2.45 \n",
      "Iteration: 2242, Loss:3.00 \n",
      "Iteration: 2243, Loss:2.73 \n",
      "Iteration: 2244, Loss:2.40 \n",
      "Iteration: 2245, Loss:2.41 \n",
      "Iteration: 2246, Loss:2.88 \n",
      "Iteration: 2247, Loss:2.63 \n",
      "Iteration: 2248, Loss:2.79 \n",
      "Iteration: 2249, Loss:2.40 \n",
      "Iteration: 2250, Loss:2.65 \n",
      "Iteration: 2251, Loss:2.64 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_2250.ckpt\n",
      "Iteration: 2252, Loss:3.15 \n",
      "Iteration: 2253, Loss:2.50 \n",
      "Iteration: 2254, Loss:2.71 \n",
      "Iteration: 2255, Loss:2.98 \n",
      "Iteration: 2256, Loss:2.61 \n",
      "Iteration: 2257, Loss:2.46 \n",
      "Iteration: 2258, Loss:2.05 \n",
      "Iteration: 2259, Loss:2.57 \n",
      "Iteration: 2260, Loss:2.85 \n",
      "Iteration: 2261, Loss:2.26 \n",
      "Iteration: 2262, Loss:2.50 \n",
      "Iteration: 2263, Loss:2.40 \n",
      "Iteration: 2264, Loss:2.11 \n",
      "Iteration: 2265, Loss:2.64 \n",
      "Iteration: 2266, Loss:3.03 \n",
      "Iteration: 2267, Loss:2.76 \n",
      "Iteration: 2268, Loss:2.55 \n",
      "Iteration: 2269, Loss:2.75 \n",
      "Iteration: 2270, Loss:2.52 \n",
      "Iteration: 2271, Loss:2.68 \n",
      "Iteration: 2272, Loss:2.51 \n",
      "Iteration: 2273, Loss:2.32 \n",
      "Iteration: 2274, Loss:2.80 \n",
      "Iteration: 2275, Loss:2.65 \n",
      "Iteration: 2276, Loss:2.43 \n",
      "Iteration: 2277, Loss:2.75 \n",
      "Iteration: 2278, Loss:2.18 \n",
      "Iteration: 2279, Loss:2.87 \n",
      "Iteration: 2280, Loss:2.41 \n",
      "Iteration: 2281, Loss:2.46 \n",
      "Iteration: 2282, Loss:2.52 \n",
      "Iteration: 2283, Loss:2.44 \n",
      "Iteration: 2284, Loss:2.68 \n",
      "Iteration: 2285, Loss:2.59 \n",
      "Iteration: 2286, Loss:2.38 \n",
      "Iteration: 2287, Loss:2.68 \n",
      "Iteration: 2288, Loss:2.57 \n",
      "Iteration: 2289, Loss:2.94 \n",
      "Iteration: 2290, Loss:2.44 \n",
      "Iteration: 2291, Loss:2.86 \n",
      "Iteration: 2292, Loss:2.87 \n",
      "Iteration: 2293, Loss:2.10 \n",
      "Iteration: 2294, Loss:2.20 \n",
      "Iteration: 2295, Loss:2.72 \n",
      "Iteration: 2296, Loss:2.41 \n",
      "Iteration: 2297, Loss:2.42 \n",
      "Iteration: 2298, Loss:2.47 \n",
      "Iteration: 2299, Loss:2.64 \n",
      "Iteration: 2300, Loss:2.65 \n",
      "Iteration: 2301, Loss:2.47 \n",
      "Iteration: 2302, Loss:2.37 \n",
      "Iteration: 2303, Loss:2.40 \n",
      "Iteration: 2304, Loss:2.48 \n",
      "Iteration: 2305, Loss:2.61 \n",
      "Iteration: 2306, Loss:2.78 \n",
      "Iteration: 2307, Loss:2.74 \n",
      "Iteration: 2308, Loss:2.06 \n",
      "Iteration: 2309, Loss:2.39 \n",
      "Iteration: 2310, Loss:2.33 \n",
      "Iteration: 2311, Loss:2.52 \n",
      "Iteration: 2312, Loss:2.67 \n",
      "Iteration: 2313, Loss:2.36 \n",
      "Iteration: 2314, Loss:2.42 \n",
      "Iteration: 2315, Loss:2.01 \n",
      "Iteration: 2316, Loss:2.62 \n",
      "Iteration: 2317, Loss:2.72 \n",
      "Iteration: 2318, Loss:2.62 \n",
      "Iteration: 2319, Loss:2.66 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2320, Loss:2.14 \n",
      "Iteration: 2321, Loss:2.18 \n",
      "Iteration: 2322, Loss:2.56 \n",
      "Iteration: 2323, Loss:2.85 \n",
      "Iteration: 2324, Loss:2.55 \n",
      "Iteration: 2325, Loss:2.88 \n",
      "Iteration: 2326, Loss:2.18 \n",
      "Iteration: 2327, Loss:2.75 \n",
      "Iteration: 2328, Loss:2.48 \n",
      "Iteration: 2329, Loss:2.39 \n",
      "Iteration: 2330, Loss:2.63 \n",
      "Iteration: 2331, Loss:2.65 \n",
      "Iteration: 2332, Loss:2.34 \n",
      "Iteration: 2333, Loss:2.84 \n",
      "Iteration: 2334, Loss:2.86 \n",
      "Iteration: 2335, Loss:2.40 \n",
      "Iteration: 2336, Loss:2.29 \n",
      "Iteration: 2337, Loss:2.55 \n",
      "Iteration: 2338, Loss:2.54 \n",
      "Iteration: 2339, Loss:2.61 \n",
      "Iteration: 2340, Loss:2.64 \n",
      "Iteration: 2341, Loss:2.37 \n",
      "Iteration: 2342, Loss:2.79 \n",
      "Iteration: 2343, Loss:2.49 \n",
      "Iteration: 2344, Loss:2.50 \n",
      "Iteration: 2345, Loss:2.33 \n",
      "Iteration: 2346, Loss:2.72 \n",
      "Iteration: 2347, Loss:2.70 \n",
      "Iteration: 2348, Loss:2.70 \n",
      "Iteration: 2349, Loss:2.49 \n",
      "Iteration: 2350, Loss:2.12 \n",
      "Iteration: 2351, Loss:2.68 \n",
      "Iteration: 2352, Loss:2.69 \n",
      "Iteration: 2353, Loss:2.02 \n",
      "Iteration: 2354, Loss:2.36 \n",
      "Iteration: 2355, Loss:2.48 \n",
      "Iteration: 2356, Loss:2.72 \n",
      "Iteration: 2357, Loss:2.87 \n",
      "Iteration: 2358, Loss:2.55 \n",
      "Iteration: 2359, Loss:2.62 \n",
      "Iteration: 2360, Loss:2.52 \n",
      "Iteration: 2361, Loss:3.06 \n",
      "Iteration: 2362, Loss:2.46 \n",
      "Iteration: 2363, Loss:2.57 \n",
      "Iteration: 2364, Loss:2.57 \n",
      "Iteration: 2365, Loss:2.04 \n",
      "Iteration: 2366, Loss:2.59 \n",
      "Iteration: 2367, Loss:2.70 \n",
      "Iteration: 2368, Loss:2.11 \n",
      "Iteration: 2369, Loss:2.82 \n",
      "Iteration: 2370, Loss:2.72 \n",
      "Iteration: 2371, Loss:2.92 \n",
      "Iteration: 2372, Loss:2.42 \n",
      "Iteration: 2373, Loss:2.46 \n",
      "Iteration: 2374, Loss:2.61 \n",
      "Iteration: 2375, Loss:2.66 \n",
      "Iteration: 2376, Loss:2.65 \n",
      "Iteration: 2377, Loss:2.40 \n",
      "Iteration: 2378, Loss:2.73 \n",
      "Iteration: 2379, Loss:2.14 \n",
      "Iteration: 2380, Loss:2.50 \n",
      "Iteration: 2381, Loss:2.49 \n",
      "Iteration: 2382, Loss:2.44 \n",
      "Iteration: 2383, Loss:2.78 \n",
      "Iteration: 2384, Loss:2.78 \n",
      "Iteration: 2385, Loss:2.57 \n",
      "Iteration: 2386, Loss:2.27 \n",
      "Iteration: 2387, Loss:2.74 \n",
      "Iteration: 2388, Loss:2.58 \n",
      "Iteration: 2389, Loss:2.53 \n",
      "Iteration: 2390, Loss:2.50 \n",
      "Iteration: 2391, Loss:2.49 \n",
      "Iteration: 2392, Loss:2.50 \n",
      "Iteration: 2393, Loss:2.77 \n",
      "Iteration: 2394, Loss:1.89 \n",
      "Iteration: 2395, Loss:2.49 \n",
      "Iteration: 2396, Loss:2.57 \n",
      "Iteration: 2397, Loss:2.80 \n",
      "Iteration: 2398, Loss:2.48 \n",
      "Iteration: 2399, Loss:2.91 \n",
      "Iteration: 2400, Loss:2.29 \n",
      "Iteration: 2401, Loss:2.29 \n",
      "Iteration: 2402, Loss:2.74 \n",
      "Iteration: 2403, Loss:2.49 \n",
      "Iteration: 2404, Loss:2.69 \n",
      "Iteration: 2405, Loss:2.28 \n",
      "Iteration: 2406, Loss:2.70 \n",
      "Iteration: 2407, Loss:2.42 \n",
      "Iteration: 2408, Loss:2.68 \n",
      "Iteration: 2409, Loss:2.51 \n",
      "Iteration: 2410, Loss:2.47 \n",
      "Iteration: 2411, Loss:2.03 \n",
      "Iteration: 2412, Loss:2.76 \n",
      "Iteration: 2413, Loss:2.14 \n",
      "Iteration: 2414, Loss:2.86 \n",
      "Iteration: 2415, Loss:2.43 \n",
      "Iteration: 2416, Loss:2.48 \n",
      "Iteration: 2417, Loss:2.45 \n",
      "Iteration: 2418, Loss:2.85 \n",
      "Iteration: 2419, Loss:2.85 \n",
      "Iteration: 2420, Loss:2.56 \n",
      "Iteration: 2421, Loss:2.26 \n",
      "Iteration: 2422, Loss:2.58 \n",
      "Iteration: 2423, Loss:2.65 \n",
      "Iteration: 2424, Loss:2.58 \n",
      "Iteration: 2425, Loss:2.37 \n",
      "Iteration: 2426, Loss:3.06 \n",
      "Iteration: 2427, Loss:2.19 \n",
      "Iteration: 2428, Loss:2.47 \n",
      "Iteration: 2429, Loss:2.65 \n",
      "Iteration: 2430, Loss:2.54 \n",
      "Iteration: 2431, Loss:2.71 \n",
      "Iteration: 2432, Loss:2.58 \n",
      "Iteration: 2433, Loss:2.80 \n",
      "Iteration: 2434, Loss:2.83 \n",
      "Iteration: 2435, Loss:2.80 \n",
      "Iteration: 2436, Loss:2.51 \n",
      "Iteration: 2437, Loss:2.55 \n",
      "Iteration: 2438, Loss:2.26 \n",
      "Iteration: 2439, Loss:2.43 \n",
      "Iteration: 2440, Loss:2.93 \n",
      "Iteration: 2441, Loss:1.97 \n",
      "Iteration: 2442, Loss:2.82 \n",
      "Iteration: 2443, Loss:2.68 \n",
      "Iteration: 2444, Loss:2.65 \n",
      "Iteration: 2445, Loss:2.68 \n",
      "Iteration: 2446, Loss:2.22 \n",
      "Iteration: 2447, Loss:2.27 \n",
      "Iteration: 2448, Loss:2.36 \n",
      "Iteration: 2449, Loss:2.99 \n",
      "Iteration: 2450, Loss:2.55 \n",
      "Iteration: 2451, Loss:2.84 \n",
      "Iteration: 2452, Loss:2.62 \n",
      "Iteration: 2453, Loss:2.81 \n",
      "Iteration: 2454, Loss:2.81 \n",
      "Iteration: 2455, Loss:2.39 \n",
      "Iteration: 2456, Loss:2.74 \n",
      "Iteration: 2457, Loss:2.48 \n",
      "Iteration: 2458, Loss:2.17 \n",
      "Iteration: 2459, Loss:2.61 \n",
      "Iteration: 2460, Loss:2.16 \n",
      "Iteration: 2461, Loss:2.48 \n",
      "Iteration: 2462, Loss:2.64 \n",
      "Iteration: 2463, Loss:2.51 \n",
      "Iteration: 2464, Loss:2.52 \n",
      "Iteration: 2465, Loss:2.39 \n",
      "Iteration: 2466, Loss:2.75 \n",
      "Iteration: 2467, Loss:2.23 \n",
      "Iteration: 2468, Loss:2.89 \n",
      "Iteration: 2469, Loss:2.17 \n",
      "Iteration: 2470, Loss:2.76 \n",
      "Iteration: 2471, Loss:2.56 \n",
      "Iteration: 2472, Loss:2.58 \n",
      "Iteration: 2473, Loss:2.26 \n",
      "Iteration: 2474, Loss:2.22 \n",
      "Iteration: 2475, Loss:2.65 \n",
      "Iteration: 2476, Loss:2.59 \n",
      "Iteration: 2477, Loss:2.93 \n",
      "Iteration: 2478, Loss:2.13 \n",
      "Iteration: 2479, Loss:2.65 \n",
      "Iteration: 2480, Loss:2.91 \n",
      "Iteration: 2481, Loss:2.52 \n",
      "Iteration: 2482, Loss:2.76 \n",
      "Iteration: 2483, Loss:2.22 \n",
      "Iteration: 2484, Loss:2.64 \n",
      "Iteration: 2485, Loss:2.79 \n",
      "Iteration: 2486, Loss:2.67 \n",
      "Iteration: 2487, Loss:2.61 \n",
      "Iteration: 2488, Loss:2.59 \n",
      "Iteration: 2489, Loss:2.10 \n",
      "Iteration: 2490, Loss:2.68 \n",
      "Iteration: 2491, Loss:2.54 \n",
      "Iteration: 2492, Loss:2.57 \n",
      "Iteration: 2493, Loss:2.54 \n",
      "Iteration: 2494, Loss:2.80 \n",
      "Iteration: 2495, Loss:2.32 \n",
      "Iteration: 2496, Loss:2.64 \n",
      "Iteration: 2497, Loss:1.95 \n",
      "Iteration: 2498, Loss:2.06 \n",
      "Iteration: 2499, Loss:2.71 \n",
      "Iteration: 2500, Loss:2.59 \n",
      "Iteration: 2501, Loss:2.71 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_2500.ckpt\n",
      "Iteration: 2502, Loss:2.67 \n",
      "Iteration: 2503, Loss:2.58 \n",
      "Iteration: 2504, Loss:2.51 \n",
      "Iteration: 2505, Loss:2.60 \n",
      "Iteration: 2506, Loss:2.76 \n",
      "Iteration: 2507, Loss:2.72 \n",
      "Iteration: 2508, Loss:2.20 \n",
      "Iteration: 2509, Loss:2.35 \n",
      "Iteration: 2510, Loss:2.22 \n",
      "Iteration: 2511, Loss:2.49 \n",
      "Iteration: 2512, Loss:2.74 \n",
      "Iteration: 2513, Loss:2.22 \n",
      "Iteration: 2514, Loss:2.77 \n",
      "Iteration: 2515, Loss:2.44 \n",
      "Iteration: 2516, Loss:2.65 \n",
      "Iteration: 2517, Loss:2.99 \n",
      "Iteration: 2518, Loss:2.62 \n",
      "Iteration: 2519, Loss:2.25 \n",
      "Iteration: 2520, Loss:2.47 \n",
      "Iteration: 2521, Loss:2.54 \n",
      "Iteration: 2522, Loss:2.71 \n",
      "Iteration: 2523, Loss:2.74 \n",
      "Iteration: 2524, Loss:2.64 \n",
      "Iteration: 2525, Loss:2.83 \n",
      "Iteration: 2526, Loss:2.56 \n",
      "Iteration: 2527, Loss:2.39 \n",
      "Iteration: 2528, Loss:2.96 \n",
      "Iteration: 2529, Loss:2.20 \n",
      "Iteration: 2530, Loss:2.38 \n",
      "Iteration: 2531, Loss:2.61 \n",
      "Iteration: 2532, Loss:2.83 \n",
      "Iteration: 2533, Loss:2.61 \n",
      "Iteration: 2534, Loss:1.87 \n",
      "Iteration: 2535, Loss:2.63 \n",
      "Iteration: 2536, Loss:2.52 \n",
      "Iteration: 2537, Loss:2.70 \n",
      "Iteration: 2538, Loss:2.07 \n",
      "Iteration: 2539, Loss:2.55 \n",
      "Iteration: 2540, Loss:2.71 \n",
      "Iteration: 2541, Loss:2.42 \n",
      "Iteration: 2542, Loss:2.66 \n",
      "Iteration: 2543, Loss:2.51 \n",
      "Iteration: 2544, Loss:2.71 \n",
      "Iteration: 2545, Loss:2.32 \n",
      "Iteration: 2546, Loss:2.30 \n",
      "Iteration: 2547, Loss:2.66 \n",
      "Iteration: 2548, Loss:2.49 \n",
      "Iteration: 2549, Loss:2.21 \n",
      "Iteration: 2550, Loss:2.38 \n",
      "Iteration: 2551, Loss:2.99 \n",
      "Iteration: 2552, Loss:2.44 \n",
      "Iteration: 2553, Loss:2.65 \n",
      "Iteration: 2554, Loss:2.54 \n",
      "Iteration: 2555, Loss:2.37 \n",
      "Iteration: 2556, Loss:2.59 \n",
      "Iteration: 2557, Loss:2.67 \n",
      "Iteration: 2558, Loss:3.02 \n",
      "Iteration: 2559, Loss:2.02 \n",
      "Iteration: 2560, Loss:2.43 \n",
      "Iteration: 2561, Loss:2.68 \n",
      "Iteration: 2562, Loss:2.37 \n",
      "Iteration: 2563, Loss:2.07 \n",
      "Iteration: 2564, Loss:2.74 \n",
      "Iteration: 2565, Loss:2.47 \n",
      "Iteration: 2566, Loss:2.24 \n",
      "Iteration: 2567, Loss:3.06 \n",
      "Iteration: 2568, Loss:2.72 \n",
      "Iteration: 2569, Loss:2.70 \n",
      "Iteration: 2570, Loss:2.49 \n",
      "Iteration: 2571, Loss:2.61 \n",
      "Iteration: 2572, Loss:2.67 \n",
      "Iteration: 2573, Loss:2.29 \n",
      "Iteration: 2574, Loss:2.40 \n",
      "Iteration: 2575, Loss:2.46 \n",
      "Iteration: 2576, Loss:2.83 \n",
      "Iteration: 2577, Loss:2.21 \n",
      "Iteration: 2578, Loss:2.42 \n",
      "Iteration: 2579, Loss:2.72 \n",
      "Iteration: 2580, Loss:2.38 \n",
      "Iteration: 2581, Loss:2.58 \n",
      "Iteration: 2582, Loss:2.94 \n",
      "Iteration: 2583, Loss:2.02 \n",
      "Iteration: 2584, Loss:2.31 \n",
      "Iteration: 2585, Loss:2.64 \n",
      "Iteration: 2586, Loss:2.19 \n",
      "Iteration: 2587, Loss:2.26 \n",
      "Iteration: 2588, Loss:2.35 \n",
      "Iteration: 2589, Loss:2.73 \n",
      "Iteration: 2590, Loss:2.84 \n",
      "Iteration: 2591, Loss:2.44 \n",
      "Iteration: 2592, Loss:2.35 \n",
      "Iteration: 2593, Loss:2.15 \n",
      "Iteration: 2594, Loss:2.31 \n",
      "Iteration: 2595, Loss:2.24 \n",
      "Iteration: 2596, Loss:2.37 \n",
      "Iteration: 2597, Loss:2.83 \n",
      "Iteration: 2598, Loss:2.71 \n",
      "Iteration: 2599, Loss:2.28 \n",
      "Iteration: 2600, Loss:2.71 \n",
      "Iteration: 2601, Loss:2.65 \n",
      "Iteration: 2602, Loss:2.41 \n",
      "Iteration: 2603, Loss:2.69 \n",
      "Iteration: 2604, Loss:2.01 \n",
      "Iteration: 2605, Loss:2.63 \n",
      "Iteration: 2606, Loss:2.65 \n",
      "Iteration: 2607, Loss:2.66 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2608, Loss:2.37 \n",
      "Iteration: 2609, Loss:2.94 \n",
      "Iteration: 2610, Loss:2.82 \n",
      "Iteration: 2611, Loss:2.43 \n",
      "Iteration: 2612, Loss:2.95 \n",
      "Iteration: 2613, Loss:2.71 \n",
      "Iteration: 2614, Loss:2.48 \n",
      "Iteration: 2615, Loss:3.07 \n",
      "Iteration: 2616, Loss:2.51 \n",
      "Iteration: 2617, Loss:2.27 \n",
      "Iteration: 2618, Loss:2.56 \n",
      "Iteration: 2619, Loss:2.60 \n",
      "Iteration: 2620, Loss:2.72 \n",
      "Iteration: 2621, Loss:2.76 \n",
      "Iteration: 2622, Loss:2.63 \n",
      "Iteration: 2623, Loss:2.47 \n",
      "Iteration: 2624, Loss:2.59 \n",
      "Iteration: 2625, Loss:2.35 \n",
      "Iteration: 2626, Loss:2.40 \n",
      "Iteration: 2627, Loss:2.55 \n",
      "Iteration: 2628, Loss:2.59 \n",
      "Iteration: 2629, Loss:2.86 \n",
      "Iteration: 2630, Loss:2.73 \n",
      "Iteration: 2631, Loss:2.63 \n",
      "Iteration: 2632, Loss:2.73 \n",
      "Iteration: 2633, Loss:2.41 \n",
      "Iteration: 2634, Loss:2.53 \n",
      "Iteration: 2635, Loss:2.70 \n",
      "Iteration: 2636, Loss:2.67 \n",
      "Iteration: 2637, Loss:2.63 \n",
      "Iteration: 2638, Loss:2.31 \n",
      "Iteration: 2639, Loss:2.72 \n",
      "Iteration: 2640, Loss:2.30 \n",
      "Iteration: 2641, Loss:2.64 \n",
      "Iteration: 2642, Loss:2.98 \n",
      "Iteration: 2643, Loss:2.46 \n",
      "Iteration: 2644, Loss:2.87 \n",
      "Iteration: 2645, Loss:2.21 \n",
      "Iteration: 2646, Loss:2.76 \n",
      "Iteration: 2647, Loss:2.44 \n",
      "Iteration: 2648, Loss:2.98 \n",
      "Iteration: 2649, Loss:2.84 \n",
      "Iteration: 2650, Loss:2.68 \n",
      "Iteration: 2651, Loss:2.64 \n",
      "Iteration: 2652, Loss:2.52 \n",
      "Iteration: 2653, Loss:2.31 \n",
      "Iteration: 2654, Loss:2.83 \n",
      "Iteration: 2655, Loss:2.21 \n",
      "Iteration: 2656, Loss:2.00 \n",
      "Iteration: 2657, Loss:2.55 \n",
      "Iteration: 2658, Loss:2.76 \n",
      "Iteration: 2659, Loss:2.42 \n",
      "Iteration: 2660, Loss:2.09 \n",
      "Iteration: 2661, Loss:2.21 \n",
      "Iteration: 2662, Loss:2.43 \n",
      "Iteration: 2663, Loss:2.43 \n",
      "Iteration: 2664, Loss:2.64 \n",
      "Iteration: 2665, Loss:2.40 \n",
      "Iteration: 2666, Loss:2.66 \n",
      "Iteration: 2667, Loss:2.93 \n",
      "Iteration: 2668, Loss:2.65 \n",
      "Iteration: 2669, Loss:2.38 \n",
      "Iteration: 2670, Loss:2.93 \n",
      "Iteration: 2671, Loss:2.22 \n",
      "Iteration: 2672, Loss:2.50 \n",
      "Iteration: 2673, Loss:2.97 \n",
      "Iteration: 2674, Loss:2.75 \n",
      "Iteration: 2675, Loss:2.85 \n",
      "Iteration: 2676, Loss:2.79 \n",
      "Iteration: 2677, Loss:2.51 \n",
      "Iteration: 2678, Loss:2.44 \n",
      "Iteration: 2679, Loss:2.52 \n",
      "Iteration: 2680, Loss:2.34 \n",
      "Iteration: 2681, Loss:2.60 \n",
      "Iteration: 2682, Loss:2.31 \n",
      "Iteration: 2683, Loss:2.80 \n",
      "Iteration: 2684, Loss:2.45 \n",
      "Iteration: 2685, Loss:2.74 \n",
      "Iteration: 2686, Loss:2.81 \n",
      "Iteration: 2687, Loss:2.86 \n",
      "Iteration: 2688, Loss:2.77 \n",
      "Iteration: 2689, Loss:2.47 \n",
      "Iteration: 2690, Loss:2.33 \n",
      "Iteration: 2691, Loss:2.42 \n",
      "Iteration: 2692, Loss:2.93 \n",
      "Iteration: 2693, Loss:2.38 \n",
      "Iteration: 2694, Loss:2.77 \n",
      "Iteration: 2695, Loss:2.35 \n",
      "Iteration: 2696, Loss:2.59 \n",
      "Iteration: 2697, Loss:2.84 \n",
      "Iteration: 2698, Loss:2.28 \n",
      "Iteration: 2699, Loss:1.96 \n",
      "Iteration: 2700, Loss:2.57 \n",
      "Iteration: 2701, Loss:2.57 \n",
      "Iteration: 2702, Loss:2.56 \n",
      "Iteration: 2703, Loss:2.70 \n",
      "Iteration: 2704, Loss:2.66 \n",
      "Iteration: 2705, Loss:2.74 \n",
      "Iteration: 2706, Loss:2.74 \n",
      "Iteration: 2707, Loss:2.95 \n",
      "Iteration: 2708, Loss:2.89 \n",
      "Iteration: 2709, Loss:2.42 \n",
      "Iteration: 2710, Loss:2.22 \n",
      "Iteration: 2711, Loss:2.42 \n",
      "Iteration: 2712, Loss:2.67 \n",
      "Iteration: 2713, Loss:1.94 \n",
      "Iteration: 2714, Loss:2.65 \n",
      "Iteration: 2715, Loss:2.62 \n",
      "Iteration: 2716, Loss:2.07 \n",
      "Iteration: 2717, Loss:2.15 \n",
      "Iteration: 2718, Loss:2.79 \n",
      "Iteration: 2719, Loss:2.39 \n",
      "Iteration: 2720, Loss:2.59 \n",
      "Iteration: 2721, Loss:2.23 \n",
      "Iteration: 2722, Loss:2.65 \n",
      "Iteration: 2723, Loss:2.66 \n",
      "Iteration: 2724, Loss:2.54 \n",
      "Iteration: 2725, Loss:2.30 \n",
      "Iteration: 2726, Loss:2.50 \n",
      "Iteration: 2727, Loss:2.76 \n",
      "Iteration: 2728, Loss:2.49 \n",
      "Iteration: 2729, Loss:2.00 \n",
      "Iteration: 2730, Loss:2.78 \n",
      "Iteration: 2731, Loss:2.81 \n",
      "Iteration: 2732, Loss:2.50 \n",
      "Iteration: 2733, Loss:2.48 \n",
      "Iteration: 2734, Loss:2.65 \n",
      "Iteration: 2735, Loss:2.22 \n",
      "Iteration: 2736, Loss:2.69 \n",
      "Iteration: 2737, Loss:2.62 \n",
      "Iteration: 2738, Loss:2.28 \n",
      "Iteration: 2739, Loss:2.73 \n",
      "Iteration: 2740, Loss:2.71 \n",
      "Iteration: 2741, Loss:2.25 \n",
      "Iteration: 2742, Loss:2.35 \n",
      "Iteration: 2743, Loss:2.93 \n",
      "Iteration: 2744, Loss:2.60 \n",
      "Iteration: 2745, Loss:2.58 \n",
      "Iteration: 2746, Loss:2.77 \n",
      "Iteration: 2747, Loss:2.57 \n",
      "Iteration: 2748, Loss:2.73 \n",
      "Iteration: 2749, Loss:2.13 \n",
      "Iteration: 2750, Loss:2.44 \n",
      "Iteration: 2751, Loss:2.14 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_2750.ckpt\n",
      "Iteration: 2752, Loss:2.60 \n",
      "Iteration: 2753, Loss:2.77 \n",
      "Iteration: 2754, Loss:2.49 \n",
      "Iteration: 2755, Loss:2.39 \n",
      "Iteration: 2756, Loss:2.44 \n",
      "Iteration: 2757, Loss:2.27 \n",
      "Iteration: 2758, Loss:2.70 \n",
      "Iteration: 2759, Loss:2.69 \n",
      "Iteration: 2760, Loss:2.97 \n",
      "Iteration: 2761, Loss:2.45 \n",
      "Iteration: 2762, Loss:2.38 \n",
      "Iteration: 2763, Loss:2.41 \n",
      "Iteration: 2764, Loss:2.29 \n",
      "Iteration: 2765, Loss:2.46 \n",
      "Iteration: 2766, Loss:2.14 \n",
      "Iteration: 2767, Loss:2.67 \n",
      "Iteration: 2768, Loss:2.28 \n",
      "Iteration: 2769, Loss:2.15 \n",
      "Iteration: 2770, Loss:2.27 \n",
      "Iteration: 2771, Loss:2.29 \n",
      "Iteration: 2772, Loss:2.91 \n",
      "Iteration: 2773, Loss:2.38 \n",
      "Iteration: 2774, Loss:2.49 \n",
      "Iteration: 2775, Loss:2.79 \n",
      "Iteration: 2776, Loss:2.79 \n",
      "Iteration: 2777, Loss:2.57 \n",
      "Iteration: 2778, Loss:2.40 \n",
      "Iteration: 2779, Loss:2.95 \n",
      "Iteration: 2780, Loss:2.72 \n",
      "Iteration: 2781, Loss:2.14 \n",
      "Iteration: 2782, Loss:2.89 \n",
      "Iteration: 2783, Loss:2.46 \n",
      "Iteration: 2784, Loss:2.42 \n",
      "Iteration: 2785, Loss:2.72 \n",
      "Iteration: 2786, Loss:2.46 \n",
      "Iteration: 2787, Loss:2.48 \n",
      "Iteration: 2788, Loss:2.83 \n",
      "Iteration: 2789, Loss:2.71 \n",
      "Iteration: 2790, Loss:2.63 \n",
      "Iteration: 2791, Loss:2.61 \n",
      "Iteration: 2792, Loss:2.26 \n",
      "Iteration: 2793, Loss:2.37 \n",
      "Iteration: 2794, Loss:2.86 \n",
      "Iteration: 2795, Loss:2.37 \n",
      "Iteration: 2796, Loss:2.38 \n",
      "Iteration: 2797, Loss:2.43 \n",
      "Iteration: 2798, Loss:2.41 \n",
      "Iteration: 2799, Loss:2.27 \n",
      "Iteration: 2800, Loss:2.88 \n",
      "Iteration: 2801, Loss:2.89 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_2800.ckpt\n",
      "Iteration: 2802, Loss:2.61 \n",
      "Iteration: 2803, Loss:2.34 \n",
      "Iteration: 2804, Loss:2.73 \n",
      "Iteration: 2805, Loss:2.40 \n",
      "Iteration: 2806, Loss:2.79 \n",
      "Iteration: 2807, Loss:2.76 \n",
      "Iteration: 2808, Loss:2.57 \n",
      "Iteration: 2809, Loss:2.68 \n",
      "Iteration: 2810, Loss:2.64 \n",
      "Iteration: 2811, Loss:2.52 \n",
      "Iteration: 2812, Loss:2.40 \n",
      "Iteration: 2813, Loss:2.57 \n",
      "Iteration: 2814, Loss:2.61 \n",
      "Iteration: 2815, Loss:2.61 \n",
      "Iteration: 2816, Loss:2.48 \n",
      "Iteration: 2817, Loss:2.70 \n",
      "Iteration: 2818, Loss:2.73 \n",
      "Iteration: 2819, Loss:2.98 \n",
      "Iteration: 2820, Loss:2.41 \n",
      "Iteration: 2821, Loss:2.65 \n",
      "Iteration: 2822, Loss:2.90 \n",
      "Iteration: 2823, Loss:2.20 \n",
      "Iteration: 2824, Loss:2.63 \n",
      "Iteration: 2825, Loss:2.44 \n",
      "Iteration: 2826, Loss:2.38 \n",
      "Iteration: 2827, Loss:2.57 \n",
      "Iteration: 2828, Loss:2.61 \n",
      "Iteration: 2829, Loss:2.32 \n",
      "Iteration: 2830, Loss:2.02 \n",
      "Iteration: 2831, Loss:2.94 \n",
      "Iteration: 2832, Loss:2.60 \n",
      "Iteration: 2833, Loss:2.40 \n",
      "Iteration: 2834, Loss:2.42 \n",
      "Iteration: 2835, Loss:2.90 \n",
      "Iteration: 2836, Loss:2.61 \n",
      "Iteration: 2837, Loss:2.29 \n",
      "Iteration: 2838, Loss:2.37 \n",
      "Iteration: 2839, Loss:1.74 \n",
      "Iteration: 2840, Loss:2.39 \n",
      "Iteration: 2841, Loss:2.62 \n",
      "Iteration: 2842, Loss:2.64 \n",
      "Iteration: 2843, Loss:2.52 \n",
      "Iteration: 2844, Loss:2.78 \n",
      "Iteration: 2845, Loss:1.87 \n",
      "Iteration: 2846, Loss:2.50 \n",
      "Iteration: 2847, Loss:2.81 \n",
      "Iteration: 2848, Loss:2.78 \n",
      "Iteration: 2849, Loss:2.81 \n",
      "Iteration: 2850, Loss:2.59 \n",
      "Iteration: 2851, Loss:2.54 \n",
      "Iteration: 2852, Loss:3.11 \n",
      "Iteration: 2853, Loss:2.55 \n",
      "Iteration: 2854, Loss:2.23 \n",
      "Iteration: 2855, Loss:2.48 \n",
      "Iteration: 2856, Loss:2.37 \n",
      "Iteration: 2857, Loss:2.27 \n",
      "Iteration: 2858, Loss:2.23 \n",
      "Iteration: 2859, Loss:2.87 \n",
      "Iteration: 2860, Loss:2.63 \n",
      "Iteration: 2861, Loss:2.68 \n",
      "Iteration: 2862, Loss:3.02 \n",
      "Iteration: 2863, Loss:2.54 \n",
      "Iteration: 2864, Loss:2.63 \n",
      "Iteration: 2865, Loss:2.82 \n",
      "Iteration: 2866, Loss:2.20 \n",
      "Iteration: 2867, Loss:2.61 \n",
      "Iteration: 2868, Loss:2.45 \n",
      "Iteration: 2869, Loss:2.72 \n",
      "Iteration: 2870, Loss:2.24 \n",
      "Iteration: 2871, Loss:2.35 \n",
      "Iteration: 2872, Loss:2.49 \n",
      "Iteration: 2873, Loss:2.40 \n",
      "Iteration: 2874, Loss:2.74 \n",
      "Iteration: 2875, Loss:2.94 \n",
      "Iteration: 2876, Loss:2.62 \n",
      "Iteration: 2877, Loss:2.74 \n",
      "Iteration: 2878, Loss:2.91 \n",
      "Iteration: 2879, Loss:2.54 \n",
      "Iteration: 2880, Loss:2.40 \n",
      "Iteration: 2881, Loss:2.67 \n",
      "Iteration: 2882, Loss:2.73 \n",
      "Iteration: 2883, Loss:2.21 \n",
      "Iteration: 2884, Loss:2.49 \n",
      "Iteration: 2885, Loss:2.07 \n",
      "Iteration: 2886, Loss:2.40 \n",
      "Iteration: 2887, Loss:2.45 \n",
      "Iteration: 2888, Loss:2.86 \n",
      "Iteration: 2889, Loss:2.49 \n",
      "Iteration: 2890, Loss:2.57 \n",
      "Iteration: 2891, Loss:2.72 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2892, Loss:2.82 \n",
      "Iteration: 2893, Loss:2.27 \n",
      "Iteration: 2894, Loss:2.85 \n",
      "Iteration: 2895, Loss:2.76 \n",
      "Iteration: 2896, Loss:2.57 \n",
      "Iteration: 2897, Loss:2.36 \n",
      "Iteration: 2898, Loss:2.69 \n",
      "Iteration: 2899, Loss:2.44 \n",
      "Iteration: 2900, Loss:2.16 \n",
      "Iteration: 2901, Loss:2.28 \n",
      "Iteration: 2902, Loss:2.52 \n",
      "Iteration: 2903, Loss:2.74 \n",
      "Iteration: 2904, Loss:2.41 \n",
      "Iteration: 2905, Loss:2.29 \n",
      "Iteration: 2906, Loss:2.43 \n",
      "Iteration: 2907, Loss:2.70 \n",
      "Iteration: 2908, Loss:2.35 \n",
      "Iteration: 2909, Loss:2.48 \n",
      "Iteration: 2910, Loss:2.86 \n",
      "Iteration: 2911, Loss:2.36 \n",
      "Iteration: 2912, Loss:2.87 \n",
      "Iteration: 2913, Loss:2.67 \n",
      "Iteration: 2914, Loss:2.61 \n",
      "Iteration: 2915, Loss:2.98 \n",
      "Iteration: 2916, Loss:2.40 \n",
      "Iteration: 2917, Loss:2.79 \n",
      "Iteration: 2918, Loss:1.95 \n",
      "Iteration: 2919, Loss:2.50 \n",
      "Iteration: 2920, Loss:2.68 \n",
      "Iteration: 2921, Loss:2.59 \n",
      "Iteration: 2922, Loss:2.81 \n",
      "Iteration: 2923, Loss:2.33 \n",
      "Iteration: 2924, Loss:2.60 \n",
      "Iteration: 2925, Loss:2.42 \n",
      "Iteration: 2926, Loss:2.39 \n",
      "Iteration: 2927, Loss:2.41 \n",
      "Iteration: 2928, Loss:2.58 \n",
      "Iteration: 2929, Loss:2.51 \n",
      "Iteration: 2930, Loss:2.37 \n",
      "Iteration: 2931, Loss:2.61 \n",
      "Iteration: 2932, Loss:2.70 \n",
      "Iteration: 2933, Loss:2.55 \n",
      "Iteration: 2934, Loss:2.45 \n",
      "Iteration: 2935, Loss:2.96 \n",
      "Iteration: 2936, Loss:2.54 \n",
      "Iteration: 2937, Loss:2.54 \n",
      "Iteration: 2938, Loss:2.53 \n",
      "Iteration: 2939, Loss:2.63 \n",
      "Iteration: 2940, Loss:2.14 \n",
      "Iteration: 2941, Loss:2.71 \n",
      "Iteration: 2942, Loss:2.70 \n",
      "Iteration: 2943, Loss:3.15 \n",
      "Iteration: 2944, Loss:2.48 \n",
      "Iteration: 2945, Loss:2.64 \n",
      "Iteration: 2946, Loss:2.39 \n",
      "Iteration: 2947, Loss:2.42 \n",
      "Iteration: 2948, Loss:2.72 \n",
      "Iteration: 2949, Loss:2.77 \n",
      "Iteration: 2950, Loss:2.33 \n",
      "Iteration: 2951, Loss:2.60 \n",
      "Iteration: 2952, Loss:2.55 \n",
      "Iteration: 2953, Loss:2.43 \n",
      "Iteration: 2954, Loss:2.81 \n",
      "Iteration: 2955, Loss:2.53 \n",
      "Iteration: 2956, Loss:2.19 \n",
      "Iteration: 2957, Loss:2.62 \n",
      "Iteration: 2958, Loss:2.08 \n",
      "Iteration: 2959, Loss:2.82 \n",
      "Iteration: 2960, Loss:2.19 \n",
      "Iteration: 2961, Loss:2.40 \n",
      "Iteration: 2962, Loss:2.31 \n",
      "Iteration: 2963, Loss:2.23 \n",
      "Iteration: 2964, Loss:2.64 \n",
      "Iteration: 2965, Loss:2.79 \n",
      "Iteration: 2966, Loss:2.55 \n",
      "Iteration: 2967, Loss:2.62 \n",
      "Iteration: 2968, Loss:2.57 \n",
      "Iteration: 2969, Loss:2.67 \n",
      "Iteration: 2970, Loss:2.77 \n",
      "Iteration: 2971, Loss:2.43 \n",
      "Iteration: 2972, Loss:2.60 \n",
      "Iteration: 2973, Loss:2.49 \n",
      "Iteration: 2974, Loss:2.57 \n",
      "Iteration: 2975, Loss:2.54 \n",
      "Iteration: 2976, Loss:2.64 \n",
      "Iteration: 2977, Loss:2.59 \n",
      "Iteration: 2978, Loss:2.41 \n",
      "Iteration: 2979, Loss:2.71 \n",
      "Iteration: 2980, Loss:2.70 \n",
      "Iteration: 2981, Loss:2.59 \n",
      "Iteration: 2982, Loss:2.42 \n",
      "Iteration: 2983, Loss:2.40 \n",
      "Iteration: 2984, Loss:2.91 \n",
      "Iteration: 2985, Loss:2.84 \n",
      "Iteration: 2986, Loss:2.74 \n",
      "Iteration: 2987, Loss:2.55 \n",
      "Iteration: 2988, Loss:2.47 \n",
      "Iteration: 2989, Loss:2.63 \n",
      "Iteration: 2990, Loss:2.63 \n",
      "Iteration: 2991, Loss:2.24 \n",
      "Iteration: 2992, Loss:2.41 \n",
      "Iteration: 2993, Loss:2.46 \n",
      "Iteration: 2994, Loss:2.07 \n",
      "Iteration: 2995, Loss:2.61 \n",
      "Iteration: 2996, Loss:2.82 \n",
      "Iteration: 2997, Loss:2.69 \n",
      "Iteration: 2998, Loss:2.30 \n",
      "Iteration: 2999, Loss:2.54 \n",
      "Iteration: 3000, Loss:2.42 \n",
      "Iteration: 3001, Loss:2.70 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_3000.ckpt\n",
      "Iteration: 3002, Loss:2.07 \n",
      "Iteration: 3003, Loss:2.72 \n",
      "Iteration: 3004, Loss:2.26 \n",
      "Iteration: 3005, Loss:1.88 \n",
      "Iteration: 3006, Loss:2.79 \n",
      "Iteration: 3007, Loss:2.84 \n",
      "Iteration: 3008, Loss:2.52 \n",
      "Iteration: 3009, Loss:2.50 \n",
      "Iteration: 3010, Loss:2.41 \n",
      "Iteration: 3011, Loss:2.55 \n",
      "Iteration: 3012, Loss:2.28 \n",
      "Iteration: 3013, Loss:2.85 \n",
      "Iteration: 3014, Loss:2.13 \n",
      "Iteration: 3015, Loss:2.69 \n",
      "Iteration: 3016, Loss:2.15 \n",
      "Iteration: 3017, Loss:2.51 \n",
      "Iteration: 3018, Loss:2.36 \n",
      "Iteration: 3019, Loss:2.58 \n",
      "Iteration: 3020, Loss:2.60 \n",
      "Iteration: 3021, Loss:2.80 \n",
      "Iteration: 3022, Loss:2.79 \n",
      "Iteration: 3023, Loss:2.50 \n",
      "Iteration: 3024, Loss:2.63 \n",
      "Iteration: 3025, Loss:2.77 \n",
      "Iteration: 3026, Loss:2.49 \n",
      "Iteration: 3027, Loss:2.41 \n",
      "Iteration: 3028, Loss:2.67 \n",
      "Iteration: 3029, Loss:2.71 \n",
      "Iteration: 3030, Loss:2.46 \n",
      "Iteration: 3031, Loss:2.59 \n",
      "Iteration: 3032, Loss:2.67 \n",
      "Iteration: 3033, Loss:2.42 \n",
      "Iteration: 3034, Loss:2.68 \n",
      "Iteration: 3035, Loss:2.51 \n",
      "Iteration: 3036, Loss:2.53 \n",
      "Iteration: 3037, Loss:2.34 \n",
      "Iteration: 3038, Loss:2.83 \n",
      "Iteration: 3039, Loss:2.53 \n",
      "Iteration: 3040, Loss:2.79 \n",
      "Iteration: 3041, Loss:2.64 \n",
      "Iteration: 3042, Loss:2.65 \n",
      "Iteration: 3043, Loss:2.31 \n",
      "Iteration: 3044, Loss:2.76 \n",
      "Iteration: 3045, Loss:2.43 \n",
      "Iteration: 3046, Loss:2.40 \n",
      "Iteration: 3047, Loss:2.39 \n",
      "Iteration: 3048, Loss:2.65 \n",
      "Iteration: 3049, Loss:2.56 \n",
      "Iteration: 3050, Loss:2.58 \n",
      "Iteration: 3051, Loss:2.72 \n",
      "Iteration: 3052, Loss:2.53 \n",
      "Iteration: 3053, Loss:2.53 \n",
      "Iteration: 3054, Loss:2.46 \n",
      "Iteration: 3055, Loss:2.49 \n",
      "Iteration: 3056, Loss:2.60 \n",
      "Iteration: 3057, Loss:2.38 \n",
      "Iteration: 3058, Loss:2.75 \n",
      "Iteration: 3059, Loss:2.82 \n",
      "Iteration: 3060, Loss:2.57 \n",
      "Iteration: 3061, Loss:2.37 \n",
      "Iteration: 3062, Loss:1.93 \n",
      "Iteration: 3063, Loss:2.52 \n",
      "Iteration: 3064, Loss:2.30 \n",
      "Iteration: 3065, Loss:1.93 \n",
      "Iteration: 3066, Loss:3.02 \n",
      "Iteration: 3067, Loss:2.54 \n",
      "Iteration: 3068, Loss:2.79 \n",
      "Iteration: 3069, Loss:2.22 \n",
      "Iteration: 3070, Loss:2.38 \n",
      "Iteration: 3071, Loss:2.51 \n",
      "Iteration: 3072, Loss:2.52 \n",
      "Iteration: 3073, Loss:2.80 \n",
      "Iteration: 3074, Loss:2.70 \n",
      "Iteration: 3075, Loss:2.50 \n",
      "Iteration: 3076, Loss:2.21 \n",
      "Iteration: 3077, Loss:2.14 \n",
      "Iteration: 3078, Loss:2.63 \n",
      "Iteration: 3079, Loss:2.90 \n",
      "Iteration: 3080, Loss:2.45 \n",
      "Iteration: 3081, Loss:2.76 \n",
      "Iteration: 3082, Loss:2.61 \n",
      "Iteration: 3083, Loss:2.39 \n",
      "Iteration: 3084, Loss:2.70 \n",
      "Iteration: 3085, Loss:2.49 \n",
      "Iteration: 3086, Loss:1.85 \n",
      "Iteration: 3087, Loss:3.05 \n",
      "Iteration: 3088, Loss:2.79 \n",
      "Iteration: 3089, Loss:2.50 \n",
      "Iteration: 3090, Loss:2.22 \n",
      "Iteration: 3091, Loss:2.33 \n",
      "Iteration: 3092, Loss:2.30 \n",
      "Iteration: 3093, Loss:2.77 \n",
      "Iteration: 3094, Loss:2.41 \n",
      "Iteration: 3095, Loss:2.77 \n",
      "Iteration: 3096, Loss:2.64 \n",
      "Iteration: 3097, Loss:2.24 \n",
      "Iteration: 3098, Loss:2.54 \n",
      "Iteration: 3099, Loss:2.32 \n",
      "Iteration: 3100, Loss:2.63 \n",
      "Iteration: 3101, Loss:2.83 \n",
      "Iteration: 3102, Loss:2.49 \n",
      "Iteration: 3103, Loss:2.78 \n",
      "Iteration: 3104, Loss:2.71 \n",
      "Iteration: 3105, Loss:2.23 \n",
      "Iteration: 3106, Loss:2.62 \n",
      "Iteration: 3107, Loss:2.58 \n",
      "Iteration: 3108, Loss:2.92 \n",
      "Iteration: 3109, Loss:2.82 \n",
      "Iteration: 3110, Loss:2.13 \n",
      "Iteration: 3111, Loss:2.50 \n",
      "Iteration: 3112, Loss:2.83 \n",
      "Iteration: 3113, Loss:2.31 \n",
      "Iteration: 3114, Loss:2.55 \n",
      "Iteration: 3115, Loss:2.51 \n",
      "Iteration: 3116, Loss:2.16 \n",
      "Iteration: 3117, Loss:2.70 \n",
      "Iteration: 3118, Loss:2.26 \n",
      "Iteration: 3119, Loss:2.41 \n",
      "Iteration: 3120, Loss:2.55 \n",
      "Iteration: 3121, Loss:2.51 \n",
      "Iteration: 3122, Loss:2.43 \n",
      "Iteration: 3123, Loss:2.73 \n",
      "Iteration: 3124, Loss:2.65 \n",
      "Iteration: 3125, Loss:2.84 \n",
      "Iteration: 3126, Loss:2.38 \n",
      "Iteration: 3127, Loss:2.55 \n",
      "Iteration: 3128, Loss:2.25 \n",
      "Iteration: 3129, Loss:2.18 \n",
      "Iteration: 3130, Loss:2.35 \n",
      "Iteration: 3131, Loss:2.16 \n",
      "Iteration: 3132, Loss:2.72 \n",
      "Iteration: 3133, Loss:2.32 \n",
      "Iteration: 3134, Loss:2.59 \n",
      "Iteration: 3135, Loss:2.46 \n",
      "Iteration: 3136, Loss:2.49 \n",
      "Iteration: 3137, Loss:2.68 \n",
      "Iteration: 3138, Loss:2.14 \n",
      "Iteration: 3139, Loss:2.06 \n",
      "Iteration: 3140, Loss:2.18 \n",
      "Iteration: 3141, Loss:2.41 \n",
      "Iteration: 3142, Loss:2.89 \n",
      "Iteration: 3143, Loss:2.52 \n",
      "Iteration: 3144, Loss:2.51 \n",
      "Iteration: 3145, Loss:2.35 \n",
      "Iteration: 3146, Loss:2.93 \n",
      "Iteration: 3147, Loss:3.08 \n",
      "Iteration: 3148, Loss:2.58 \n",
      "Iteration: 3149, Loss:2.26 \n",
      "Iteration: 3150, Loss:2.62 \n",
      "Iteration: 3151, Loss:2.51 \n",
      "Iteration: 3152, Loss:2.66 \n",
      "Iteration: 3153, Loss:2.48 \n",
      "Iteration: 3154, Loss:2.78 \n",
      "Iteration: 3155, Loss:2.82 \n",
      "Iteration: 3156, Loss:2.54 \n",
      "Iteration: 3157, Loss:2.44 \n",
      "Iteration: 3158, Loss:2.07 \n",
      "Iteration: 3159, Loss:2.51 \n",
      "Iteration: 3160, Loss:2.58 \n",
      "Iteration: 3161, Loss:2.86 \n",
      "Iteration: 3162, Loss:2.76 \n",
      "Iteration: 3163, Loss:2.05 \n",
      "Iteration: 3164, Loss:2.84 \n",
      "Iteration: 3165, Loss:2.73 \n",
      "Iteration: 3166, Loss:2.80 \n",
      "Iteration: 3167, Loss:2.82 \n",
      "Iteration: 3168, Loss:2.69 \n",
      "Iteration: 3169, Loss:2.29 \n",
      "Iteration: 3170, Loss:2.73 \n",
      "Iteration: 3171, Loss:2.89 \n",
      "Iteration: 3172, Loss:2.22 \n",
      "Iteration: 3173, Loss:2.23 \n",
      "Iteration: 3174, Loss:2.80 \n",
      "Iteration: 3175, Loss:2.51 \n",
      "Iteration: 3176, Loss:2.38 \n",
      "Iteration: 3177, Loss:2.51 \n",
      "Iteration: 3178, Loss:2.61 \n",
      "Iteration: 3179, Loss:2.75 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3180, Loss:2.79 \n",
      "Iteration: 3181, Loss:2.39 \n",
      "Iteration: 3182, Loss:2.65 \n",
      "Iteration: 3183, Loss:2.42 \n",
      "Iteration: 3184, Loss:2.60 \n",
      "Iteration: 3185, Loss:2.49 \n",
      "Iteration: 3186, Loss:2.76 \n",
      "Iteration: 3187, Loss:2.67 \n",
      "Iteration: 3188, Loss:2.56 \n",
      "Iteration: 3189, Loss:2.57 \n",
      "Iteration: 3190, Loss:2.74 \n",
      "Iteration: 3191, Loss:2.68 \n",
      "Iteration: 3192, Loss:2.50 \n",
      "Iteration: 3193, Loss:2.08 \n",
      "Iteration: 3194, Loss:2.56 \n",
      "Iteration: 3195, Loss:2.45 \n",
      "Iteration: 3196, Loss:2.60 \n",
      "Iteration: 3197, Loss:2.16 \n",
      "Iteration: 3198, Loss:2.42 \n",
      "Iteration: 3199, Loss:2.52 \n",
      "Iteration: 3200, Loss:2.80 \n",
      "Iteration: 3201, Loss:2.75 \n",
      "Iteration: 3202, Loss:2.39 \n",
      "Iteration: 3203, Loss:2.29 \n",
      "Iteration: 3204, Loss:2.84 \n",
      "Iteration: 3205, Loss:2.27 \n",
      "Iteration: 3206, Loss:2.66 \n",
      "Iteration: 3207, Loss:2.56 \n",
      "Iteration: 3208, Loss:2.13 \n",
      "Iteration: 3209, Loss:2.28 \n",
      "Iteration: 3210, Loss:2.79 \n",
      "Iteration: 3211, Loss:2.56 \n",
      "Iteration: 3212, Loss:2.79 \n",
      "Iteration: 3213, Loss:2.42 \n",
      "Iteration: 3214, Loss:2.63 \n",
      "Iteration: 3215, Loss:2.72 \n",
      "Iteration: 3216, Loss:2.75 \n",
      "Iteration: 3217, Loss:2.05 \n",
      "Iteration: 3218, Loss:2.22 \n",
      "Iteration: 3219, Loss:2.57 \n",
      "Iteration: 3220, Loss:2.76 \n",
      "Iteration: 3221, Loss:2.17 \n",
      "Iteration: 3222, Loss:3.01 \n",
      "Iteration: 3223, Loss:2.22 \n",
      "Iteration: 3224, Loss:2.37 \n",
      "Iteration: 3225, Loss:2.83 \n",
      "Iteration: 3226, Loss:2.46 \n",
      "Iteration: 3227, Loss:2.73 \n",
      "Iteration: 3228, Loss:2.90 \n",
      "Iteration: 3229, Loss:2.64 \n",
      "Iteration: 3230, Loss:2.47 \n",
      "Iteration: 3231, Loss:1.87 \n",
      "Iteration: 3232, Loss:2.65 \n",
      "Iteration: 3233, Loss:2.88 \n",
      "Iteration: 3234, Loss:2.79 \n",
      "Iteration: 3235, Loss:2.60 \n",
      "Iteration: 3236, Loss:2.61 \n",
      "Iteration: 3237, Loss:2.52 \n",
      "Iteration: 3238, Loss:2.48 \n",
      "Iteration: 3239, Loss:2.65 \n",
      "Iteration: 3240, Loss:2.58 \n",
      "Iteration: 3241, Loss:2.64 \n",
      "Iteration: 3242, Loss:2.42 \n",
      "Iteration: 3243, Loss:2.32 \n",
      "Iteration: 3244, Loss:2.85 \n",
      "Iteration: 3245, Loss:2.54 \n",
      "Iteration: 3246, Loss:2.58 \n",
      "Iteration: 3247, Loss:2.89 \n",
      "Iteration: 3248, Loss:2.50 \n",
      "Iteration: 3249, Loss:2.64 \n",
      "Iteration: 3250, Loss:2.72 \n",
      "Iteration: 3251, Loss:2.38 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_3250.ckpt\n",
      "Iteration: 3252, Loss:2.85 \n",
      "Iteration: 3253, Loss:2.66 \n",
      "Iteration: 3254, Loss:2.40 \n",
      "Iteration: 3255, Loss:2.24 \n",
      "Iteration: 3256, Loss:2.87 \n",
      "Iteration: 3257, Loss:2.42 \n",
      "Iteration: 3258, Loss:2.79 \n",
      "Iteration: 3259, Loss:2.66 \n",
      "Iteration: 3260, Loss:2.70 \n",
      "Iteration: 3261, Loss:2.53 \n",
      "Iteration: 3262, Loss:2.62 \n",
      "Iteration: 3263, Loss:2.62 \n",
      "Iteration: 3264, Loss:2.40 \n",
      "Iteration: 3265, Loss:2.79 \n",
      "Iteration: 3266, Loss:2.68 \n",
      "Iteration: 3267, Loss:2.50 \n",
      "Iteration: 3268, Loss:2.01 \n",
      "Iteration: 3269, Loss:2.60 \n",
      "Iteration: 3270, Loss:2.55 \n",
      "Iteration: 3271, Loss:2.53 \n",
      "Iteration: 3272, Loss:2.51 \n",
      "Iteration: 3273, Loss:2.93 \n",
      "Iteration: 3274, Loss:2.61 \n",
      "Iteration: 3275, Loss:2.64 \n",
      "Iteration: 3276, Loss:2.41 \n",
      "Iteration: 3277, Loss:2.48 \n",
      "Iteration: 3278, Loss:2.38 \n",
      "Iteration: 3279, Loss:2.22 \n",
      "Iteration: 3280, Loss:2.57 \n",
      "Iteration: 3281, Loss:2.28 \n",
      "Iteration: 3282, Loss:2.54 \n",
      "Iteration: 3283, Loss:2.55 \n",
      "Iteration: 3284, Loss:2.82 \n",
      "Iteration: 3285, Loss:2.59 \n",
      "Iteration: 3286, Loss:2.52 \n",
      "Iteration: 3287, Loss:2.30 \n",
      "Iteration: 3288, Loss:2.48 \n",
      "Iteration: 3289, Loss:2.73 \n",
      "Iteration: 3290, Loss:2.52 \n",
      "Iteration: 3291, Loss:2.53 \n",
      "Iteration: 3292, Loss:2.86 \n",
      "Iteration: 3293, Loss:2.74 \n",
      "Iteration: 3294, Loss:2.23 \n",
      "Iteration: 3295, Loss:2.34 \n",
      "Iteration: 3296, Loss:2.86 \n",
      "Iteration: 3297, Loss:2.62 \n",
      "Iteration: 3298, Loss:2.37 \n",
      "Iteration: 3299, Loss:2.24 \n",
      "Iteration: 3300, Loss:2.74 \n",
      "Iteration: 3301, Loss:2.19 \n",
      "Iteration: 3302, Loss:2.41 \n",
      "Iteration: 3303, Loss:2.26 \n",
      "Iteration: 3304, Loss:2.78 \n",
      "Iteration: 3305, Loss:2.42 \n",
      "Iteration: 3306, Loss:2.75 \n",
      "Iteration: 3307, Loss:2.62 \n",
      "Iteration: 3308, Loss:2.27 \n",
      "Iteration: 3309, Loss:2.70 \n",
      "Iteration: 3310, Loss:2.65 \n",
      "Iteration: 3311, Loss:2.44 \n",
      "Iteration: 3312, Loss:2.56 \n",
      "Iteration: 3313, Loss:2.62 \n",
      "Iteration: 3314, Loss:2.70 \n",
      "Iteration: 3315, Loss:2.55 \n",
      "Iteration: 3316, Loss:2.74 \n",
      "Iteration: 3317, Loss:2.58 \n",
      "Iteration: 3318, Loss:3.03 \n",
      "Iteration: 3319, Loss:2.41 \n",
      "Iteration: 3320, Loss:2.10 \n",
      "Iteration: 3321, Loss:2.82 \n",
      "Iteration: 3322, Loss:2.92 \n",
      "Iteration: 3323, Loss:2.75 \n",
      "Iteration: 3324, Loss:2.64 \n",
      "Iteration: 3325, Loss:2.57 \n",
      "Iteration: 3326, Loss:2.44 \n",
      "Iteration: 3327, Loss:2.40 \n",
      "Iteration: 3328, Loss:2.39 \n",
      "Iteration: 3329, Loss:2.34 \n",
      "Iteration: 3330, Loss:2.98 \n",
      "Iteration: 3331, Loss:2.31 \n",
      "Iteration: 3332, Loss:2.48 \n",
      "Iteration: 3333, Loss:2.67 \n",
      "Iteration: 3334, Loss:2.57 \n",
      "Iteration: 3335, Loss:2.24 \n",
      "Iteration: 3336, Loss:2.43 \n",
      "Iteration: 3337, Loss:2.47 \n",
      "Iteration: 3338, Loss:2.19 \n",
      "Iteration: 3339, Loss:2.31 \n",
      "Iteration: 3340, Loss:2.56 \n",
      "Iteration: 3341, Loss:2.36 \n",
      "Iteration: 3342, Loss:2.24 \n",
      "Iteration: 3343, Loss:2.47 \n",
      "Iteration: 3344, Loss:2.41 \n",
      "Iteration: 3345, Loss:2.72 \n",
      "Iteration: 3346, Loss:2.49 \n",
      "Iteration: 3347, Loss:2.63 \n",
      "Iteration: 3348, Loss:2.30 \n",
      "Iteration: 3349, Loss:2.56 \n",
      "Iteration: 3350, Loss:2.30 \n",
      "Iteration: 3351, Loss:2.37 \n",
      "Iteration: 3352, Loss:2.06 \n",
      "Iteration: 3353, Loss:2.44 \n",
      "Iteration: 3354, Loss:2.46 \n",
      "Iteration: 3355, Loss:2.86 \n",
      "Iteration: 3356, Loss:2.43 \n",
      "Iteration: 3357, Loss:2.68 \n",
      "Iteration: 3358, Loss:2.70 \n",
      "Iteration: 3359, Loss:2.51 \n",
      "Iteration: 3360, Loss:2.69 \n",
      "Iteration: 3361, Loss:2.42 \n",
      "Iteration: 3362, Loss:2.35 \n",
      "Iteration: 3363, Loss:2.76 \n",
      "Iteration: 3364, Loss:2.51 \n",
      "Iteration: 3365, Loss:2.56 \n",
      "Iteration: 3366, Loss:2.68 \n",
      "Iteration: 3367, Loss:2.64 \n",
      "Iteration: 3368, Loss:2.21 \n",
      "Iteration: 3369, Loss:2.33 \n",
      "Iteration: 3370, Loss:2.79 \n",
      "Iteration: 3371, Loss:2.33 \n",
      "Iteration: 3372, Loss:2.79 \n",
      "Iteration: 3373, Loss:2.58 \n",
      "Iteration: 3374, Loss:2.58 \n",
      "Iteration: 3375, Loss:2.42 \n",
      "Iteration: 3376, Loss:2.60 \n",
      "Iteration: 3377, Loss:2.32 \n",
      "Iteration: 3378, Loss:2.63 \n",
      "Iteration: 3379, Loss:2.62 \n",
      "Iteration: 3380, Loss:2.68 \n",
      "Iteration: 3381, Loss:2.62 \n",
      "Iteration: 3382, Loss:2.42 \n",
      "Iteration: 3383, Loss:2.50 \n",
      "Iteration: 3384, Loss:2.76 \n",
      "Iteration: 3385, Loss:2.54 \n",
      "Iteration: 3386, Loss:2.87 \n",
      "Iteration: 3387, Loss:2.53 \n",
      "Iteration: 3388, Loss:2.40 \n",
      "Iteration: 3389, Loss:2.84 \n",
      "Iteration: 3390, Loss:2.69 \n",
      "Iteration: 3391, Loss:2.16 \n",
      "Iteration: 3392, Loss:2.11 \n",
      "Iteration: 3393, Loss:2.39 \n",
      "Iteration: 3394, Loss:2.45 \n",
      "Iteration: 3395, Loss:2.48 \n",
      "Iteration: 3396, Loss:2.40 \n",
      "Iteration: 3397, Loss:2.64 \n",
      "Iteration: 3398, Loss:2.85 \n",
      "Iteration: 3399, Loss:2.39 \n",
      "Iteration: 3400, Loss:2.45 \n",
      "Iteration: 3401, Loss:2.63 \n",
      "Iteration: 3402, Loss:2.36 \n",
      "Iteration: 3403, Loss:2.70 \n",
      "Iteration: 3404, Loss:2.41 \n",
      "Iteration: 3405, Loss:2.44 \n",
      "Iteration: 3406, Loss:2.62 \n",
      "Iteration: 3407, Loss:2.65 \n",
      "Iteration: 3408, Loss:2.58 \n",
      "Iteration: 3409, Loss:2.81 \n",
      "Iteration: 3410, Loss:2.64 \n",
      "Iteration: 3411, Loss:2.45 \n",
      "Iteration: 3412, Loss:2.76 \n",
      "Iteration: 3413, Loss:2.81 \n",
      "Iteration: 3414, Loss:2.70 \n",
      "Iteration: 3415, Loss:2.84 \n",
      "Iteration: 3416, Loss:2.71 \n",
      "Iteration: 3417, Loss:2.20 \n",
      "Iteration: 3418, Loss:2.48 \n",
      "Iteration: 3419, Loss:2.34 \n",
      "Iteration: 3420, Loss:2.85 \n",
      "Iteration: 3421, Loss:2.71 \n",
      "Iteration: 3422, Loss:2.64 \n",
      "Iteration: 3423, Loss:2.59 \n",
      "Iteration: 3424, Loss:2.71 \n",
      "Iteration: 3425, Loss:2.18 \n",
      "Iteration: 3426, Loss:2.59 \n",
      "Iteration: 3427, Loss:2.43 \n",
      "Iteration: 3428, Loss:2.43 \n",
      "Iteration: 3429, Loss:2.72 \n",
      "Iteration: 3430, Loss:2.49 \n",
      "Iteration: 3431, Loss:2.92 \n",
      "Iteration: 3432, Loss:2.96 \n",
      "Iteration: 3433, Loss:2.65 \n",
      "Iteration: 3434, Loss:2.18 \n",
      "Iteration: 3435, Loss:2.51 \n",
      "Iteration: 3436, Loss:2.30 \n",
      "Iteration: 3437, Loss:2.42 \n",
      "Iteration: 3438, Loss:2.51 \n",
      "Iteration: 3439, Loss:1.92 \n",
      "Iteration: 3440, Loss:2.74 \n",
      "Iteration: 3441, Loss:2.63 \n",
      "Iteration: 3442, Loss:2.69 \n",
      "Iteration: 3443, Loss:2.25 \n",
      "Iteration: 3444, Loss:2.43 \n",
      "Iteration: 3445, Loss:2.62 \n",
      "Iteration: 3446, Loss:2.52 \n",
      "Iteration: 3447, Loss:2.34 \n",
      "Iteration: 3448, Loss:2.81 \n",
      "Iteration: 3449, Loss:2.39 \n",
      "Iteration: 3450, Loss:2.42 \n",
      "Iteration: 3451, Loss:2.69 \n",
      "Iteration: 3452, Loss:2.59 \n",
      "Iteration: 3453, Loss:2.28 \n",
      "Iteration: 3454, Loss:2.87 \n",
      "Iteration: 3455, Loss:2.48 \n",
      "Iteration: 3456, Loss:2.79 \n",
      "Iteration: 3457, Loss:3.03 \n",
      "Iteration: 3458, Loss:2.51 \n",
      "Iteration: 3459, Loss:2.35 \n",
      "Iteration: 3460, Loss:2.44 \n",
      "Iteration: 3461, Loss:2.28 \n",
      "Iteration: 3462, Loss:2.51 \n",
      "Iteration: 3463, Loss:2.32 \n",
      "Iteration: 3464, Loss:3.06 \n",
      "Iteration: 3465, Loss:2.73 \n",
      "Iteration: 3466, Loss:2.60 \n",
      "Iteration: 3467, Loss:2.51 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3468, Loss:3.16 \n",
      "Iteration: 3469, Loss:2.49 \n",
      "Iteration: 3470, Loss:2.17 \n",
      "Iteration: 3471, Loss:2.75 \n",
      "Iteration: 3472, Loss:2.77 \n",
      "Iteration: 3473, Loss:2.59 \n",
      "Iteration: 3474, Loss:2.87 \n",
      "Iteration: 3475, Loss:2.40 \n",
      "Iteration: 3476, Loss:2.62 \n",
      "Iteration: 3477, Loss:2.77 \n",
      "Iteration: 3478, Loss:2.28 \n",
      "Iteration: 3479, Loss:2.40 \n",
      "Iteration: 3480, Loss:2.41 \n",
      "Iteration: 3481, Loss:2.77 \n",
      "Iteration: 3482, Loss:2.67 \n",
      "Iteration: 3483, Loss:2.41 \n",
      "Iteration: 3484, Loss:2.70 \n",
      "Iteration: 3485, Loss:2.78 \n",
      "Iteration: 3486, Loss:2.43 \n",
      "Iteration: 3487, Loss:2.87 \n",
      "Iteration: 3488, Loss:2.74 \n",
      "Iteration: 3489, Loss:2.22 \n",
      "Iteration: 3490, Loss:2.40 \n",
      "Iteration: 3491, Loss:2.97 \n",
      "Iteration: 3492, Loss:2.59 \n",
      "Iteration: 3493, Loss:2.94 \n",
      "Iteration: 3494, Loss:2.22 \n",
      "Iteration: 3495, Loss:2.09 \n",
      "Iteration: 3496, Loss:2.60 \n",
      "Iteration: 3497, Loss:2.26 \n",
      "Iteration: 3498, Loss:2.15 \n",
      "Iteration: 3499, Loss:2.58 \n",
      "Iteration: 3500, Loss:2.73 \n",
      "Iteration: 3501, Loss:2.29 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_3500.ckpt\n",
      "Iteration: 3502, Loss:2.84 \n",
      "Iteration: 3503, Loss:2.85 \n",
      "Iteration: 3504, Loss:2.41 \n",
      "Iteration: 3505, Loss:2.62 \n",
      "Iteration: 3506, Loss:2.15 \n",
      "Iteration: 3507, Loss:2.98 \n",
      "Iteration: 3508, Loss:2.64 \n",
      "Iteration: 3509, Loss:2.84 \n",
      "Iteration: 3510, Loss:2.84 \n",
      "Iteration: 3511, Loss:2.57 \n",
      "Iteration: 3512, Loss:2.70 \n",
      "Iteration: 3513, Loss:2.72 \n",
      "Iteration: 3514, Loss:2.45 \n",
      "Iteration: 3515, Loss:2.22 \n",
      "Iteration: 3516, Loss:2.29 \n",
      "Iteration: 3517, Loss:1.90 \n",
      "Iteration: 3518, Loss:2.66 \n",
      "Iteration: 3519, Loss:2.37 \n",
      "Iteration: 3520, Loss:2.17 \n",
      "Iteration: 3521, Loss:2.54 \n",
      "Iteration: 3522, Loss:2.53 \n",
      "Iteration: 3523, Loss:2.48 \n",
      "Iteration: 3524, Loss:2.17 \n",
      "Iteration: 3525, Loss:2.61 \n",
      "Iteration: 3526, Loss:2.61 \n",
      "Iteration: 3527, Loss:2.69 \n",
      "Iteration: 3528, Loss:2.43 \n",
      "Iteration: 3529, Loss:2.92 \n",
      "Iteration: 3530, Loss:2.94 \n",
      "Iteration: 3531, Loss:2.23 \n",
      "Iteration: 3532, Loss:2.40 \n",
      "Iteration: 3533, Loss:2.38 \n",
      "Iteration: 3534, Loss:2.83 \n",
      "Iteration: 3535, Loss:2.75 \n",
      "Iteration: 3536, Loss:2.64 \n",
      "Iteration: 3537, Loss:2.40 \n",
      "Iteration: 3538, Loss:2.50 \n",
      "Iteration: 3539, Loss:2.39 \n",
      "Iteration: 3540, Loss:2.47 \n",
      "Iteration: 3541, Loss:2.44 \n",
      "Iteration: 3542, Loss:2.81 \n",
      "Iteration: 3543, Loss:2.50 \n",
      "Iteration: 3544, Loss:2.25 \n",
      "Iteration: 3545, Loss:2.20 \n",
      "Iteration: 3546, Loss:2.38 \n",
      "Iteration: 3547, Loss:2.83 \n",
      "Iteration: 3548, Loss:2.29 \n",
      "Iteration: 3549, Loss:2.35 \n",
      "Iteration: 3550, Loss:2.47 \n",
      "Iteration: 3551, Loss:2.24 \n",
      "Iteration: 3552, Loss:2.76 \n",
      "Iteration: 3553, Loss:2.52 \n",
      "Iteration: 3554, Loss:2.46 \n",
      "Iteration: 3555, Loss:1.87 \n",
      "Iteration: 3556, Loss:2.70 \n",
      "Iteration: 3557, Loss:2.73 \n",
      "Iteration: 3558, Loss:2.48 \n",
      "Iteration: 3559, Loss:2.69 \n",
      "Iteration: 3560, Loss:2.88 \n",
      "Iteration: 3561, Loss:2.44 \n",
      "Iteration: 3562, Loss:2.75 \n",
      "Iteration: 3563, Loss:2.25 \n",
      "Iteration: 3564, Loss:2.64 \n",
      "Iteration: 3565, Loss:2.39 \n",
      "Iteration: 3566, Loss:2.37 \n",
      "Iteration: 3567, Loss:2.41 \n",
      "Iteration: 3568, Loss:2.72 \n",
      "Iteration: 3569, Loss:2.59 \n",
      "Iteration: 3570, Loss:2.62 \n",
      "Iteration: 3571, Loss:2.35 \n",
      "Iteration: 3572, Loss:2.01 \n",
      "Iteration: 3573, Loss:2.24 \n",
      "Iteration: 3574, Loss:2.08 \n",
      "Iteration: 3575, Loss:2.94 \n",
      "Iteration: 3576, Loss:2.29 \n",
      "Iteration: 3577, Loss:2.44 \n",
      "Iteration: 3578, Loss:2.21 \n",
      "Iteration: 3579, Loss:2.40 \n",
      "Iteration: 3580, Loss:2.76 \n",
      "Iteration: 3581, Loss:2.40 \n",
      "Iteration: 3582, Loss:2.71 \n",
      "Iteration: 3583, Loss:2.53 \n",
      "Iteration: 3584, Loss:2.42 \n",
      "Iteration: 3585, Loss:2.46 \n",
      "Iteration: 3586, Loss:2.63 \n",
      "Iteration: 3587, Loss:2.25 \n",
      "Iteration: 3588, Loss:2.63 \n",
      "Iteration: 3589, Loss:2.89 \n",
      "Iteration: 3590, Loss:2.48 \n",
      "Iteration: 3591, Loss:2.54 \n",
      "Iteration: 3592, Loss:2.50 \n",
      "Iteration: 3593, Loss:2.26 \n",
      "Iteration: 3594, Loss:2.14 \n",
      "Iteration: 3595, Loss:2.65 \n",
      "Iteration: 3596, Loss:2.28 \n",
      "Iteration: 3597, Loss:2.24 \n",
      "Iteration: 3598, Loss:2.60 \n",
      "Iteration: 3599, Loss:2.79 \n",
      "Iteration: 3600, Loss:2.60 \n",
      "Iteration: 3601, Loss:2.75 \n",
      "Iteration: 3602, Loss:2.65 \n",
      "Iteration: 3603, Loss:2.60 \n",
      "Iteration: 3604, Loss:2.28 \n",
      "Iteration: 3605, Loss:2.90 \n",
      "Iteration: 3606, Loss:2.42 \n",
      "Iteration: 3607, Loss:2.54 \n",
      "Iteration: 3608, Loss:2.83 \n",
      "Iteration: 3609, Loss:3.02 \n",
      "Iteration: 3610, Loss:2.84 \n",
      "Iteration: 3611, Loss:2.62 \n",
      "Iteration: 3612, Loss:2.55 \n",
      "Iteration: 3613, Loss:2.73 \n",
      "Iteration: 3614, Loss:2.65 \n",
      "Iteration: 3615, Loss:2.65 \n",
      "Iteration: 3616, Loss:1.98 \n",
      "Iteration: 3617, Loss:2.74 \n",
      "Iteration: 3618, Loss:2.67 \n",
      "Iteration: 3619, Loss:2.72 \n",
      "Iteration: 3620, Loss:2.26 \n",
      "Iteration: 3621, Loss:2.34 \n",
      "Iteration: 3622, Loss:2.35 \n",
      "Iteration: 3623, Loss:2.86 \n",
      "Iteration: 3624, Loss:2.40 \n",
      "Iteration: 3625, Loss:2.46 \n",
      "Iteration: 3626, Loss:2.70 \n",
      "Iteration: 3627, Loss:2.34 \n",
      "Iteration: 3628, Loss:2.77 \n",
      "Iteration: 3629, Loss:2.85 \n",
      "Iteration: 3630, Loss:2.66 \n",
      "Iteration: 3631, Loss:2.57 \n",
      "Iteration: 3632, Loss:2.81 \n",
      "Iteration: 3633, Loss:3.04 \n",
      "Iteration: 3634, Loss:2.31 \n",
      "Iteration: 3635, Loss:2.79 \n",
      "Iteration: 3636, Loss:2.44 \n",
      "Iteration: 3637, Loss:2.38 \n",
      "Iteration: 3638, Loss:2.60 \n",
      "Iteration: 3639, Loss:2.64 \n",
      "Iteration: 3640, Loss:2.68 \n",
      "Iteration: 3641, Loss:2.85 \n",
      "Iteration: 3642, Loss:2.63 \n",
      "Iteration: 3643, Loss:2.55 \n",
      "Iteration: 3644, Loss:2.45 \n",
      "Iteration: 3645, Loss:2.92 \n",
      "Iteration: 3646, Loss:2.61 \n",
      "Iteration: 3647, Loss:2.81 \n",
      "Iteration: 3648, Loss:2.56 \n",
      "Iteration: 3649, Loss:2.90 \n",
      "Iteration: 3650, Loss:2.37 \n",
      "Iteration: 3651, Loss:2.77 \n",
      "Iteration: 3652, Loss:2.36 \n",
      "Iteration: 3653, Loss:2.87 \n",
      "Iteration: 3654, Loss:2.46 \n",
      "Iteration: 3655, Loss:2.84 \n",
      "Iteration: 3656, Loss:2.24 \n",
      "Iteration: 3657, Loss:2.66 \n",
      "Iteration: 3658, Loss:2.68 \n",
      "Iteration: 3659, Loss:2.22 \n",
      "Iteration: 3660, Loss:2.83 \n",
      "Iteration: 3661, Loss:1.88 \n",
      "Iteration: 3662, Loss:2.32 \n",
      "Iteration: 3663, Loss:2.79 \n",
      "Iteration: 3664, Loss:2.05 \n",
      "Iteration: 3665, Loss:2.37 \n",
      "Iteration: 3666, Loss:3.10 \n",
      "Iteration: 3667, Loss:2.78 \n",
      "Iteration: 3668, Loss:2.59 \n",
      "Iteration: 3669, Loss:2.40 \n",
      "Iteration: 3670, Loss:2.42 \n",
      "Iteration: 3671, Loss:2.71 \n",
      "Iteration: 3672, Loss:2.22 \n",
      "Iteration: 3673, Loss:2.36 \n",
      "Iteration: 3674, Loss:2.75 \n",
      "Iteration: 3675, Loss:2.38 \n",
      "Iteration: 3676, Loss:2.70 \n",
      "Iteration: 3677, Loss:2.70 \n",
      "Iteration: 3678, Loss:2.45 \n",
      "Iteration: 3679, Loss:2.36 \n",
      "Iteration: 3680, Loss:2.11 \n",
      "Iteration: 3681, Loss:2.21 \n",
      "Iteration: 3682, Loss:2.27 \n",
      "Iteration: 3683, Loss:2.61 \n",
      "Iteration: 3684, Loss:2.58 \n",
      "Iteration: 3685, Loss:2.67 \n",
      "Iteration: 3686, Loss:2.53 \n",
      "Iteration: 3687, Loss:2.44 \n",
      "Iteration: 3688, Loss:2.44 \n",
      "Iteration: 3689, Loss:2.47 \n",
      "Iteration: 3690, Loss:2.79 \n",
      "Iteration: 3691, Loss:2.21 \n",
      "Iteration: 3692, Loss:2.67 \n",
      "Iteration: 3693, Loss:2.62 \n",
      "Iteration: 3694, Loss:2.42 \n",
      "Iteration: 3695, Loss:2.62 \n",
      "Iteration: 3696, Loss:2.65 \n",
      "Iteration: 3697, Loss:2.80 \n",
      "Iteration: 3698, Loss:2.37 \n",
      "Iteration: 3699, Loss:2.70 \n",
      "Iteration: 3700, Loss:2.22 \n",
      "Iteration: 3701, Loss:2.92 \n",
      "Iteration: 3702, Loss:2.79 \n",
      "Iteration: 3703, Loss:3.03 \n",
      "Iteration: 3704, Loss:2.13 \n",
      "Iteration: 3705, Loss:2.52 \n",
      "Iteration: 3706, Loss:2.72 \n",
      "Iteration: 3707, Loss:2.45 \n",
      "Iteration: 3708, Loss:2.69 \n",
      "Iteration: 3709, Loss:2.63 \n",
      "Iteration: 3710, Loss:2.65 \n",
      "Iteration: 3711, Loss:2.91 \n",
      "Iteration: 3712, Loss:2.55 \n",
      "Iteration: 3713, Loss:2.72 \n",
      "Iteration: 3714, Loss:2.66 \n",
      "Iteration: 3715, Loss:2.23 \n",
      "Iteration: 3716, Loss:2.73 \n",
      "Iteration: 3717, Loss:2.49 \n",
      "Iteration: 3718, Loss:2.30 \n",
      "Iteration: 3719, Loss:2.47 \n",
      "Iteration: 3720, Loss:2.81 \n",
      "Iteration: 3721, Loss:2.32 \n",
      "Iteration: 3722, Loss:2.62 \n",
      "Iteration: 3723, Loss:2.47 \n",
      "Iteration: 3724, Loss:2.85 \n",
      "Iteration: 3725, Loss:2.65 \n",
      "Iteration: 3726, Loss:2.78 \n",
      "Iteration: 3727, Loss:2.92 \n",
      "Iteration: 3728, Loss:2.54 \n",
      "Iteration: 3729, Loss:2.57 \n",
      "Iteration: 3730, Loss:2.67 \n",
      "Iteration: 3731, Loss:2.08 \n",
      "Iteration: 3732, Loss:2.49 \n",
      "Iteration: 3733, Loss:2.51 \n",
      "Iteration: 3734, Loss:2.24 \n",
      "Iteration: 3735, Loss:2.64 \n",
      "Iteration: 3736, Loss:2.50 \n",
      "Iteration: 3737, Loss:2.10 \n",
      "Iteration: 3738, Loss:2.62 \n",
      "Iteration: 3739, Loss:2.56 \n",
      "Iteration: 3740, Loss:2.65 \n",
      "Iteration: 3741, Loss:2.95 \n",
      "Iteration: 3742, Loss:2.74 \n",
      "Iteration: 3743, Loss:2.42 \n",
      "Iteration: 3744, Loss:2.82 \n",
      "Iteration: 3745, Loss:2.75 \n",
      "Iteration: 3746, Loss:2.06 \n",
      "Iteration: 3747, Loss:2.78 \n",
      "Iteration: 3748, Loss:2.78 \n",
      "Iteration: 3749, Loss:2.20 \n",
      "Iteration: 3750, Loss:2.48 \n",
      "Iteration: 3751, Loss:2.75 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_3750.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 3752, Loss:2.79 \n",
      "Iteration: 3753, Loss:2.45 \n",
      "Iteration: 3754, Loss:3.01 \n",
      "Iteration: 3755, Loss:2.82 \n",
      "Iteration: 3756, Loss:2.45 \n",
      "Iteration: 3757, Loss:2.30 \n",
      "Iteration: 3758, Loss:2.40 \n",
      "Iteration: 3759, Loss:2.69 \n",
      "Iteration: 3760, Loss:2.75 \n",
      "Iteration: 3761, Loss:2.45 \n",
      "Iteration: 3762, Loss:2.48 \n",
      "Iteration: 3763, Loss:2.65 \n",
      "Iteration: 3764, Loss:2.17 \n",
      "Iteration: 3765, Loss:2.80 \n",
      "Iteration: 3766, Loss:2.79 \n",
      "Iteration: 3767, Loss:2.42 \n",
      "Iteration: 3768, Loss:2.47 \n",
      "Iteration: 3769, Loss:2.37 \n",
      "Iteration: 3770, Loss:2.50 \n",
      "Iteration: 3771, Loss:2.13 \n",
      "Iteration: 3772, Loss:2.10 \n",
      "Iteration: 3773, Loss:2.71 \n",
      "Iteration: 3774, Loss:2.75 \n",
      "Iteration: 3775, Loss:2.28 \n",
      "Iteration: 3776, Loss:2.43 \n",
      "Iteration: 3777, Loss:2.44 \n",
      "Iteration: 3778, Loss:2.54 \n",
      "Iteration: 3779, Loss:2.46 \n",
      "Iteration: 3780, Loss:2.41 \n",
      "Iteration: 3781, Loss:2.43 \n",
      "Iteration: 3782, Loss:2.71 \n",
      "Iteration: 3783, Loss:2.37 \n",
      "Iteration: 3784, Loss:2.80 \n",
      "Iteration: 3785, Loss:2.73 \n",
      "Iteration: 3786, Loss:2.35 \n",
      "Iteration: 3787, Loss:2.47 \n",
      "Iteration: 3788, Loss:2.60 \n",
      "Iteration: 3789, Loss:2.60 \n",
      "Iteration: 3790, Loss:2.61 \n",
      "Iteration: 3791, Loss:2.53 \n",
      "Iteration: 3792, Loss:2.58 \n",
      "Iteration: 3793, Loss:2.75 \n",
      "Iteration: 3794, Loss:2.49 \n",
      "Iteration: 3795, Loss:2.66 \n",
      "Iteration: 3796, Loss:2.78 \n",
      "Iteration: 3797, Loss:2.37 \n",
      "Iteration: 3798, Loss:2.28 \n",
      "Iteration: 3799, Loss:2.26 \n",
      "Iteration: 3800, Loss:2.76 \n",
      "Iteration: 3801, Loss:2.87 \n",
      "Iteration: 3802, Loss:2.71 \n",
      "Iteration: 3803, Loss:2.27 \n",
      "Iteration: 3804, Loss:2.05 \n",
      "Iteration: 3805, Loss:2.07 \n",
      "Iteration: 3806, Loss:2.25 \n",
      "Iteration: 3807, Loss:2.75 \n",
      "Iteration: 3808, Loss:2.23 \n",
      "Iteration: 3809, Loss:2.59 \n",
      "Iteration: 3810, Loss:2.30 \n",
      "Iteration: 3811, Loss:2.16 \n",
      "Iteration: 3812, Loss:2.37 \n",
      "Iteration: 3813, Loss:2.66 \n",
      "Iteration: 3814, Loss:2.50 \n",
      "Iteration: 3815, Loss:2.63 \n",
      "Iteration: 3816, Loss:2.71 \n",
      "Iteration: 3817, Loss:2.35 \n",
      "Iteration: 3818, Loss:2.67 \n",
      "Iteration: 3819, Loss:2.36 \n",
      "Iteration: 3820, Loss:2.80 \n",
      "Iteration: 3821, Loss:2.53 \n",
      "Iteration: 3822, Loss:2.67 \n",
      "Iteration: 3823, Loss:2.48 \n",
      "Iteration: 3824, Loss:2.51 \n",
      "Iteration: 3825, Loss:2.62 \n",
      "Iteration: 3826, Loss:2.59 \n",
      "Iteration: 3827, Loss:2.62 \n",
      "Iteration: 3828, Loss:2.72 \n",
      "Iteration: 3829, Loss:2.30 \n",
      "Iteration: 3830, Loss:2.47 \n",
      "Iteration: 3831, Loss:2.28 \n",
      "Iteration: 3832, Loss:2.85 \n",
      "Iteration: 3833, Loss:3.03 \n",
      "Iteration: 3834, Loss:2.96 \n",
      "Iteration: 3835, Loss:2.27 \n",
      "Iteration: 3836, Loss:2.81 \n",
      "Iteration: 3837, Loss:2.65 \n",
      "Iteration: 3838, Loss:2.66 \n",
      "Iteration: 3839, Loss:2.41 \n",
      "Iteration: 3840, Loss:2.32 \n",
      "Iteration: 3841, Loss:2.41 \n",
      "Iteration: 3842, Loss:2.44 \n",
      "Iteration: 3843, Loss:2.61 \n",
      "Iteration: 3844, Loss:2.74 \n",
      "Iteration: 3845, Loss:3.02 \n",
      "Iteration: 3846, Loss:2.46 \n",
      "Iteration: 3847, Loss:2.64 \n",
      "Iteration: 3848, Loss:2.13 \n",
      "Iteration: 3849, Loss:2.18 \n",
      "Iteration: 3850, Loss:2.58 \n",
      "Iteration: 3851, Loss:2.75 \n",
      "Iteration: 3852, Loss:2.90 \n",
      "Iteration: 3853, Loss:2.27 \n",
      "Iteration: 3854, Loss:1.95 \n",
      "Iteration: 3855, Loss:2.80 \n",
      "Iteration: 3856, Loss:2.39 \n",
      "Iteration: 3857, Loss:2.22 \n",
      "Iteration: 3858, Loss:2.68 \n",
      "Iteration: 3859, Loss:2.51 \n",
      "Iteration: 3860, Loss:2.64 \n",
      "Iteration: 3861, Loss:2.44 \n",
      "Iteration: 3862, Loss:2.50 \n",
      "Iteration: 3863, Loss:2.66 \n",
      "Iteration: 3864, Loss:2.45 \n",
      "Iteration: 3865, Loss:2.55 \n",
      "Iteration: 3866, Loss:2.10 \n",
      "Iteration: 3867, Loss:2.51 \n",
      "Iteration: 3868, Loss:2.57 \n",
      "Iteration: 3869, Loss:2.47 \n",
      "Iteration: 3870, Loss:2.23 \n",
      "Iteration: 3871, Loss:2.69 \n",
      "Iteration: 3872, Loss:2.72 \n",
      "Iteration: 3873, Loss:2.78 \n",
      "Iteration: 3874, Loss:2.93 \n",
      "Iteration: 3875, Loss:2.71 \n",
      "Iteration: 3876, Loss:2.37 \n",
      "Iteration: 3877, Loss:2.62 \n",
      "Iteration: 3878, Loss:2.09 \n",
      "Iteration: 3879, Loss:2.46 \n",
      "Iteration: 3880, Loss:2.74 \n",
      "Iteration: 3881, Loss:2.58 \n",
      "Iteration: 3882, Loss:2.22 \n",
      "Iteration: 3883, Loss:3.21 \n",
      "Iteration: 3884, Loss:2.83 \n",
      "Iteration: 3885, Loss:2.75 \n",
      "Iteration: 3886, Loss:2.28 \n",
      "Iteration: 3887, Loss:2.59 \n",
      "Iteration: 3888, Loss:2.28 \n",
      "Iteration: 3889, Loss:2.71 \n",
      "Iteration: 3890, Loss:2.53 \n",
      "Iteration: 3891, Loss:2.18 \n",
      "Iteration: 3892, Loss:2.70 \n",
      "Iteration: 3893, Loss:2.81 \n",
      "Iteration: 3894, Loss:2.64 \n",
      "Iteration: 3895, Loss:2.84 \n",
      "Iteration: 3896, Loss:2.75 \n",
      "Iteration: 3897, Loss:2.75 \n",
      "Iteration: 3898, Loss:2.42 \n",
      "Iteration: 3899, Loss:2.74 \n",
      "Iteration: 3900, Loss:2.54 \n",
      "Iteration: 3901, Loss:2.20 \n",
      "Iteration: 3902, Loss:2.78 \n",
      "Iteration: 3903, Loss:2.63 \n",
      "Iteration: 3904, Loss:2.29 \n",
      "Iteration: 3905, Loss:2.66 \n",
      "Iteration: 3906, Loss:2.87 \n",
      "Iteration: 3907, Loss:2.71 \n",
      "Iteration: 3908, Loss:2.83 \n",
      "Iteration: 3909, Loss:2.64 \n",
      "Iteration: 3910, Loss:2.25 \n",
      "Iteration: 3911, Loss:2.48 \n",
      "Iteration: 3912, Loss:2.58 \n",
      "Iteration: 3913, Loss:2.55 \n",
      "Iteration: 3914, Loss:2.83 \n",
      "Iteration: 3915, Loss:2.90 \n",
      "Iteration: 3916, Loss:2.23 \n",
      "Iteration: 3917, Loss:2.80 \n",
      "Iteration: 3918, Loss:2.29 \n",
      "Iteration: 3919, Loss:2.10 \n",
      "Iteration: 3920, Loss:2.25 \n",
      "Iteration: 3921, Loss:2.61 \n",
      "Iteration: 3922, Loss:2.63 \n",
      "Iteration: 3923, Loss:2.61 \n",
      "Iteration: 3924, Loss:2.51 \n",
      "Iteration: 3925, Loss:2.68 \n",
      "Iteration: 3926, Loss:2.30 \n",
      "Iteration: 3927, Loss:2.69 \n",
      "Iteration: 3928, Loss:2.46 \n",
      "Iteration: 3929, Loss:2.74 \n",
      "Iteration: 3930, Loss:2.87 \n",
      "Iteration: 3931, Loss:2.81 \n",
      "Iteration: 3932, Loss:2.52 \n",
      "Iteration: 3933, Loss:2.23 \n",
      "Iteration: 3934, Loss:2.56 \n",
      "Iteration: 3935, Loss:2.68 \n",
      "Iteration: 3936, Loss:2.69 \n",
      "Iteration: 3937, Loss:2.49 \n",
      "Iteration: 3938, Loss:2.39 \n",
      "Iteration: 3939, Loss:2.12 \n",
      "Iteration: 3940, Loss:2.32 \n",
      "Iteration: 3941, Loss:2.41 \n",
      "Iteration: 3942, Loss:2.35 \n",
      "Iteration: 3943, Loss:2.29 \n",
      "Iteration: 3944, Loss:2.58 \n",
      "Iteration: 3945, Loss:2.72 \n",
      "Iteration: 3946, Loss:2.72 \n",
      "Iteration: 3947, Loss:3.01 \n",
      "Iteration: 3948, Loss:2.11 \n",
      "Iteration: 3949, Loss:1.94 \n",
      "Iteration: 3950, Loss:2.43 \n",
      "Iteration: 3951, Loss:2.70 \n",
      "Iteration: 3952, Loss:2.65 \n",
      "Iteration: 3953, Loss:2.85 \n",
      "Iteration: 3954, Loss:2.62 \n",
      "Iteration: 3955, Loss:2.00 \n",
      "Iteration: 3956, Loss:2.69 \n",
      "Iteration: 3957, Loss:2.30 \n",
      "Iteration: 3958, Loss:2.82 \n",
      "Iteration: 3959, Loss:2.73 \n",
      "Iteration: 3960, Loss:2.32 \n",
      "Iteration: 3961, Loss:2.61 \n",
      "Iteration: 3962, Loss:2.37 \n",
      "Iteration: 3963, Loss:2.93 \n",
      "Iteration: 3964, Loss:2.99 \n",
      "Iteration: 3965, Loss:2.53 \n",
      "Iteration: 3966, Loss:2.44 \n",
      "Iteration: 3967, Loss:2.21 \n",
      "Iteration: 3968, Loss:2.40 \n",
      "Iteration: 3969, Loss:2.57 \n",
      "Iteration: 3970, Loss:2.65 \n",
      "Iteration: 3971, Loss:2.75 \n",
      "Iteration: 3972, Loss:2.80 \n",
      "Iteration: 3973, Loss:2.51 \n",
      "Iteration: 3974, Loss:2.30 \n",
      "Iteration: 3975, Loss:2.16 \n",
      "Iteration: 3976, Loss:2.87 \n",
      "Iteration: 3977, Loss:2.67 \n",
      "Iteration: 3978, Loss:2.33 \n",
      "Iteration: 3979, Loss:2.17 \n",
      "Iteration: 3980, Loss:2.58 \n",
      "Iteration: 3981, Loss:2.59 \n",
      "Iteration: 3982, Loss:2.70 \n",
      "Iteration: 3983, Loss:2.66 \n",
      "Iteration: 3984, Loss:2.78 \n",
      "Iteration: 3985, Loss:2.71 \n",
      "Iteration: 3986, Loss:2.00 \n",
      "Iteration: 3987, Loss:2.50 \n",
      "Iteration: 3988, Loss:2.37 \n",
      "Iteration: 3989, Loss:2.07 \n",
      "Iteration: 3990, Loss:2.69 \n",
      "Iteration: 3991, Loss:2.68 \n",
      "Iteration: 3992, Loss:2.37 \n",
      "Iteration: 3993, Loss:2.53 \n",
      "Iteration: 3994, Loss:2.07 \n",
      "Iteration: 3995, Loss:2.57 \n",
      "Iteration: 3996, Loss:2.52 \n",
      "Iteration: 3997, Loss:2.54 \n",
      "Iteration: 3998, Loss:2.82 \n",
      "Iteration: 3999, Loss:2.35 \n",
      "Iteration: 4000, Loss:2.55 \n",
      "Iteration: 4001, Loss:2.95 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_4000.ckpt\n",
      "Iteration: 4002, Loss:2.57 \n",
      "Iteration: 4003, Loss:2.61 \n",
      "Iteration: 4004, Loss:2.61 \n",
      "Iteration: 4005, Loss:2.78 \n",
      "Iteration: 4006, Loss:2.80 \n",
      "Iteration: 4007, Loss:2.24 \n",
      "Iteration: 4008, Loss:2.71 \n",
      "Iteration: 4009, Loss:2.55 \n",
      "Iteration: 4010, Loss:2.33 \n",
      "Iteration: 4011, Loss:2.57 \n",
      "Iteration: 4012, Loss:2.60 \n",
      "Iteration: 4013, Loss:2.35 \n",
      "Iteration: 4014, Loss:2.50 \n",
      "Iteration: 4015, Loss:2.73 \n",
      "Iteration: 4016, Loss:2.44 \n",
      "Iteration: 4017, Loss:2.26 \n",
      "Iteration: 4018, Loss:2.34 \n",
      "Iteration: 4019, Loss:2.95 \n",
      "Iteration: 4020, Loss:2.37 \n",
      "Iteration: 4021, Loss:2.63 \n",
      "Iteration: 4022, Loss:2.30 \n",
      "Iteration: 4023, Loss:2.12 \n",
      "Iteration: 4024, Loss:2.88 \n",
      "Iteration: 4025, Loss:2.62 \n",
      "Iteration: 4026, Loss:2.77 \n",
      "Iteration: 4027, Loss:2.49 \n",
      "Iteration: 4028, Loss:2.63 \n",
      "Iteration: 4029, Loss:2.41 \n",
      "Iteration: 4030, Loss:2.83 \n",
      "Iteration: 4031, Loss:2.29 \n",
      "Iteration: 4032, Loss:2.41 \n",
      "Iteration: 4033, Loss:2.34 \n",
      "Iteration: 4034, Loss:1.91 \n",
      "Iteration: 4035, Loss:2.61 \n",
      "Iteration: 4036, Loss:2.39 \n",
      "Iteration: 4037, Loss:2.46 \n",
      "Iteration: 4038, Loss:2.85 \n",
      "Iteration: 4039, Loss:2.53 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4040, Loss:2.58 \n",
      "Iteration: 4041, Loss:2.76 \n",
      "Iteration: 4042, Loss:2.49 \n",
      "Iteration: 4043, Loss:2.61 \n",
      "Iteration: 4044, Loss:2.99 \n",
      "Iteration: 4045, Loss:2.69 \n",
      "Iteration: 4046, Loss:2.42 \n",
      "Iteration: 4047, Loss:2.39 \n",
      "Iteration: 4048, Loss:2.17 \n",
      "Iteration: 4049, Loss:2.74 \n",
      "Iteration: 4050, Loss:2.52 \n",
      "Iteration: 4051, Loss:3.17 \n",
      "Iteration: 4052, Loss:2.38 \n",
      "Iteration: 4053, Loss:2.40 \n",
      "Iteration: 4054, Loss:2.10 \n",
      "Iteration: 4055, Loss:2.25 \n",
      "Iteration: 4056, Loss:2.93 \n",
      "Iteration: 4057, Loss:2.54 \n",
      "Iteration: 4058, Loss:2.82 \n",
      "Iteration: 4059, Loss:2.60 \n",
      "Iteration: 4060, Loss:2.75 \n",
      "Iteration: 4061, Loss:2.71 \n",
      "Iteration: 4062, Loss:2.59 \n",
      "Iteration: 4063, Loss:2.87 \n",
      "Iteration: 4064, Loss:2.52 \n",
      "Iteration: 4065, Loss:2.30 \n",
      "Iteration: 4066, Loss:2.98 \n",
      "Iteration: 4067, Loss:2.89 \n",
      "Iteration: 4068, Loss:2.10 \n",
      "Iteration: 4069, Loss:2.60 \n",
      "Iteration: 4070, Loss:2.77 \n",
      "Iteration: 4071, Loss:2.52 \n",
      "Iteration: 4072, Loss:2.49 \n",
      "Iteration: 4073, Loss:2.31 \n",
      "Iteration: 4074, Loss:2.80 \n",
      "Iteration: 4075, Loss:2.51 \n",
      "Iteration: 4076, Loss:2.71 \n",
      "Iteration: 4077, Loss:2.26 \n",
      "Iteration: 4078, Loss:2.34 \n",
      "Iteration: 4079, Loss:2.60 \n",
      "Iteration: 4080, Loss:2.66 \n",
      "Iteration: 4081, Loss:2.42 \n",
      "Iteration: 4082, Loss:3.12 \n",
      "Iteration: 4083, Loss:2.56 \n",
      "Iteration: 4084, Loss:2.42 \n",
      "Iteration: 4085, Loss:2.40 \n",
      "Iteration: 4086, Loss:2.56 \n",
      "Iteration: 4087, Loss:2.66 \n",
      "Iteration: 4088, Loss:2.42 \n",
      "Iteration: 4089, Loss:2.80 \n",
      "Iteration: 4090, Loss:2.53 \n",
      "Iteration: 4091, Loss:2.90 \n",
      "Iteration: 4092, Loss:2.47 \n",
      "Iteration: 4093, Loss:2.16 \n",
      "Iteration: 4094, Loss:2.60 \n",
      "Iteration: 4095, Loss:2.58 \n",
      "Iteration: 4096, Loss:2.43 \n",
      "Iteration: 4097, Loss:2.81 \n",
      "Iteration: 4098, Loss:2.60 \n",
      "Iteration: 4099, Loss:2.83 \n",
      "Iteration: 4100, Loss:2.29 \n",
      "Iteration: 4101, Loss:2.48 \n",
      "Iteration: 4102, Loss:2.63 \n",
      "Iteration: 4103, Loss:2.63 \n",
      "Iteration: 4104, Loss:2.62 \n",
      "Iteration: 4105, Loss:2.66 \n",
      "Iteration: 4106, Loss:2.26 \n",
      "Iteration: 4107, Loss:2.53 \n",
      "Iteration: 4108, Loss:2.16 \n",
      "Iteration: 4109, Loss:2.48 \n",
      "Iteration: 4110, Loss:2.50 \n",
      "Iteration: 4111, Loss:2.67 \n",
      "Iteration: 4112, Loss:2.31 \n",
      "Iteration: 4113, Loss:2.54 \n",
      "Iteration: 4114, Loss:2.54 \n",
      "Iteration: 4115, Loss:2.37 \n",
      "Iteration: 4116, Loss:2.43 \n",
      "Iteration: 4117, Loss:2.44 \n",
      "Iteration: 4118, Loss:2.36 \n",
      "Iteration: 4119, Loss:2.44 \n",
      "Iteration: 4120, Loss:2.26 \n",
      "Iteration: 4121, Loss:2.70 \n",
      "Iteration: 4122, Loss:2.88 \n",
      "Iteration: 4123, Loss:2.36 \n",
      "Iteration: 4124, Loss:2.60 \n",
      "Iteration: 4125, Loss:2.73 \n",
      "Iteration: 4126, Loss:2.27 \n",
      "Iteration: 4127, Loss:2.82 \n",
      "Iteration: 4128, Loss:2.70 \n",
      "Iteration: 4129, Loss:2.42 \n",
      "Iteration: 4130, Loss:2.25 \n",
      "Iteration: 4131, Loss:2.74 \n",
      "Iteration: 4132, Loss:2.19 \n",
      "Iteration: 4133, Loss:2.11 \n",
      "Iteration: 4134, Loss:2.72 \n",
      "Iteration: 4135, Loss:2.76 \n",
      "Iteration: 4136, Loss:2.44 \n",
      "Iteration: 4137, Loss:2.27 \n",
      "Iteration: 4138, Loss:2.39 \n",
      "Iteration: 4139, Loss:2.67 \n",
      "Iteration: 4140, Loss:2.71 \n",
      "Iteration: 4141, Loss:2.76 \n",
      "Iteration: 4142, Loss:2.22 \n",
      "Iteration: 4143, Loss:2.49 \n",
      "Iteration: 4144, Loss:2.55 \n",
      "Iteration: 4145, Loss:2.56 \n",
      "Iteration: 4146, Loss:2.66 \n",
      "Iteration: 4147, Loss:2.85 \n",
      "Iteration: 4148, Loss:2.56 \n",
      "Iteration: 4149, Loss:2.67 \n",
      "Iteration: 4150, Loss:2.28 \n",
      "Iteration: 4151, Loss:2.68 \n",
      "Iteration: 4152, Loss:2.51 \n",
      "Iteration: 4153, Loss:2.76 \n",
      "Iteration: 4154, Loss:2.33 \n",
      "Iteration: 4155, Loss:2.52 \n",
      "Iteration: 4156, Loss:2.47 \n",
      "Iteration: 4157, Loss:2.77 \n",
      "Iteration: 4158, Loss:2.71 \n",
      "Iteration: 4159, Loss:2.32 \n",
      "Iteration: 4160, Loss:2.35 \n",
      "Iteration: 4161, Loss:2.42 \n",
      "Iteration: 4162, Loss:2.88 \n",
      "Iteration: 4163, Loss:2.33 \n",
      "Iteration: 4164, Loss:2.51 \n",
      "Iteration: 4165, Loss:2.72 \n",
      "Iteration: 4166, Loss:2.90 \n",
      "Iteration: 4167, Loss:2.68 \n",
      "Iteration: 4168, Loss:2.40 \n",
      "Iteration: 4169, Loss:2.66 \n",
      "Iteration: 4170, Loss:2.42 \n",
      "Iteration: 4171, Loss:2.45 \n",
      "Iteration: 4172, Loss:2.28 \n",
      "Iteration: 4173, Loss:1.99 \n",
      "Iteration: 4174, Loss:2.67 \n",
      "Iteration: 4175, Loss:2.72 \n",
      "Iteration: 4176, Loss:2.62 \n",
      "Iteration: 4177, Loss:2.63 \n",
      "Iteration: 4178, Loss:2.40 \n",
      "Iteration: 4179, Loss:2.76 \n",
      "Iteration: 4180, Loss:2.89 \n",
      "Iteration: 4181, Loss:2.51 \n",
      "Iteration: 4182, Loss:1.99 \n",
      "Iteration: 4183, Loss:2.86 \n",
      "Iteration: 4184, Loss:2.72 \n",
      "Iteration: 4185, Loss:2.10 \n",
      "Iteration: 4186, Loss:2.51 \n",
      "Iteration: 4187, Loss:2.13 \n",
      "Iteration: 4188, Loss:2.23 \n",
      "Iteration: 4189, Loss:2.34 \n",
      "Iteration: 4190, Loss:2.31 \n",
      "Iteration: 4191, Loss:2.78 \n",
      "Iteration: 4192, Loss:2.50 \n",
      "Iteration: 4193, Loss:2.61 \n",
      "Iteration: 4194, Loss:3.04 \n",
      "Iteration: 4195, Loss:2.55 \n",
      "Iteration: 4196, Loss:2.80 \n",
      "Iteration: 4197, Loss:2.81 \n",
      "Iteration: 4198, Loss:2.61 \n",
      "Iteration: 4199, Loss:2.32 \n",
      "Iteration: 4200, Loss:2.94 \n",
      "Iteration: 4201, Loss:2.53 \n",
      "Iteration: 4202, Loss:2.46 \n",
      "Iteration: 4203, Loss:2.79 \n",
      "Iteration: 4204, Loss:2.34 \n",
      "Iteration: 4205, Loss:2.34 \n",
      "Iteration: 4206, Loss:3.04 \n",
      "Iteration: 4207, Loss:2.59 \n",
      "Iteration: 4208, Loss:2.97 \n",
      "Iteration: 4209, Loss:2.70 \n",
      "Iteration: 4210, Loss:2.51 \n",
      "Iteration: 4211, Loss:2.33 \n",
      "Iteration: 4212, Loss:2.72 \n",
      "Iteration: 4213, Loss:2.43 \n",
      "Iteration: 4214, Loss:2.85 \n",
      "Iteration: 4215, Loss:2.82 \n",
      "Iteration: 4216, Loss:2.59 \n",
      "Iteration: 4217, Loss:2.42 \n",
      "Iteration: 4218, Loss:2.41 \n",
      "Iteration: 4219, Loss:2.47 \n",
      "Iteration: 4220, Loss:2.43 \n",
      "Iteration: 4221, Loss:2.57 \n",
      "Iteration: 4222, Loss:2.90 \n",
      "Iteration: 4223, Loss:2.33 \n",
      "Iteration: 4224, Loss:2.61 \n",
      "Iteration: 4225, Loss:2.47 \n",
      "Iteration: 4226, Loss:2.61 \n",
      "Iteration: 4227, Loss:2.52 \n",
      "Iteration: 4228, Loss:2.61 \n",
      "Iteration: 4229, Loss:2.78 \n",
      "Iteration: 4230, Loss:2.33 \n",
      "Iteration: 4231, Loss:2.28 \n",
      "Iteration: 4232, Loss:2.34 \n",
      "Iteration: 4233, Loss:2.10 \n",
      "Iteration: 4234, Loss:2.52 \n",
      "Iteration: 4235, Loss:2.88 \n",
      "Iteration: 4236, Loss:2.41 \n",
      "Iteration: 4237, Loss:2.42 \n",
      "Iteration: 4238, Loss:2.71 \n",
      "Iteration: 4239, Loss:2.70 \n",
      "Iteration: 4240, Loss:2.43 \n",
      "Iteration: 4241, Loss:2.58 \n",
      "Iteration: 4242, Loss:2.27 \n",
      "Iteration: 4243, Loss:2.50 \n",
      "Iteration: 4244, Loss:2.61 \n",
      "Iteration: 4245, Loss:2.48 \n",
      "Iteration: 4246, Loss:2.66 \n",
      "Iteration: 4247, Loss:2.34 \n",
      "Iteration: 4248, Loss:2.69 \n",
      "Iteration: 4249, Loss:2.70 \n",
      "Iteration: 4250, Loss:2.59 \n",
      "Iteration: 4251, Loss:2.54 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_4250.ckpt\n",
      "Iteration: 4252, Loss:2.56 \n",
      "Iteration: 4253, Loss:2.66 \n",
      "Iteration: 4254, Loss:2.75 \n",
      "Iteration: 4255, Loss:2.59 \n",
      "Iteration: 4256, Loss:2.76 \n",
      "Iteration: 4257, Loss:2.45 \n",
      "Iteration: 4258, Loss:2.47 \n",
      "Iteration: 4259, Loss:2.45 \n",
      "Iteration: 4260, Loss:2.47 \n",
      "Iteration: 4261, Loss:2.30 \n",
      "Iteration: 4262, Loss:2.68 \n",
      "Iteration: 4263, Loss:2.70 \n",
      "Iteration: 4264, Loss:2.42 \n",
      "Iteration: 4265, Loss:2.50 \n",
      "Iteration: 4266, Loss:2.51 \n",
      "Iteration: 4267, Loss:2.56 \n",
      "Iteration: 4268, Loss:2.60 \n",
      "Iteration: 4269, Loss:2.81 \n",
      "Iteration: 4270, Loss:2.70 \n",
      "Iteration: 4271, Loss:2.18 \n",
      "Iteration: 4272, Loss:2.72 \n",
      "Iteration: 4273, Loss:2.66 \n",
      "Iteration: 4274, Loss:2.66 \n",
      "Iteration: 4275, Loss:2.87 \n",
      "Iteration: 4276, Loss:2.34 \n",
      "Iteration: 4277, Loss:2.23 \n",
      "Iteration: 4278, Loss:2.54 \n",
      "Iteration: 4279, Loss:2.89 \n",
      "Iteration: 4280, Loss:2.70 \n",
      "Iteration: 4281, Loss:2.04 \n",
      "Iteration: 4282, Loss:2.71 \n",
      "Iteration: 4283, Loss:2.72 \n",
      "Iteration: 4284, Loss:2.66 \n",
      "Iteration: 4285, Loss:2.60 \n",
      "Iteration: 4286, Loss:2.63 \n",
      "Iteration: 4287, Loss:2.67 \n",
      "Iteration: 4288, Loss:2.92 \n",
      "Iteration: 4289, Loss:2.66 \n",
      "Iteration: 4290, Loss:2.65 \n",
      "Iteration: 4291, Loss:2.60 \n",
      "Iteration: 4292, Loss:2.71 \n",
      "Iteration: 4293, Loss:2.48 \n",
      "Iteration: 4294, Loss:2.73 \n",
      "Iteration: 4295, Loss:2.75 \n",
      "Iteration: 4296, Loss:2.85 \n",
      "Iteration: 4297, Loss:2.73 \n",
      "Iteration: 4298, Loss:2.83 \n",
      "Iteration: 4299, Loss:2.17 \n",
      "Iteration: 4300, Loss:2.48 \n",
      "Iteration: 4301, Loss:2.86 \n",
      "Iteration: 4302, Loss:2.21 \n",
      "Iteration: 4303, Loss:2.21 \n",
      "Iteration: 4304, Loss:2.42 \n",
      "Iteration: 4305, Loss:2.28 \n",
      "Iteration: 4306, Loss:2.64 \n",
      "Iteration: 4307, Loss:2.67 \n",
      "Iteration: 4308, Loss:2.42 \n",
      "Iteration: 4309, Loss:2.41 \n",
      "Iteration: 4310, Loss:2.52 \n",
      "Iteration: 4311, Loss:2.32 \n",
      "Iteration: 4312, Loss:2.80 \n",
      "Iteration: 4313, Loss:2.39 \n",
      "Iteration: 4314, Loss:2.77 \n",
      "Iteration: 4315, Loss:2.53 \n",
      "Iteration: 4316, Loss:2.32 \n",
      "Iteration: 4317, Loss:2.26 \n",
      "Iteration: 4318, Loss:2.20 \n",
      "Iteration: 4319, Loss:2.67 \n",
      "Iteration: 4320, Loss:2.67 \n",
      "Iteration: 4321, Loss:2.83 \n",
      "Iteration: 4322, Loss:2.77 \n",
      "Iteration: 4323, Loss:2.14 \n",
      "Iteration: 4324, Loss:2.07 \n",
      "Iteration: 4325, Loss:2.44 \n",
      "Iteration: 4326, Loss:2.89 \n",
      "Iteration: 4327, Loss:2.34 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4328, Loss:2.94 \n",
      "Iteration: 4329, Loss:2.91 \n",
      "Iteration: 4330, Loss:2.97 \n",
      "Iteration: 4331, Loss:2.56 \n",
      "Iteration: 4332, Loss:2.09 \n",
      "Iteration: 4333, Loss:2.26 \n",
      "Iteration: 4334, Loss:2.69 \n",
      "Iteration: 4335, Loss:2.61 \n",
      "Iteration: 4336, Loss:1.88 \n",
      "Iteration: 4337, Loss:2.87 \n",
      "Iteration: 4338, Loss:2.60 \n",
      "Iteration: 4339, Loss:2.53 \n",
      "Iteration: 4340, Loss:2.53 \n",
      "Iteration: 4341, Loss:2.45 \n",
      "Iteration: 4342, Loss:2.29 \n",
      "Iteration: 4343, Loss:2.44 \n",
      "Iteration: 4344, Loss:2.78 \n",
      "Iteration: 4345, Loss:2.87 \n",
      "Iteration: 4346, Loss:2.60 \n",
      "Iteration: 4347, Loss:2.76 \n",
      "Iteration: 4348, Loss:2.24 \n",
      "Iteration: 4349, Loss:2.50 \n",
      "Iteration: 4350, Loss:2.38 \n",
      "Iteration: 4351, Loss:2.35 \n",
      "Iteration: 4352, Loss:1.92 \n",
      "Iteration: 4353, Loss:2.14 \n",
      "Iteration: 4354, Loss:2.94 \n",
      "Iteration: 4355, Loss:2.20 \n",
      "Iteration: 4356, Loss:2.43 \n",
      "Iteration: 4357, Loss:2.63 \n",
      "Iteration: 4358, Loss:2.58 \n",
      "Iteration: 4359, Loss:2.48 \n",
      "Iteration: 4360, Loss:2.21 \n",
      "Iteration: 4361, Loss:2.83 \n",
      "Iteration: 4362, Loss:2.77 \n",
      "Iteration: 4363, Loss:2.41 \n",
      "Iteration: 4364, Loss:2.08 \n",
      "Iteration: 4365, Loss:2.36 \n",
      "Iteration: 4366, Loss:2.55 \n",
      "Iteration: 4367, Loss:2.69 \n",
      "Iteration: 4368, Loss:2.64 \n",
      "Iteration: 4369, Loss:2.63 \n",
      "Iteration: 4370, Loss:2.71 \n",
      "Iteration: 4371, Loss:2.66 \n",
      "Iteration: 4372, Loss:2.68 \n",
      "Iteration: 4373, Loss:2.42 \n",
      "Iteration: 4374, Loss:3.02 \n",
      "Iteration: 4375, Loss:2.06 \n",
      "Iteration: 4376, Loss:2.22 \n",
      "Iteration: 4377, Loss:2.41 \n",
      "Iteration: 4378, Loss:2.12 \n",
      "Iteration: 4379, Loss:2.63 \n",
      "Iteration: 4380, Loss:2.66 \n",
      "Iteration: 4381, Loss:2.24 \n",
      "Iteration: 4382, Loss:2.61 \n",
      "Iteration: 4383, Loss:2.56 \n",
      "Iteration: 4384, Loss:2.26 \n",
      "Iteration: 4385, Loss:2.99 \n",
      "Iteration: 4386, Loss:2.58 \n",
      "Iteration: 4387, Loss:2.48 \n",
      "Iteration: 4388, Loss:2.76 \n",
      "Iteration: 4389, Loss:2.71 \n",
      "Iteration: 4390, Loss:2.37 \n",
      "Iteration: 4391, Loss:2.47 \n",
      "Iteration: 4392, Loss:2.39 \n",
      "Iteration: 4393, Loss:2.67 \n",
      "Iteration: 4394, Loss:2.32 \n",
      "Iteration: 4395, Loss:2.81 \n",
      "Iteration: 4396, Loss:2.36 \n",
      "Iteration: 4397, Loss:2.17 \n",
      "Iteration: 4398, Loss:2.54 \n",
      "Iteration: 4399, Loss:3.01 \n",
      "Iteration: 4400, Loss:2.49 \n",
      "Iteration: 4401, Loss:2.41 \n",
      "Iteration: 4402, Loss:2.57 \n",
      "Iteration: 4403, Loss:2.55 \n",
      "Iteration: 4404, Loss:2.76 \n",
      "Iteration: 4405, Loss:3.13 \n",
      "Iteration: 4406, Loss:2.74 \n",
      "Iteration: 4407, Loss:2.50 \n",
      "Iteration: 4408, Loss:2.78 \n",
      "Iteration: 4409, Loss:2.48 \n",
      "Iteration: 4410, Loss:2.78 \n",
      "Iteration: 4411, Loss:2.58 \n",
      "Iteration: 4412, Loss:2.46 \n",
      "Iteration: 4413, Loss:2.92 \n",
      "Iteration: 4414, Loss:2.41 \n",
      "Iteration: 4415, Loss:2.62 \n",
      "Iteration: 4416, Loss:2.43 \n",
      "Iteration: 4417, Loss:2.29 \n",
      "Iteration: 4418, Loss:2.67 \n",
      "Iteration: 4419, Loss:2.41 \n",
      "Iteration: 4420, Loss:2.20 \n",
      "Iteration: 4421, Loss:2.52 \n",
      "Iteration: 4422, Loss:2.89 \n",
      "Iteration: 4423, Loss:2.51 \n",
      "Iteration: 4424, Loss:2.45 \n",
      "Iteration: 4425, Loss:2.88 \n",
      "Iteration: 4426, Loss:2.54 \n",
      "Iteration: 4427, Loss:2.63 \n",
      "Iteration: 4428, Loss:2.62 \n",
      "Iteration: 4429, Loss:2.71 \n",
      "Iteration: 4430, Loss:2.88 \n",
      "Iteration: 4431, Loss:2.58 \n",
      "Iteration: 4432, Loss:2.56 \n",
      "Iteration: 4433, Loss:2.71 \n",
      "Iteration: 4434, Loss:2.32 \n",
      "Iteration: 4435, Loss:2.55 \n",
      "Iteration: 4436, Loss:2.33 \n",
      "Iteration: 4437, Loss:2.69 \n",
      "Iteration: 4438, Loss:2.43 \n",
      "Iteration: 4439, Loss:2.79 \n",
      "Iteration: 4440, Loss:2.61 \n",
      "Iteration: 4441, Loss:2.72 \n",
      "Iteration: 4442, Loss:2.88 \n",
      "Iteration: 4443, Loss:2.37 \n",
      "Iteration: 4444, Loss:2.46 \n",
      "Iteration: 4445, Loss:2.62 \n",
      "Iteration: 4446, Loss:2.86 \n",
      "Iteration: 4447, Loss:2.21 \n",
      "Iteration: 4448, Loss:2.27 \n",
      "Iteration: 4449, Loss:2.46 \n",
      "Iteration: 4450, Loss:2.43 \n",
      "Iteration: 4451, Loss:2.64 \n",
      "Iteration: 4452, Loss:2.55 \n",
      "Iteration: 4453, Loss:2.64 \n",
      "Iteration: 4454, Loss:2.26 \n",
      "Iteration: 4455, Loss:2.55 \n",
      "Iteration: 4456, Loss:2.98 \n",
      "Iteration: 4457, Loss:2.60 \n",
      "Iteration: 4458, Loss:2.35 \n",
      "Iteration: 4459, Loss:2.30 \n",
      "Iteration: 4460, Loss:2.87 \n",
      "Iteration: 4461, Loss:2.53 \n",
      "Iteration: 4462, Loss:2.37 \n",
      "Iteration: 4463, Loss:2.44 \n",
      "Iteration: 4464, Loss:2.96 \n",
      "Iteration: 4465, Loss:2.87 \n",
      "Iteration: 4466, Loss:2.54 \n",
      "Iteration: 4467, Loss:2.50 \n",
      "Iteration: 4468, Loss:1.98 \n",
      "Iteration: 4469, Loss:2.39 \n",
      "Iteration: 4470, Loss:2.76 \n",
      "Iteration: 4471, Loss:2.40 \n",
      "Iteration: 4472, Loss:2.80 \n",
      "Iteration: 4473, Loss:2.51 \n",
      "Iteration: 4474, Loss:2.05 \n",
      "Iteration: 4475, Loss:2.92 \n",
      "Iteration: 4476, Loss:2.84 \n",
      "Iteration: 4477, Loss:2.22 \n",
      "Iteration: 4478, Loss:2.42 \n",
      "Iteration: 4479, Loss:2.64 \n",
      "Iteration: 4480, Loss:2.33 \n",
      "Iteration: 4481, Loss:1.85 \n",
      "Iteration: 4482, Loss:2.41 \n",
      "Iteration: 4483, Loss:2.25 \n",
      "Iteration: 4484, Loss:2.88 \n",
      "Iteration: 4485, Loss:2.53 \n",
      "Iteration: 4486, Loss:2.54 \n",
      "Iteration: 4487, Loss:2.21 \n",
      "Iteration: 4488, Loss:2.60 \n",
      "Iteration: 4489, Loss:2.77 \n",
      "Iteration: 4490, Loss:2.55 \n",
      "Iteration: 4491, Loss:2.61 \n",
      "Iteration: 4492, Loss:2.60 \n",
      "Iteration: 4493, Loss:2.44 \n",
      "Iteration: 4494, Loss:2.73 \n",
      "Iteration: 4495, Loss:2.80 \n",
      "Iteration: 4496, Loss:2.27 \n",
      "Iteration: 4497, Loss:2.38 \n",
      "Iteration: 4498, Loss:2.41 \n",
      "Iteration: 4499, Loss:2.58 \n",
      "Iteration: 4500, Loss:2.14 \n",
      "Iteration: 4501, Loss:2.57 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_4500.ckpt\n",
      "Iteration: 4502, Loss:2.55 \n",
      "Iteration: 4503, Loss:2.68 \n",
      "Iteration: 4504, Loss:2.63 \n",
      "Iteration: 4505, Loss:2.92 \n",
      "Iteration: 4506, Loss:2.39 \n",
      "Iteration: 4507, Loss:2.33 \n",
      "Iteration: 4508, Loss:2.15 \n",
      "Iteration: 4509, Loss:2.57 \n",
      "Iteration: 4510, Loss:2.92 \n",
      "Iteration: 4511, Loss:2.60 \n",
      "Iteration: 4512, Loss:2.18 \n",
      "Iteration: 4513, Loss:2.26 \n",
      "Iteration: 4514, Loss:2.58 \n",
      "Iteration: 4515, Loss:2.51 \n",
      "Iteration: 4516, Loss:2.63 \n",
      "Iteration: 4517, Loss:2.72 \n",
      "Iteration: 4518, Loss:2.71 \n",
      "Iteration: 4519, Loss:2.50 \n",
      "Iteration: 4520, Loss:2.84 \n",
      "Iteration: 4521, Loss:2.90 \n",
      "Iteration: 4522, Loss:2.48 \n",
      "Iteration: 4523, Loss:2.71 \n",
      "Iteration: 4524, Loss:2.76 \n",
      "Iteration: 4525, Loss:2.38 \n",
      "Iteration: 4526, Loss:2.33 \n",
      "Iteration: 4527, Loss:2.59 \n",
      "Iteration: 4528, Loss:2.45 \n",
      "Iteration: 4529, Loss:2.88 \n",
      "Iteration: 4530, Loss:2.66 \n",
      "Iteration: 4531, Loss:2.91 \n",
      "Iteration: 4532, Loss:2.47 \n",
      "Iteration: 4533, Loss:2.53 \n",
      "Iteration: 4534, Loss:2.11 \n",
      "Iteration: 4535, Loss:2.56 \n",
      "Iteration: 4536, Loss:2.74 \n",
      "Iteration: 4537, Loss:3.00 \n",
      "Iteration: 4538, Loss:2.68 \n",
      "Iteration: 4539, Loss:2.75 \n",
      "Iteration: 4540, Loss:2.69 \n",
      "Iteration: 4541, Loss:2.70 \n",
      "Iteration: 4542, Loss:2.57 \n",
      "Iteration: 4543, Loss:2.70 \n",
      "Iteration: 4544, Loss:2.22 \n",
      "Iteration: 4545, Loss:2.29 \n",
      "Iteration: 4546, Loss:2.63 \n",
      "Iteration: 4547, Loss:2.00 \n",
      "Iteration: 4548, Loss:2.56 \n",
      "Iteration: 4549, Loss:2.84 \n",
      "Iteration: 4550, Loss:2.13 \n",
      "Iteration: 4551, Loss:3.02 \n",
      "Iteration: 4552, Loss:2.75 \n",
      "Iteration: 4553, Loss:2.38 \n",
      "Iteration: 4554, Loss:2.80 \n",
      "Iteration: 4555, Loss:2.43 \n",
      "Iteration: 4556, Loss:2.70 \n",
      "Iteration: 4557, Loss:2.15 \n",
      "Iteration: 4558, Loss:2.15 \n",
      "Iteration: 4559, Loss:2.29 \n",
      "Iteration: 4560, Loss:2.65 \n",
      "Iteration: 4561, Loss:2.21 \n",
      "Iteration: 4562, Loss:2.54 \n",
      "Iteration: 4563, Loss:2.42 \n",
      "Iteration: 4564, Loss:2.70 \n",
      "Iteration: 4565, Loss:2.14 \n",
      "Iteration: 4566, Loss:2.64 \n",
      "Iteration: 4567, Loss:2.30 \n",
      "Iteration: 4568, Loss:2.62 \n",
      "Iteration: 4569, Loss:2.61 \n",
      "Iteration: 4570, Loss:2.00 \n",
      "Iteration: 4571, Loss:2.21 \n",
      "Iteration: 4572, Loss:2.41 \n",
      "Iteration: 4573, Loss:2.22 \n",
      "Iteration: 4574, Loss:2.39 \n",
      "Iteration: 4575, Loss:2.86 \n",
      "Iteration: 4576, Loss:2.35 \n",
      "Iteration: 4577, Loss:2.63 \n",
      "Iteration: 4578, Loss:2.75 \n",
      "Iteration: 4579, Loss:2.81 \n",
      "Iteration: 4580, Loss:2.69 \n",
      "Iteration: 4581, Loss:2.17 \n",
      "Iteration: 4582, Loss:2.56 \n",
      "Iteration: 4583, Loss:2.22 \n",
      "Iteration: 4584, Loss:2.85 \n",
      "Iteration: 4585, Loss:2.20 \n",
      "Iteration: 4586, Loss:2.70 \n",
      "Iteration: 4587, Loss:2.66 \n",
      "Iteration: 4588, Loss:2.68 \n",
      "Iteration: 4589, Loss:2.37 \n",
      "Iteration: 4590, Loss:2.44 \n",
      "Iteration: 4591, Loss:2.97 \n",
      "Iteration: 4592, Loss:2.43 \n",
      "Iteration: 4593, Loss:2.83 \n",
      "Iteration: 4594, Loss:2.33 \n",
      "Iteration: 4595, Loss:2.34 \n",
      "Iteration: 4596, Loss:2.38 \n",
      "Iteration: 4597, Loss:2.13 \n",
      "Iteration: 4598, Loss:2.84 \n",
      "Iteration: 4599, Loss:2.42 \n",
      "Iteration: 4600, Loss:2.87 \n",
      "Iteration: 4601, Loss:2.79 \n",
      "Iteration: 4602, Loss:3.01 \n",
      "Iteration: 4603, Loss:2.74 \n",
      "Iteration: 4604, Loss:2.48 \n",
      "Iteration: 4605, Loss:2.62 \n",
      "Iteration: 4606, Loss:2.29 \n",
      "Iteration: 4607, Loss:2.58 \n",
      "Iteration: 4608, Loss:2.61 \n",
      "Iteration: 4609, Loss:2.43 \n",
      "Iteration: 4610, Loss:2.58 \n",
      "Iteration: 4611, Loss:2.73 \n",
      "Iteration: 4612, Loss:2.27 \n",
      "Iteration: 4613, Loss:2.16 \n",
      "Iteration: 4614, Loss:2.73 \n",
      "Iteration: 4615, Loss:2.83 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4616, Loss:2.67 \n",
      "Iteration: 4617, Loss:2.26 \n",
      "Iteration: 4618, Loss:2.20 \n",
      "Iteration: 4619, Loss:2.59 \n",
      "Iteration: 4620, Loss:2.24 \n",
      "Iteration: 4621, Loss:2.58 \n",
      "Iteration: 4622, Loss:2.55 \n",
      "Iteration: 4623, Loss:2.91 \n",
      "Iteration: 4624, Loss:2.77 \n",
      "Iteration: 4625, Loss:2.57 \n",
      "Iteration: 4626, Loss:2.09 \n",
      "Iteration: 4627, Loss:2.29 \n",
      "Iteration: 4628, Loss:2.54 \n",
      "Iteration: 4629, Loss:2.63 \n",
      "Iteration: 4630, Loss:2.62 \n",
      "Iteration: 4631, Loss:2.49 \n",
      "Iteration: 4632, Loss:2.20 \n",
      "Iteration: 4633, Loss:2.36 \n",
      "Iteration: 4634, Loss:2.50 \n",
      "Iteration: 4635, Loss:2.38 \n",
      "Iteration: 4636, Loss:2.49 \n",
      "Iteration: 4637, Loss:2.52 \n",
      "Iteration: 4638, Loss:2.56 \n",
      "Iteration: 4639, Loss:2.58 \n",
      "Iteration: 4640, Loss:2.42 \n",
      "Iteration: 4641, Loss:2.92 \n",
      "Iteration: 4642, Loss:2.50 \n",
      "Iteration: 4643, Loss:2.75 \n",
      "Iteration: 4644, Loss:3.07 \n",
      "Iteration: 4645, Loss:2.66 \n",
      "Iteration: 4646, Loss:2.68 \n",
      "Iteration: 4647, Loss:2.74 \n",
      "Iteration: 4648, Loss:2.86 \n",
      "Iteration: 4649, Loss:2.56 \n",
      "Iteration: 4650, Loss:2.88 \n",
      "Iteration: 4651, Loss:2.68 \n",
      "Iteration: 4652, Loss:2.66 \n",
      "Iteration: 4653, Loss:2.57 \n",
      "Iteration: 4654, Loss:2.38 \n",
      "Iteration: 4655, Loss:2.82 \n",
      "Iteration: 4656, Loss:2.66 \n",
      "Iteration: 4657, Loss:2.72 \n",
      "Iteration: 4658, Loss:2.93 \n",
      "Iteration: 4659, Loss:2.67 \n",
      "Iteration: 4660, Loss:2.33 \n",
      "Iteration: 4661, Loss:2.68 \n",
      "Iteration: 4662, Loss:2.12 \n",
      "Iteration: 4663, Loss:2.37 \n",
      "Iteration: 4664, Loss:2.45 \n",
      "Iteration: 4665, Loss:2.28 \n",
      "Iteration: 4666, Loss:2.33 \n",
      "Iteration: 4667, Loss:1.90 \n",
      "Iteration: 4668, Loss:2.58 \n",
      "Iteration: 4669, Loss:2.78 \n",
      "Iteration: 4670, Loss:2.82 \n",
      "Iteration: 4671, Loss:2.37 \n",
      "Iteration: 4672, Loss:2.95 \n",
      "Iteration: 4673, Loss:2.70 \n",
      "Iteration: 4674, Loss:2.47 \n",
      "Iteration: 4675, Loss:2.61 \n",
      "Iteration: 4676, Loss:2.76 \n",
      "Iteration: 4677, Loss:2.85 \n",
      "Iteration: 4678, Loss:2.45 \n",
      "Iteration: 4679, Loss:2.30 \n",
      "Iteration: 4680, Loss:2.63 \n",
      "Iteration: 4681, Loss:2.63 \n",
      "Iteration: 4682, Loss:2.69 \n",
      "Iteration: 4683, Loss:2.22 \n",
      "Iteration: 4684, Loss:2.43 \n",
      "Iteration: 4685, Loss:2.42 \n",
      "Iteration: 4686, Loss:2.37 \n",
      "Iteration: 4687, Loss:2.68 \n",
      "Iteration: 4688, Loss:2.73 \n",
      "Iteration: 4689, Loss:2.88 \n",
      "Iteration: 4690, Loss:2.89 \n",
      "Iteration: 4691, Loss:2.79 \n",
      "Iteration: 4692, Loss:2.54 \n",
      "Iteration: 4693, Loss:2.34 \n",
      "Iteration: 4694, Loss:2.66 \n",
      "Iteration: 4695, Loss:2.63 \n",
      "Iteration: 4696, Loss:2.32 \n",
      "Iteration: 4697, Loss:2.83 \n",
      "Iteration: 4698, Loss:2.87 \n",
      "Iteration: 4699, Loss:2.41 \n",
      "Iteration: 4700, Loss:2.56 \n",
      "Iteration: 4701, Loss:2.33 \n",
      "Iteration: 4702, Loss:2.52 \n",
      "Iteration: 4703, Loss:2.69 \n",
      "Iteration: 4704, Loss:2.45 \n",
      "Iteration: 4705, Loss:2.44 \n",
      "Iteration: 4706, Loss:2.24 \n",
      "Iteration: 4707, Loss:2.45 \n",
      "Iteration: 4708, Loss:2.58 \n",
      "Iteration: 4709, Loss:2.13 \n",
      "Iteration: 4710, Loss:2.02 \n",
      "Iteration: 4711, Loss:2.69 \n",
      "Iteration: 4712, Loss:2.44 \n",
      "Iteration: 4713, Loss:2.29 \n",
      "Iteration: 4714, Loss:2.97 \n",
      "Iteration: 4715, Loss:2.74 \n",
      "Iteration: 4716, Loss:3.06 \n",
      "Iteration: 4717, Loss:2.28 \n",
      "Iteration: 4718, Loss:2.42 \n",
      "Iteration: 4719, Loss:2.42 \n",
      "Iteration: 4720, Loss:2.99 \n",
      "Iteration: 4721, Loss:2.50 \n",
      "Iteration: 4722, Loss:2.73 \n",
      "Iteration: 4723, Loss:2.77 \n",
      "Iteration: 4724, Loss:2.33 \n",
      "Iteration: 4725, Loss:2.79 \n",
      "Iteration: 4726, Loss:2.93 \n",
      "Iteration: 4727, Loss:2.09 \n",
      "Iteration: 4728, Loss:2.50 \n",
      "Iteration: 4729, Loss:2.18 \n",
      "Iteration: 4730, Loss:2.64 \n",
      "Iteration: 4731, Loss:2.30 \n",
      "Iteration: 4732, Loss:2.07 \n",
      "Iteration: 4733, Loss:2.69 \n",
      "Iteration: 4734, Loss:2.90 \n",
      "Iteration: 4735, Loss:2.64 \n",
      "Iteration: 4736, Loss:2.76 \n",
      "Iteration: 4737, Loss:2.80 \n",
      "Iteration: 4738, Loss:2.78 \n",
      "Iteration: 4739, Loss:2.30 \n",
      "Iteration: 4740, Loss:2.31 \n",
      "Iteration: 4741, Loss:2.61 \n",
      "Iteration: 4742, Loss:2.43 \n",
      "Iteration: 4743, Loss:2.16 \n",
      "Iteration: 4744, Loss:2.58 \n",
      "Iteration: 4745, Loss:2.75 \n",
      "Iteration: 4746, Loss:2.60 \n",
      "Iteration: 4747, Loss:2.55 \n",
      "Iteration: 4748, Loss:2.65 \n",
      "Iteration: 4749, Loss:2.58 \n",
      "Iteration: 4750, Loss:2.86 \n",
      "Iteration: 4751, Loss:2.72 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_4750.ckpt\n",
      "Iteration: 4752, Loss:2.21 \n",
      "Iteration: 4753, Loss:2.35 \n",
      "Iteration: 4754, Loss:2.86 \n",
      "Iteration: 4755, Loss:3.02 \n",
      "Iteration: 4756, Loss:2.86 \n",
      "Iteration: 4757, Loss:2.32 \n",
      "Iteration: 4758, Loss:1.99 \n",
      "Iteration: 4759, Loss:1.91 \n",
      "Iteration: 4760, Loss:2.95 \n",
      "Iteration: 4761, Loss:2.80 \n",
      "Iteration: 4762, Loss:2.34 \n",
      "Iteration: 4763, Loss:2.25 \n",
      "Iteration: 4764, Loss:2.47 \n",
      "Iteration: 4765, Loss:2.61 \n",
      "Iteration: 4766, Loss:2.48 \n",
      "Iteration: 4767, Loss:2.89 \n",
      "Iteration: 4768, Loss:2.90 \n",
      "Iteration: 4769, Loss:2.67 \n",
      "Iteration: 4770, Loss:2.43 \n",
      "Iteration: 4771, Loss:2.97 \n",
      "Iteration: 4772, Loss:2.78 \n",
      "Iteration: 4773, Loss:2.69 \n",
      "Iteration: 4774, Loss:2.56 \n",
      "Iteration: 4775, Loss:2.32 \n",
      "Iteration: 4776, Loss:2.64 \n",
      "Iteration: 4777, Loss:2.63 \n",
      "Iteration: 4778, Loss:2.82 \n",
      "Iteration: 4779, Loss:2.54 \n",
      "Iteration: 4780, Loss:2.62 \n",
      "Iteration: 4781, Loss:2.44 \n",
      "Iteration: 4782, Loss:2.27 \n",
      "Iteration: 4783, Loss:2.55 \n",
      "Iteration: 4784, Loss:2.70 \n",
      "Iteration: 4785, Loss:2.78 \n",
      "Iteration: 4786, Loss:2.30 \n",
      "Iteration: 4787, Loss:2.45 \n",
      "Iteration: 4788, Loss:2.03 \n",
      "Iteration: 4789, Loss:2.58 \n",
      "Iteration: 4790, Loss:2.57 \n",
      "Iteration: 4791, Loss:2.36 \n",
      "Iteration: 4792, Loss:2.08 \n",
      "Iteration: 4793, Loss:2.57 \n",
      "Iteration: 4794, Loss:2.54 \n",
      "Iteration: 4795, Loss:2.41 \n",
      "Iteration: 4796, Loss:2.03 \n",
      "Iteration: 4797, Loss:2.91 \n",
      "Iteration: 4798, Loss:2.05 \n",
      "Iteration: 4799, Loss:2.72 \n",
      "Iteration: 4800, Loss:2.36 \n",
      "Iteration: 4801, Loss:1.93 \n",
      "Iteration: 4802, Loss:2.36 \n",
      "Iteration: 4803, Loss:2.15 \n",
      "Iteration: 4804, Loss:2.45 \n",
      "Iteration: 4805, Loss:2.48 \n",
      "Iteration: 4806, Loss:2.79 \n",
      "Iteration: 4807, Loss:2.68 \n",
      "Iteration: 4808, Loss:2.70 \n",
      "Iteration: 4809, Loss:2.39 \n",
      "Iteration: 4810, Loss:2.56 \n",
      "Iteration: 4811, Loss:2.80 \n",
      "Iteration: 4812, Loss:2.49 \n",
      "Iteration: 4813, Loss:2.62 \n",
      "Iteration: 4814, Loss:2.44 \n",
      "Iteration: 4815, Loss:2.72 \n",
      "Iteration: 4816, Loss:2.43 \n",
      "Iteration: 4817, Loss:2.77 \n",
      "Iteration: 4818, Loss:2.63 \n",
      "Iteration: 4819, Loss:2.51 \n",
      "Iteration: 4820, Loss:2.03 \n",
      "Iteration: 4821, Loss:2.59 \n",
      "Iteration: 4822, Loss:2.40 \n",
      "Iteration: 4823, Loss:2.57 \n",
      "Iteration: 4824, Loss:2.58 \n",
      "Iteration: 4825, Loss:2.34 \n",
      "Iteration: 4826, Loss:2.82 \n",
      "Iteration: 4827, Loss:2.69 \n",
      "Iteration: 4828, Loss:2.59 \n",
      "Iteration: 4829, Loss:2.36 \n",
      "Iteration: 4830, Loss:2.55 \n",
      "Iteration: 4831, Loss:2.72 \n",
      "Iteration: 4832, Loss:2.78 \n",
      "Iteration: 4833, Loss:2.64 \n",
      "Iteration: 4834, Loss:2.37 \n",
      "Iteration: 4835, Loss:2.14 \n",
      "Iteration: 4836, Loss:2.34 \n",
      "Iteration: 4837, Loss:2.23 \n",
      "Iteration: 4838, Loss:2.77 \n",
      "Iteration: 4839, Loss:2.35 \n",
      "Iteration: 4840, Loss:2.24 \n",
      "Iteration: 4841, Loss:2.89 \n",
      "Iteration: 4842, Loss:2.73 \n",
      "Iteration: 4843, Loss:2.72 \n",
      "Iteration: 4844, Loss:2.59 \n",
      "Iteration: 4845, Loss:2.55 \n",
      "Iteration: 4846, Loss:2.50 \n",
      "Iteration: 4847, Loss:2.45 \n",
      "Iteration: 4848, Loss:2.36 \n",
      "Iteration: 4849, Loss:2.97 \n",
      "Iteration: 4850, Loss:2.23 \n",
      "Iteration: 4851, Loss:2.34 \n",
      "Iteration: 4852, Loss:2.27 \n",
      "Iteration: 4853, Loss:2.33 \n",
      "Iteration: 4854, Loss:2.11 \n",
      "Iteration: 4855, Loss:2.13 \n",
      "Iteration: 4856, Loss:2.54 \n",
      "Iteration: 4857, Loss:2.37 \n",
      "Iteration: 4858, Loss:2.52 \n",
      "Iteration: 4859, Loss:2.71 \n",
      "Iteration: 4860, Loss:2.87 \n",
      "Iteration: 4861, Loss:2.30 \n",
      "Iteration: 4862, Loss:2.37 \n",
      "Iteration: 4863, Loss:2.84 \n",
      "Iteration: 4864, Loss:2.55 \n",
      "Iteration: 4865, Loss:2.15 \n",
      "Iteration: 4866, Loss:1.68 \n",
      "Iteration: 4867, Loss:2.25 \n",
      "Iteration: 4868, Loss:2.94 \n",
      "Iteration: 4869, Loss:2.76 \n",
      "Iteration: 4870, Loss:2.69 \n",
      "Iteration: 4871, Loss:2.55 \n",
      "Iteration: 4872, Loss:2.50 \n",
      "Iteration: 4873, Loss:2.43 \n",
      "Iteration: 4874, Loss:2.25 \n",
      "Iteration: 4875, Loss:2.23 \n",
      "Iteration: 4876, Loss:2.80 \n",
      "Iteration: 4877, Loss:2.53 \n",
      "Iteration: 4878, Loss:2.37 \n",
      "Iteration: 4879, Loss:2.28 \n",
      "Iteration: 4880, Loss:2.58 \n",
      "Iteration: 4881, Loss:2.70 \n",
      "Iteration: 4882, Loss:2.77 \n",
      "Iteration: 4883, Loss:2.11 \n",
      "Iteration: 4884, Loss:2.05 \n",
      "Iteration: 4885, Loss:2.34 \n",
      "Iteration: 4886, Loss:2.70 \n",
      "Iteration: 4887, Loss:2.36 \n",
      "Iteration: 4888, Loss:2.91 \n",
      "Iteration: 4889, Loss:2.27 \n",
      "Iteration: 4890, Loss:2.53 \n",
      "Iteration: 4891, Loss:2.23 \n",
      "Iteration: 4892, Loss:2.90 \n",
      "Iteration: 4893, Loss:2.40 \n",
      "Iteration: 4894, Loss:2.43 \n",
      "Iteration: 4895, Loss:2.53 \n",
      "Iteration: 4896, Loss:2.37 \n",
      "Iteration: 4897, Loss:2.68 \n",
      "Iteration: 4898, Loss:2.23 \n",
      "Iteration: 4899, Loss:2.43 \n",
      "Iteration: 4900, Loss:2.86 \n",
      "Iteration: 4901, Loss:2.37 \n",
      "Iteration: 4902, Loss:2.75 \n",
      "Iteration: 4903, Loss:2.57 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 4904, Loss:2.88 \n",
      "Iteration: 4905, Loss:2.85 \n",
      "Iteration: 4906, Loss:2.42 \n",
      "Iteration: 4907, Loss:2.75 \n",
      "Iteration: 4908, Loss:2.89 \n",
      "Iteration: 4909, Loss:2.71 \n",
      "Iteration: 4910, Loss:2.86 \n",
      "Iteration: 4911, Loss:2.96 \n",
      "Iteration: 4912, Loss:2.36 \n",
      "Iteration: 4913, Loss:2.82 \n",
      "Iteration: 4914, Loss:2.59 \n",
      "Iteration: 4915, Loss:2.63 \n",
      "Iteration: 4916, Loss:2.38 \n",
      "Iteration: 4917, Loss:2.37 \n",
      "Iteration: 4918, Loss:3.06 \n",
      "Iteration: 4919, Loss:2.80 \n",
      "Iteration: 4920, Loss:2.82 \n",
      "Iteration: 4921, Loss:2.88 \n",
      "Iteration: 4922, Loss:2.82 \n",
      "Iteration: 4923, Loss:2.72 \n",
      "Iteration: 4924, Loss:2.62 \n",
      "Iteration: 4925, Loss:2.68 \n",
      "Iteration: 4926, Loss:2.52 \n",
      "Iteration: 4927, Loss:2.16 \n",
      "Iteration: 4928, Loss:2.84 \n",
      "Iteration: 4929, Loss:2.66 \n",
      "Iteration: 4930, Loss:2.57 \n",
      "Iteration: 4931, Loss:2.80 \n",
      "Iteration: 4932, Loss:2.73 \n",
      "Iteration: 4933, Loss:2.17 \n",
      "Iteration: 4934, Loss:3.04 \n",
      "Iteration: 4935, Loss:2.84 \n",
      "Iteration: 4936, Loss:2.56 \n",
      "Iteration: 4937, Loss:2.71 \n",
      "Iteration: 4938, Loss:2.87 \n",
      "Iteration: 4939, Loss:2.29 \n",
      "Iteration: 4940, Loss:2.68 \n",
      "Iteration: 4941, Loss:2.75 \n",
      "Iteration: 4942, Loss:2.70 \n",
      "Iteration: 4943, Loss:2.34 \n",
      "Iteration: 4944, Loss:2.77 \n",
      "Iteration: 4945, Loss:2.51 \n",
      "Iteration: 4946, Loss:2.59 \n",
      "Iteration: 4947, Loss:2.37 \n",
      "Iteration: 4948, Loss:2.94 \n",
      "Iteration: 4949, Loss:2.33 \n",
      "Iteration: 4950, Loss:2.75 \n",
      "Iteration: 4951, Loss:2.81 \n",
      "Iteration: 4952, Loss:2.55 \n",
      "Iteration: 4953, Loss:2.35 \n",
      "Iteration: 4954, Loss:1.90 \n",
      "Iteration: 4955, Loss:2.70 \n",
      "Iteration: 4956, Loss:2.60 \n",
      "Iteration: 4957, Loss:2.85 \n",
      "Iteration: 4958, Loss:2.54 \n",
      "Iteration: 4959, Loss:2.40 \n",
      "Iteration: 4960, Loss:2.40 \n",
      "Iteration: 4961, Loss:2.59 \n",
      "Iteration: 4962, Loss:2.27 \n",
      "Iteration: 4963, Loss:2.54 \n",
      "Iteration: 4964, Loss:1.89 \n",
      "Iteration: 4965, Loss:2.77 \n",
      "Iteration: 4966, Loss:2.73 \n",
      "Iteration: 4967, Loss:2.46 \n",
      "Iteration: 4968, Loss:2.61 \n",
      "Iteration: 4969, Loss:2.47 \n",
      "Iteration: 4970, Loss:2.53 \n",
      "Iteration: 4971, Loss:2.20 \n",
      "Iteration: 4972, Loss:2.13 \n",
      "Iteration: 4973, Loss:2.15 \n",
      "Iteration: 4974, Loss:2.91 \n",
      "Iteration: 4975, Loss:2.60 \n",
      "Iteration: 4976, Loss:2.76 \n",
      "Iteration: 4977, Loss:2.71 \n",
      "Iteration: 4978, Loss:2.41 \n",
      "Iteration: 4979, Loss:2.61 \n",
      "Iteration: 4980, Loss:2.59 \n",
      "Iteration: 4981, Loss:2.15 \n",
      "Iteration: 4982, Loss:2.58 \n",
      "Iteration: 4983, Loss:2.49 \n",
      "Iteration: 4984, Loss:2.56 \n",
      "Iteration: 4985, Loss:2.64 \n",
      "Iteration: 4986, Loss:2.52 \n",
      "Iteration: 4987, Loss:2.53 \n",
      "Iteration: 4988, Loss:2.56 \n",
      "Iteration: 4989, Loss:2.44 \n",
      "Iteration: 4990, Loss:2.89 \n",
      "Iteration: 4991, Loss:2.68 \n",
      "Iteration: 4992, Loss:2.70 \n",
      "Iteration: 4993, Loss:2.58 \n",
      "Iteration: 4994, Loss:2.29 \n",
      "Iteration: 4995, Loss:2.32 \n",
      "Iteration: 4996, Loss:2.95 \n",
      "Iteration: 4997, Loss:2.50 \n",
      "Iteration: 4998, Loss:2.92 \n",
      "Iteration: 4999, Loss:2.34 \n",
      "Iteration: 5000, Loss:2.86 \n",
      "Iteration: 5001, Loss:2.51 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_5000.ckpt\n",
      "Iteration: 5002, Loss:2.40 \n",
      "Iteration: 5003, Loss:2.66 \n",
      "Iteration: 5004, Loss:2.48 \n",
      "Iteration: 5005, Loss:1.88 \n",
      "Iteration: 5006, Loss:2.55 \n",
      "Iteration: 5007, Loss:2.39 \n",
      "Iteration: 5008, Loss:2.64 \n",
      "Iteration: 5009, Loss:2.66 \n",
      "Iteration: 5010, Loss:2.34 \n",
      "Iteration: 5011, Loss:2.50 \n",
      "Iteration: 5012, Loss:2.30 \n",
      "Iteration: 5013, Loss:2.62 \n",
      "Iteration: 5014, Loss:2.51 \n",
      "Iteration: 5015, Loss:2.35 \n",
      "Iteration: 5016, Loss:2.16 \n",
      "Iteration: 5017, Loss:2.04 \n",
      "Iteration: 5018, Loss:2.57 \n",
      "Iteration: 5019, Loss:2.16 \n",
      "Iteration: 5020, Loss:2.63 \n",
      "Iteration: 5021, Loss:2.24 \n",
      "Iteration: 5022, Loss:2.68 \n",
      "Iteration: 5023, Loss:2.30 \n",
      "Iteration: 5024, Loss:2.58 \n",
      "Iteration: 5025, Loss:2.43 \n",
      "Iteration: 5026, Loss:2.88 \n",
      "Iteration: 5027, Loss:2.40 \n",
      "Iteration: 5028, Loss:2.90 \n",
      "Iteration: 5029, Loss:2.90 \n",
      "Iteration: 5030, Loss:2.81 \n",
      "Iteration: 5031, Loss:2.46 \n",
      "Iteration: 5032, Loss:2.80 \n",
      "Iteration: 5033, Loss:2.49 \n",
      "Iteration: 5034, Loss:2.90 \n",
      "Iteration: 5035, Loss:2.61 \n",
      "Iteration: 5036, Loss:2.52 \n",
      "Iteration: 5037, Loss:2.52 \n",
      "Iteration: 5038, Loss:2.65 \n",
      "Iteration: 5039, Loss:2.49 \n",
      "Iteration: 5040, Loss:2.49 \n",
      "Iteration: 5041, Loss:2.13 \n",
      "Iteration: 5042, Loss:2.83 \n",
      "Iteration: 5043, Loss:2.96 \n",
      "Iteration: 5044, Loss:2.63 \n",
      "Iteration: 5045, Loss:2.72 \n",
      "Iteration: 5046, Loss:2.31 \n",
      "Iteration: 5047, Loss:2.51 \n",
      "Iteration: 5048, Loss:2.62 \n",
      "Iteration: 5049, Loss:2.36 \n",
      "Iteration: 5050, Loss:2.63 \n",
      "Iteration: 5051, Loss:2.41 \n",
      "Iteration: 5052, Loss:2.17 \n",
      "Iteration: 5053, Loss:2.73 \n",
      "Iteration: 5054, Loss:2.15 \n",
      "Iteration: 5055, Loss:2.09 \n",
      "Iteration: 5056, Loss:2.87 \n",
      "Iteration: 5057, Loss:2.67 \n",
      "Iteration: 5058, Loss:2.51 \n",
      "Iteration: 5059, Loss:3.11 \n",
      "Iteration: 5060, Loss:2.34 \n",
      "Iteration: 5061, Loss:2.55 \n",
      "Iteration: 5062, Loss:2.48 \n",
      "Iteration: 5063, Loss:2.89 \n",
      "Iteration: 5064, Loss:2.43 \n",
      "Iteration: 5065, Loss:2.89 \n",
      "Iteration: 5066, Loss:2.36 \n",
      "Iteration: 5067, Loss:2.77 \n",
      "Iteration: 5068, Loss:2.58 \n",
      "Iteration: 5069, Loss:2.80 \n",
      "Iteration: 5070, Loss:2.50 \n",
      "Iteration: 5071, Loss:2.41 \n",
      "Iteration: 5072, Loss:2.84 \n",
      "Iteration: 5073, Loss:2.75 \n",
      "Iteration: 5074, Loss:1.90 \n",
      "Iteration: 5075, Loss:2.44 \n",
      "Iteration: 5076, Loss:2.48 \n",
      "Iteration: 5077, Loss:2.53 \n",
      "Iteration: 5078, Loss:2.29 \n",
      "Iteration: 5079, Loss:2.60 \n",
      "Iteration: 5080, Loss:2.57 \n",
      "Iteration: 5081, Loss:2.49 \n",
      "Iteration: 5082, Loss:2.63 \n",
      "Iteration: 5083, Loss:2.63 \n",
      "Iteration: 5084, Loss:2.69 \n",
      "Iteration: 5085, Loss:2.74 \n",
      "Iteration: 5086, Loss:2.29 \n",
      "Iteration: 5087, Loss:2.40 \n",
      "Iteration: 5088, Loss:2.38 \n",
      "Iteration: 5089, Loss:2.50 \n",
      "Iteration: 5090, Loss:2.42 \n",
      "Iteration: 5091, Loss:2.74 \n",
      "Iteration: 5092, Loss:2.52 \n",
      "Iteration: 5093, Loss:2.71 \n",
      "Iteration: 5094, Loss:2.22 \n",
      "Iteration: 5095, Loss:1.99 \n",
      "Iteration: 5096, Loss:2.11 \n",
      "Iteration: 5097, Loss:2.36 \n",
      "Iteration: 5098, Loss:2.66 \n",
      "Iteration: 5099, Loss:2.37 \n",
      "Iteration: 5100, Loss:3.11 \n",
      "Iteration: 5101, Loss:2.40 \n",
      "Iteration: 5102, Loss:3.18 \n",
      "Iteration: 5103, Loss:2.45 \n",
      "Iteration: 5104, Loss:2.69 \n",
      "Iteration: 5105, Loss:2.48 \n",
      "Iteration: 5106, Loss:2.56 \n",
      "Iteration: 5107, Loss:2.65 \n",
      "Iteration: 5108, Loss:2.55 \n",
      "Iteration: 5109, Loss:2.26 \n",
      "Iteration: 5110, Loss:2.37 \n",
      "Iteration: 5111, Loss:2.43 \n",
      "Iteration: 5112, Loss:2.37 \n",
      "Iteration: 5113, Loss:2.74 \n",
      "Iteration: 5114, Loss:2.29 \n",
      "Iteration: 5115, Loss:2.91 \n",
      "Iteration: 5116, Loss:2.88 \n",
      "Iteration: 5117, Loss:2.18 \n",
      "Iteration: 5118, Loss:2.40 \n",
      "Iteration: 5119, Loss:2.66 \n",
      "Iteration: 5120, Loss:3.01 \n",
      "Iteration: 5121, Loss:2.63 \n",
      "Iteration: 5122, Loss:2.51 \n",
      "Iteration: 5123, Loss:2.30 \n",
      "Iteration: 5124, Loss:2.90 \n",
      "Iteration: 5125, Loss:2.29 \n",
      "Iteration: 5126, Loss:2.80 \n",
      "Iteration: 5127, Loss:2.26 \n",
      "Iteration: 5128, Loss:2.90 \n",
      "Iteration: 5129, Loss:2.60 \n",
      "Iteration: 5130, Loss:2.48 \n",
      "Iteration: 5131, Loss:2.48 \n",
      "Iteration: 5132, Loss:2.22 \n",
      "Iteration: 5133, Loss:1.95 \n",
      "Iteration: 5134, Loss:2.57 \n",
      "Iteration: 5135, Loss:2.59 \n",
      "Iteration: 5136, Loss:1.71 \n",
      "Iteration: 5137, Loss:2.51 \n",
      "Iteration: 5138, Loss:2.58 \n",
      "Iteration: 5139, Loss:2.91 \n",
      "Iteration: 5140, Loss:2.71 \n",
      "Iteration: 5141, Loss:2.72 \n",
      "Iteration: 5142, Loss:2.23 \n",
      "Iteration: 5143, Loss:2.91 \n",
      "Iteration: 5144, Loss:2.80 \n",
      "Iteration: 5145, Loss:2.63 \n",
      "Iteration: 5146, Loss:2.78 \n",
      "Iteration: 5147, Loss:2.56 \n",
      "Iteration: 5148, Loss:2.43 \n",
      "Iteration: 5149, Loss:2.80 \n",
      "Iteration: 5150, Loss:2.43 \n",
      "Iteration: 5151, Loss:2.66 \n",
      "Iteration: 5152, Loss:2.51 \n",
      "Iteration: 5153, Loss:2.34 \n",
      "Iteration: 5154, Loss:2.47 \n",
      "Iteration: 5155, Loss:2.79 \n",
      "Iteration: 5156, Loss:2.40 \n",
      "Iteration: 5157, Loss:2.58 \n",
      "Iteration: 5158, Loss:2.48 \n",
      "Iteration: 5159, Loss:2.83 \n",
      "Iteration: 5160, Loss:2.78 \n",
      "Iteration: 5161, Loss:2.81 \n",
      "Iteration: 5162, Loss:2.68 \n",
      "Iteration: 5163, Loss:2.17 \n",
      "Iteration: 5164, Loss:2.62 \n",
      "Iteration: 5165, Loss:2.00 \n",
      "Iteration: 5166, Loss:2.59 \n",
      "Iteration: 5167, Loss:2.71 \n",
      "Iteration: 5168, Loss:2.61 \n",
      "Iteration: 5169, Loss:2.62 \n",
      "Iteration: 5170, Loss:2.35 \n",
      "Iteration: 5171, Loss:2.34 \n",
      "Iteration: 5172, Loss:2.68 \n",
      "Iteration: 5173, Loss:2.48 \n",
      "Iteration: 5174, Loss:2.70 \n",
      "Iteration: 5175, Loss:2.84 \n",
      "Iteration: 5176, Loss:3.00 \n",
      "Iteration: 5177, Loss:2.84 \n",
      "Iteration: 5178, Loss:2.60 \n",
      "Iteration: 5179, Loss:2.83 \n",
      "Iteration: 5180, Loss:1.88 \n",
      "Iteration: 5181, Loss:2.58 \n",
      "Iteration: 5182, Loss:2.40 \n",
      "Iteration: 5183, Loss:2.31 \n",
      "Iteration: 5184, Loss:2.16 \n",
      "Iteration: 5185, Loss:2.78 \n",
      "Iteration: 5186, Loss:2.73 \n",
      "Iteration: 5187, Loss:2.19 \n",
      "Iteration: 5188, Loss:2.31 \n",
      "Iteration: 5189, Loss:2.71 \n",
      "Iteration: 5190, Loss:2.61 \n",
      "Iteration: 5191, Loss:2.67 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5192, Loss:2.35 \n",
      "Iteration: 5193, Loss:2.35 \n",
      "Iteration: 5194, Loss:2.44 \n",
      "Iteration: 5195, Loss:2.62 \n",
      "Iteration: 5196, Loss:2.81 \n",
      "Iteration: 5197, Loss:2.63 \n",
      "Iteration: 5198, Loss:2.48 \n",
      "Iteration: 5199, Loss:2.56 \n",
      "Iteration: 5200, Loss:2.83 \n",
      "Iteration: 5201, Loss:2.38 \n",
      "Iteration: 5202, Loss:2.50 \n",
      "Iteration: 5203, Loss:2.43 \n",
      "Iteration: 5204, Loss:2.56 \n",
      "Iteration: 5205, Loss:2.55 \n",
      "Iteration: 5206, Loss:2.34 \n",
      "Iteration: 5207, Loss:2.52 \n",
      "Iteration: 5208, Loss:2.53 \n",
      "Iteration: 5209, Loss:2.87 \n",
      "Iteration: 5210, Loss:2.70 \n",
      "Iteration: 5211, Loss:2.23 \n",
      "Iteration: 5212, Loss:2.64 \n",
      "Iteration: 5213, Loss:2.57 \n",
      "Iteration: 5214, Loss:2.71 \n",
      "Iteration: 5215, Loss:2.83 \n",
      "Iteration: 5216, Loss:2.78 \n",
      "Iteration: 5217, Loss:2.97 \n",
      "Iteration: 5218, Loss:2.48 \n",
      "Iteration: 5219, Loss:2.55 \n",
      "Iteration: 5220, Loss:2.61 \n",
      "Iteration: 5221, Loss:2.29 \n",
      "Iteration: 5222, Loss:2.42 \n",
      "Iteration: 5223, Loss:2.15 \n",
      "Iteration: 5224, Loss:2.61 \n",
      "Iteration: 5225, Loss:2.70 \n",
      "Iteration: 5226, Loss:2.24 \n",
      "Iteration: 5227, Loss:2.51 \n",
      "Iteration: 5228, Loss:2.87 \n",
      "Iteration: 5229, Loss:2.54 \n",
      "Iteration: 5230, Loss:2.73 \n",
      "Iteration: 5231, Loss:2.24 \n",
      "Iteration: 5232, Loss:2.71 \n",
      "Iteration: 5233, Loss:2.61 \n",
      "Iteration: 5234, Loss:2.86 \n",
      "Iteration: 5235, Loss:2.71 \n",
      "Iteration: 5236, Loss:2.47 \n",
      "Iteration: 5237, Loss:2.55 \n",
      "Iteration: 5238, Loss:1.88 \n",
      "Iteration: 5239, Loss:2.51 \n",
      "Iteration: 5240, Loss:2.62 \n",
      "Iteration: 5241, Loss:2.78 \n",
      "Iteration: 5242, Loss:2.21 \n",
      "Iteration: 5243, Loss:2.64 \n",
      "Iteration: 5244, Loss:2.21 \n",
      "Iteration: 5245, Loss:2.21 \n",
      "Iteration: 5246, Loss:2.68 \n",
      "Iteration: 5247, Loss:2.21 \n",
      "Iteration: 5248, Loss:2.75 \n",
      "Iteration: 5249, Loss:2.66 \n",
      "Iteration: 5250, Loss:2.61 \n",
      "Iteration: 5251, Loss:2.43 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_5250.ckpt\n",
      "Iteration: 5252, Loss:2.63 \n",
      "Iteration: 5253, Loss:2.71 \n",
      "Iteration: 5254, Loss:2.80 \n",
      "Iteration: 5255, Loss:2.59 \n",
      "Iteration: 5256, Loss:2.56 \n",
      "Iteration: 5257, Loss:2.74 \n",
      "Iteration: 5258, Loss:2.06 \n",
      "Iteration: 5259, Loss:2.26 \n",
      "Iteration: 5260, Loss:2.34 \n",
      "Iteration: 5261, Loss:2.64 \n",
      "Iteration: 5262, Loss:2.32 \n",
      "Iteration: 5263, Loss:2.85 \n",
      "Iteration: 5264, Loss:2.25 \n",
      "Iteration: 5265, Loss:2.62 \n",
      "Iteration: 5266, Loss:2.52 \n",
      "Iteration: 5267, Loss:2.52 \n",
      "Iteration: 5268, Loss:2.53 \n",
      "Iteration: 5269, Loss:2.24 \n",
      "Iteration: 5270, Loss:2.97 \n",
      "Iteration: 5271, Loss:2.65 \n",
      "Iteration: 5272, Loss:2.51 \n",
      "Iteration: 5273, Loss:2.21 \n",
      "Iteration: 5274, Loss:2.51 \n",
      "Iteration: 5275, Loss:2.91 \n",
      "Iteration: 5276, Loss:2.47 \n",
      "Iteration: 5277, Loss:2.60 \n",
      "Iteration: 5278, Loss:2.83 \n",
      "Iteration: 5279, Loss:2.32 \n",
      "Iteration: 5280, Loss:2.59 \n",
      "Iteration: 5281, Loss:2.52 \n",
      "Iteration: 5282, Loss:2.34 \n",
      "Iteration: 5283, Loss:2.32 \n",
      "Iteration: 5284, Loss:2.74 \n",
      "Iteration: 5285, Loss:2.76 \n",
      "Iteration: 5286, Loss:2.57 \n",
      "Iteration: 5287, Loss:2.62 \n",
      "Iteration: 5288, Loss:2.32 \n",
      "Iteration: 5289, Loss:2.66 \n",
      "Iteration: 5290, Loss:2.52 \n",
      "Iteration: 5291, Loss:2.86 \n",
      "Iteration: 5292, Loss:2.52 \n",
      "Iteration: 5293, Loss:2.00 \n",
      "Iteration: 5294, Loss:2.27 \n",
      "Iteration: 5295, Loss:2.35 \n",
      "Iteration: 5296, Loss:2.17 \n",
      "Iteration: 5297, Loss:2.64 \n",
      "Iteration: 5298, Loss:2.24 \n",
      "Iteration: 5299, Loss:2.17 \n",
      "Iteration: 5300, Loss:2.81 \n",
      "Iteration: 5301, Loss:2.78 \n",
      "Iteration: 5302, Loss:2.66 \n",
      "Iteration: 5303, Loss:2.45 \n",
      "Iteration: 5304, Loss:2.44 \n",
      "Iteration: 5305, Loss:2.23 \n",
      "Iteration: 5306, Loss:2.53 \n",
      "Iteration: 5307, Loss:2.54 \n",
      "Iteration: 5308, Loss:2.55 \n",
      "Iteration: 5309, Loss:2.80 \n",
      "Iteration: 5310, Loss:2.37 \n",
      "Iteration: 5311, Loss:2.77 \n",
      "Iteration: 5312, Loss:2.32 \n",
      "Iteration: 5313, Loss:2.14 \n",
      "Iteration: 5314, Loss:2.62 \n",
      "Iteration: 5315, Loss:2.66 \n",
      "Iteration: 5316, Loss:2.92 \n",
      "Iteration: 5317, Loss:2.57 \n",
      "Iteration: 5318, Loss:2.63 \n",
      "Iteration: 5319, Loss:2.36 \n",
      "Iteration: 5320, Loss:2.66 \n",
      "Iteration: 5321, Loss:2.54 \n",
      "Iteration: 5322, Loss:2.39 \n",
      "Iteration: 5323, Loss:2.97 \n",
      "Iteration: 5324, Loss:2.81 \n",
      "Iteration: 5325, Loss:2.65 \n",
      "Iteration: 5326, Loss:2.44 \n",
      "Iteration: 5327, Loss:2.73 \n",
      "Iteration: 5328, Loss:2.25 \n",
      "Iteration: 5329, Loss:2.65 \n",
      "Iteration: 5330, Loss:2.39 \n",
      "Iteration: 5331, Loss:2.59 \n",
      "Iteration: 5332, Loss:2.28 \n",
      "Iteration: 5333, Loss:2.56 \n",
      "Iteration: 5334, Loss:2.79 \n",
      "Iteration: 5335, Loss:2.37 \n",
      "Iteration: 5336, Loss:2.40 \n",
      "Iteration: 5337, Loss:2.09 \n",
      "Iteration: 5338, Loss:2.56 \n",
      "Iteration: 5339, Loss:2.44 \n",
      "Iteration: 5340, Loss:2.85 \n",
      "Iteration: 5341, Loss:2.52 \n",
      "Iteration: 5342, Loss:2.69 \n",
      "Iteration: 5343, Loss:2.77 \n",
      "Iteration: 5344, Loss:2.30 \n",
      "Iteration: 5345, Loss:2.79 \n",
      "Iteration: 5346, Loss:2.61 \n",
      "Iteration: 5347, Loss:2.58 \n",
      "Iteration: 5348, Loss:2.33 \n",
      "Iteration: 5349, Loss:2.73 \n",
      "Iteration: 5350, Loss:2.32 \n",
      "Iteration: 5351, Loss:2.58 \n",
      "Iteration: 5352, Loss:2.42 \n",
      "Iteration: 5353, Loss:2.36 \n",
      "Iteration: 5354, Loss:2.43 \n",
      "Iteration: 5355, Loss:2.63 \n",
      "Iteration: 5356, Loss:2.66 \n",
      "Iteration: 5357, Loss:2.70 \n",
      "Iteration: 5358, Loss:2.72 \n",
      "Iteration: 5359, Loss:2.56 \n",
      "Iteration: 5360, Loss:2.56 \n",
      "Iteration: 5361, Loss:2.48 \n",
      "Iteration: 5362, Loss:2.88 \n",
      "Iteration: 5363, Loss:2.71 \n",
      "Iteration: 5364, Loss:2.48 \n",
      "Iteration: 5365, Loss:2.02 \n",
      "Iteration: 5366, Loss:2.78 \n",
      "Iteration: 5367, Loss:2.37 \n",
      "Iteration: 5368, Loss:2.97 \n",
      "Iteration: 5369, Loss:2.75 \n",
      "Iteration: 5370, Loss:2.75 \n",
      "Iteration: 5371, Loss:2.68 \n",
      "Iteration: 5372, Loss:2.48 \n",
      "Iteration: 5373, Loss:1.87 \n",
      "Iteration: 5374, Loss:2.69 \n",
      "Iteration: 5375, Loss:2.27 \n",
      "Iteration: 5376, Loss:2.28 \n",
      "Iteration: 5377, Loss:2.64 \n",
      "Iteration: 5378, Loss:2.40 \n",
      "Iteration: 5379, Loss:2.87 \n",
      "Iteration: 5380, Loss:2.21 \n",
      "Iteration: 5381, Loss:3.00 \n",
      "Iteration: 5382, Loss:2.18 \n",
      "Iteration: 5383, Loss:1.93 \n",
      "Iteration: 5384, Loss:2.16 \n",
      "Iteration: 5385, Loss:2.96 \n",
      "Iteration: 5386, Loss:2.66 \n",
      "Iteration: 5387, Loss:2.53 \n",
      "Iteration: 5388, Loss:1.92 \n",
      "Iteration: 5389, Loss:2.74 \n",
      "Iteration: 5390, Loss:2.89 \n",
      "Iteration: 5391, Loss:2.60 \n",
      "Iteration: 5392, Loss:2.46 \n",
      "Iteration: 5393, Loss:2.30 \n",
      "Iteration: 5394, Loss:2.66 \n",
      "Iteration: 5395, Loss:2.66 \n",
      "Iteration: 5396, Loss:2.39 \n",
      "Iteration: 5397, Loss:2.57 \n",
      "Iteration: 5398, Loss:2.67 \n",
      "Iteration: 5399, Loss:2.82 \n",
      "Iteration: 5400, Loss:2.60 \n",
      "Iteration: 5401, Loss:2.58 \n",
      "Iteration: 5402, Loss:2.51 \n",
      "Iteration: 5403, Loss:1.98 \n",
      "Iteration: 5404, Loss:2.86 \n",
      "Iteration: 5405, Loss:2.49 \n",
      "Iteration: 5406, Loss:2.77 \n",
      "Iteration: 5407, Loss:2.47 \n",
      "Iteration: 5408, Loss:2.27 \n",
      "Iteration: 5409, Loss:2.36 \n",
      "Iteration: 5410, Loss:2.40 \n",
      "Iteration: 5411, Loss:2.21 \n",
      "Iteration: 5412, Loss:2.94 \n",
      "Iteration: 5413, Loss:2.68 \n",
      "Iteration: 5414, Loss:2.84 \n",
      "Iteration: 5415, Loss:2.53 \n",
      "Iteration: 5416, Loss:2.67 \n",
      "Iteration: 5417, Loss:2.55 \n",
      "Iteration: 5418, Loss:2.57 \n",
      "Iteration: 5419, Loss:2.52 \n",
      "Iteration: 5420, Loss:2.80 \n",
      "Iteration: 5421, Loss:2.71 \n",
      "Iteration: 5422, Loss:2.58 \n",
      "Iteration: 5423, Loss:2.32 \n",
      "Iteration: 5424, Loss:2.69 \n",
      "Iteration: 5425, Loss:2.78 \n",
      "Iteration: 5426, Loss:2.56 \n",
      "Iteration: 5427, Loss:2.64 \n",
      "Iteration: 5428, Loss:2.53 \n",
      "Iteration: 5429, Loss:2.43 \n",
      "Iteration: 5430, Loss:2.69 \n",
      "Iteration: 5431, Loss:2.66 \n",
      "Iteration: 5432, Loss:2.62 \n",
      "Iteration: 5433, Loss:2.63 \n",
      "Iteration: 5434, Loss:2.42 \n",
      "Iteration: 5435, Loss:2.42 \n",
      "Iteration: 5436, Loss:3.00 \n",
      "Iteration: 5437, Loss:2.67 \n",
      "Iteration: 5438, Loss:2.64 \n",
      "Iteration: 5439, Loss:2.22 \n",
      "Iteration: 5440, Loss:2.21 \n",
      "Iteration: 5441, Loss:2.27 \n",
      "Iteration: 5442, Loss:2.73 \n",
      "Iteration: 5443, Loss:3.03 \n",
      "Iteration: 5444, Loss:2.72 \n",
      "Iteration: 5445, Loss:2.57 \n",
      "Iteration: 5446, Loss:2.67 \n",
      "Iteration: 5447, Loss:2.22 \n",
      "Iteration: 5448, Loss:2.10 \n",
      "Iteration: 5449, Loss:2.41 \n",
      "Iteration: 5450, Loss:2.42 \n",
      "Iteration: 5451, Loss:2.68 \n",
      "Iteration: 5452, Loss:2.43 \n",
      "Iteration: 5453, Loss:2.37 \n",
      "Iteration: 5454, Loss:2.45 \n",
      "Iteration: 5455, Loss:2.61 \n",
      "Iteration: 5456, Loss:2.70 \n",
      "Iteration: 5457, Loss:2.52 \n",
      "Iteration: 5458, Loss:2.07 \n",
      "Iteration: 5459, Loss:2.30 \n",
      "Iteration: 5460, Loss:2.52 \n",
      "Iteration: 5461, Loss:2.38 \n",
      "Iteration: 5462, Loss:2.03 \n",
      "Iteration: 5463, Loss:2.72 \n",
      "Iteration: 5464, Loss:2.54 \n",
      "Iteration: 5465, Loss:2.80 \n",
      "Iteration: 5466, Loss:2.69 \n",
      "Iteration: 5467, Loss:2.46 \n",
      "Iteration: 5468, Loss:2.85 \n",
      "Iteration: 5469, Loss:2.50 \n",
      "Iteration: 5470, Loss:2.87 \n",
      "Iteration: 5471, Loss:2.82 \n",
      "Iteration: 5472, Loss:2.67 \n",
      "Iteration: 5473, Loss:2.41 \n",
      "Iteration: 5474, Loss:2.70 \n",
      "Iteration: 5475, Loss:2.41 \n",
      "Iteration: 5476, Loss:2.40 \n",
      "Iteration: 5477, Loss:2.42 \n",
      "Iteration: 5478, Loss:2.80 \n",
      "Iteration: 5479, Loss:3.04 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5480, Loss:2.78 \n",
      "Iteration: 5481, Loss:2.73 \n",
      "Iteration: 5482, Loss:2.44 \n",
      "Iteration: 5483, Loss:2.69 \n",
      "Iteration: 5484, Loss:2.03 \n",
      "Iteration: 5485, Loss:2.70 \n",
      "Iteration: 5486, Loss:2.96 \n",
      "Iteration: 5487, Loss:2.41 \n",
      "Iteration: 5488, Loss:2.34 \n",
      "Iteration: 5489, Loss:2.71 \n",
      "Iteration: 5490, Loss:2.50 \n",
      "Iteration: 5491, Loss:2.85 \n",
      "Iteration: 5492, Loss:2.41 \n",
      "Iteration: 5493, Loss:2.76 \n",
      "Iteration: 5494, Loss:2.74 \n",
      "Iteration: 5495, Loss:2.30 \n",
      "Iteration: 5496, Loss:2.46 \n",
      "Iteration: 5497, Loss:2.53 \n",
      "Iteration: 5498, Loss:2.79 \n",
      "Iteration: 5499, Loss:2.24 \n",
      "Iteration: 5500, Loss:2.51 \n",
      "Iteration: 5501, Loss:2.47 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_5500.ckpt\n",
      "Iteration: 5502, Loss:2.17 \n",
      "Iteration: 5503, Loss:2.27 \n",
      "Iteration: 5504, Loss:2.37 \n",
      "Iteration: 5505, Loss:1.98 \n",
      "Iteration: 5506, Loss:2.82 \n",
      "Iteration: 5507, Loss:2.89 \n",
      "Iteration: 5508, Loss:2.91 \n",
      "Iteration: 5509, Loss:2.83 \n",
      "Iteration: 5510, Loss:2.79 \n",
      "Iteration: 5511, Loss:2.52 \n",
      "Iteration: 5512, Loss:2.12 \n",
      "Iteration: 5513, Loss:2.58 \n",
      "Iteration: 5514, Loss:2.49 \n",
      "Iteration: 5515, Loss:2.71 \n",
      "Iteration: 5516, Loss:2.38 \n",
      "Iteration: 5517, Loss:2.09 \n",
      "Iteration: 5518, Loss:2.64 \n",
      "Iteration: 5519, Loss:2.59 \n",
      "Iteration: 5520, Loss:2.37 \n",
      "Iteration: 5521, Loss:2.68 \n",
      "Iteration: 5522, Loss:2.42 \n",
      "Iteration: 5523, Loss:2.36 \n",
      "Iteration: 5524, Loss:2.60 \n",
      "Iteration: 5525, Loss:2.63 \n",
      "Iteration: 5526, Loss:2.50 \n",
      "Iteration: 5527, Loss:2.82 \n",
      "Iteration: 5528, Loss:2.35 \n",
      "Iteration: 5529, Loss:2.77 \n",
      "Iteration: 5530, Loss:2.72 \n",
      "Iteration: 5531, Loss:2.42 \n",
      "Iteration: 5532, Loss:2.36 \n",
      "Iteration: 5533, Loss:1.98 \n",
      "Iteration: 5534, Loss:2.43 \n",
      "Iteration: 5535, Loss:2.86 \n",
      "Iteration: 5536, Loss:2.65 \n",
      "Iteration: 5537, Loss:2.41 \n",
      "Iteration: 5538, Loss:2.72 \n",
      "Iteration: 5539, Loss:2.18 \n",
      "Iteration: 5540, Loss:2.81 \n",
      "Iteration: 5541, Loss:2.95 \n",
      "Iteration: 5542, Loss:2.81 \n",
      "Iteration: 5543, Loss:2.78 \n",
      "Iteration: 5544, Loss:2.29 \n",
      "Iteration: 5545, Loss:2.25 \n",
      "Iteration: 5546, Loss:2.51 \n",
      "Iteration: 5547, Loss:2.60 \n",
      "Iteration: 5548, Loss:2.62 \n",
      "Iteration: 5549, Loss:3.03 \n",
      "Iteration: 5550, Loss:2.05 \n",
      "Iteration: 5551, Loss:2.31 \n",
      "Iteration: 5552, Loss:2.33 \n",
      "Iteration: 5553, Loss:2.43 \n",
      "Iteration: 5554, Loss:2.45 \n",
      "Iteration: 5555, Loss:2.73 \n",
      "Iteration: 5556, Loss:2.53 \n",
      "Iteration: 5557, Loss:2.36 \n",
      "Iteration: 5558, Loss:2.64 \n",
      "Iteration: 5559, Loss:2.14 \n",
      "Iteration: 5560, Loss:1.68 \n",
      "Iteration: 5561, Loss:1.99 \n",
      "Iteration: 5562, Loss:2.56 \n",
      "Iteration: 5563, Loss:2.47 \n",
      "Iteration: 5564, Loss:2.57 \n",
      "Iteration: 5565, Loss:2.39 \n",
      "Iteration: 5566, Loss:2.43 \n",
      "Iteration: 5567, Loss:2.41 \n",
      "Iteration: 5568, Loss:2.31 \n",
      "Iteration: 5569, Loss:2.64 \n",
      "Iteration: 5570, Loss:2.58 \n",
      "Iteration: 5571, Loss:2.67 \n",
      "Iteration: 5572, Loss:2.42 \n",
      "Iteration: 5573, Loss:2.38 \n",
      "Iteration: 5574, Loss:2.84 \n",
      "Iteration: 5575, Loss:2.36 \n",
      "Iteration: 5576, Loss:2.07 \n",
      "Iteration: 5577, Loss:2.77 \n",
      "Iteration: 5578, Loss:2.59 \n",
      "Iteration: 5579, Loss:2.18 \n",
      "Iteration: 5580, Loss:2.76 \n",
      "Iteration: 5581, Loss:2.50 \n",
      "Iteration: 5582, Loss:2.78 \n",
      "Iteration: 5583, Loss:2.62 \n",
      "Iteration: 5584, Loss:2.43 \n",
      "Iteration: 5585, Loss:2.40 \n",
      "Iteration: 5586, Loss:2.69 \n",
      "Iteration: 5587, Loss:2.12 \n",
      "Iteration: 5588, Loss:2.78 \n",
      "Iteration: 5589, Loss:2.18 \n",
      "Iteration: 5590, Loss:2.55 \n",
      "Iteration: 5591, Loss:2.59 \n",
      "Iteration: 5592, Loss:2.73 \n",
      "Iteration: 5593, Loss:2.60 \n",
      "Iteration: 5594, Loss:2.55 \n",
      "Iteration: 5595, Loss:2.70 \n",
      "Iteration: 5596, Loss:2.45 \n",
      "Iteration: 5597, Loss:2.74 \n",
      "Iteration: 5598, Loss:2.37 \n",
      "Iteration: 5599, Loss:2.72 \n",
      "Iteration: 5600, Loss:2.75 \n",
      "Iteration: 5601, Loss:2.73 \n",
      "Iteration: 5602, Loss:2.46 \n",
      "Iteration: 5603, Loss:2.71 \n",
      "Iteration: 5604, Loss:2.66 \n",
      "Iteration: 5605, Loss:2.66 \n",
      "Iteration: 5606, Loss:2.81 \n",
      "Iteration: 5607, Loss:2.73 \n",
      "Iteration: 5608, Loss:1.89 \n",
      "Iteration: 5609, Loss:2.94 \n",
      "Iteration: 5610, Loss:2.39 \n",
      "Iteration: 5611, Loss:2.50 \n",
      "Iteration: 5612, Loss:2.31 \n",
      "Iteration: 5613, Loss:2.38 \n",
      "Iteration: 5614, Loss:2.54 \n",
      "Iteration: 5615, Loss:2.68 \n",
      "Iteration: 5616, Loss:2.25 \n",
      "Iteration: 5617, Loss:2.44 \n",
      "Iteration: 5618, Loss:2.78 \n",
      "Iteration: 5619, Loss:2.70 \n",
      "Iteration: 5620, Loss:2.86 \n",
      "Iteration: 5621, Loss:2.35 \n",
      "Iteration: 5622, Loss:2.20 \n",
      "Iteration: 5623, Loss:2.81 \n",
      "Iteration: 5624, Loss:2.60 \n",
      "Iteration: 5625, Loss:2.80 \n",
      "Iteration: 5626, Loss:2.61 \n",
      "Iteration: 5627, Loss:2.40 \n",
      "Iteration: 5628, Loss:2.37 \n",
      "Iteration: 5629, Loss:1.83 \n",
      "Iteration: 5630, Loss:2.46 \n",
      "Iteration: 5631, Loss:2.96 \n",
      "Iteration: 5632, Loss:2.86 \n",
      "Iteration: 5633, Loss:2.44 \n",
      "Iteration: 5634, Loss:2.57 \n",
      "Iteration: 5635, Loss:2.90 \n",
      "Iteration: 5636, Loss:2.41 \n",
      "Iteration: 5637, Loss:2.86 \n",
      "Iteration: 5638, Loss:2.24 \n",
      "Iteration: 5639, Loss:2.58 \n",
      "Iteration: 5640, Loss:2.53 \n",
      "Iteration: 5641, Loss:2.98 \n",
      "Iteration: 5642, Loss:2.56 \n",
      "Iteration: 5643, Loss:2.83 \n",
      "Iteration: 5644, Loss:2.78 \n",
      "Iteration: 5645, Loss:2.47 \n",
      "Iteration: 5646, Loss:2.66 \n",
      "Iteration: 5647, Loss:2.83 \n",
      "Iteration: 5648, Loss:2.52 \n",
      "Iteration: 5649, Loss:2.39 \n",
      "Iteration: 5650, Loss:2.00 \n",
      "Iteration: 5651, Loss:2.08 \n",
      "Iteration: 5652, Loss:2.31 \n",
      "Iteration: 5653, Loss:2.29 \n",
      "Iteration: 5654, Loss:2.64 \n",
      "Iteration: 5655, Loss:2.58 \n",
      "Iteration: 5656, Loss:2.53 \n",
      "Iteration: 5657, Loss:2.15 \n",
      "Iteration: 5658, Loss:2.58 \n",
      "Iteration: 5659, Loss:2.86 \n",
      "Iteration: 5660, Loss:3.04 \n",
      "Iteration: 5661, Loss:2.33 \n",
      "Iteration: 5662, Loss:2.55 \n",
      "Iteration: 5663, Loss:2.90 \n",
      "Iteration: 5664, Loss:2.53 \n",
      "Iteration: 5665, Loss:2.84 \n",
      "Iteration: 5666, Loss:2.93 \n",
      "Iteration: 5667, Loss:2.16 \n",
      "Iteration: 5668, Loss:2.82 \n",
      "Iteration: 5669, Loss:2.03 \n",
      "Iteration: 5670, Loss:2.64 \n",
      "Iteration: 5671, Loss:2.30 \n",
      "Iteration: 5672, Loss:2.43 \n",
      "Iteration: 5673, Loss:2.79 \n",
      "Iteration: 5674, Loss:2.83 \n",
      "Iteration: 5675, Loss:2.80 \n",
      "Iteration: 5676, Loss:2.69 \n",
      "Iteration: 5677, Loss:2.54 \n",
      "Iteration: 5678, Loss:2.98 \n",
      "Iteration: 5679, Loss:2.90 \n",
      "Iteration: 5680, Loss:2.71 \n",
      "Iteration: 5681, Loss:2.31 \n",
      "Iteration: 5682, Loss:2.46 \n",
      "Iteration: 5683, Loss:2.70 \n",
      "Iteration: 5684, Loss:2.87 \n",
      "Iteration: 5685, Loss:2.23 \n",
      "Iteration: 5686, Loss:2.39 \n",
      "Iteration: 5687, Loss:2.27 \n",
      "Iteration: 5688, Loss:2.86 \n",
      "Iteration: 5689, Loss:2.96 \n",
      "Iteration: 5690, Loss:2.51 \n",
      "Iteration: 5691, Loss:2.42 \n",
      "Iteration: 5692, Loss:2.91 \n",
      "Iteration: 5693, Loss:2.72 \n",
      "Iteration: 5694, Loss:2.24 \n",
      "Iteration: 5695, Loss:2.87 \n",
      "Iteration: 5696, Loss:2.56 \n",
      "Iteration: 5697, Loss:2.79 \n",
      "Iteration: 5698, Loss:2.84 \n",
      "Iteration: 5699, Loss:2.64 \n",
      "Iteration: 5700, Loss:2.58 \n",
      "Iteration: 5701, Loss:2.27 \n",
      "Iteration: 5702, Loss:2.36 \n",
      "Iteration: 5703, Loss:2.59 \n",
      "Iteration: 5704, Loss:2.76 \n",
      "Iteration: 5705, Loss:2.81 \n",
      "Iteration: 5706, Loss:2.43 \n",
      "Iteration: 5707, Loss:2.65 \n",
      "Iteration: 5708, Loss:2.62 \n",
      "Iteration: 5709, Loss:1.90 \n",
      "Iteration: 5710, Loss:2.49 \n",
      "Iteration: 5711, Loss:2.92 \n",
      "Iteration: 5712, Loss:2.49 \n",
      "Iteration: 5713, Loss:2.49 \n",
      "Iteration: 5714, Loss:2.36 \n",
      "Iteration: 5715, Loss:2.60 \n",
      "Iteration: 5716, Loss:2.67 \n",
      "Iteration: 5717, Loss:2.60 \n",
      "Iteration: 5718, Loss:2.55 \n",
      "Iteration: 5719, Loss:2.42 \n",
      "Iteration: 5720, Loss:2.44 \n",
      "Iteration: 5721, Loss:2.44 \n",
      "Iteration: 5722, Loss:2.81 \n",
      "Iteration: 5723, Loss:2.36 \n",
      "Iteration: 5724, Loss:2.40 \n",
      "Iteration: 5725, Loss:2.59 \n",
      "Iteration: 5726, Loss:2.68 \n",
      "Iteration: 5727, Loss:2.44 \n",
      "Iteration: 5728, Loss:2.53 \n",
      "Iteration: 5729, Loss:2.77 \n",
      "Iteration: 5730, Loss:2.43 \n",
      "Iteration: 5731, Loss:2.80 \n",
      "Iteration: 5732, Loss:2.80 \n",
      "Iteration: 5733, Loss:2.78 \n",
      "Iteration: 5734, Loss:2.99 \n",
      "Iteration: 5735, Loss:2.47 \n",
      "Iteration: 5736, Loss:2.32 \n",
      "Iteration: 5737, Loss:2.73 \n",
      "Iteration: 5738, Loss:2.38 \n",
      "Iteration: 5739, Loss:2.63 \n",
      "Iteration: 5740, Loss:2.59 \n",
      "Iteration: 5741, Loss:2.64 \n",
      "Iteration: 5742, Loss:2.65 \n",
      "Iteration: 5743, Loss:2.11 \n",
      "Iteration: 5744, Loss:2.76 \n",
      "Iteration: 5745, Loss:2.33 \n",
      "Iteration: 5746, Loss:2.53 \n",
      "Iteration: 5747, Loss:2.83 \n",
      "Iteration: 5748, Loss:2.97 \n",
      "Iteration: 5749, Loss:2.96 \n",
      "Iteration: 5750, Loss:2.78 \n",
      "Iteration: 5751, Loss:2.16 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_5750.ckpt\n",
      "Iteration: 5752, Loss:2.74 \n",
      "Iteration: 5753, Loss:2.48 \n",
      "Iteration: 5754, Loss:2.75 \n",
      "Iteration: 5755, Loss:2.32 \n",
      "Iteration: 5756, Loss:2.43 \n",
      "Iteration: 5757, Loss:2.71 \n",
      "Iteration: 5758, Loss:2.27 \n",
      "Iteration: 5759, Loss:2.16 \n",
      "Iteration: 5760, Loss:2.28 \n",
      "Iteration: 5761, Loss:2.22 \n",
      "Iteration: 5762, Loss:2.68 \n",
      "Iteration: 5763, Loss:2.89 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 5764, Loss:2.47 \n",
      "Iteration: 5765, Loss:2.93 \n",
      "Iteration: 5766, Loss:2.38 \n",
      "Iteration: 5767, Loss:2.81 \n",
      "Iteration: 5768, Loss:2.14 \n",
      "Iteration: 5769, Loss:2.63 \n",
      "Iteration: 5770, Loss:2.67 \n",
      "Iteration: 5771, Loss:2.44 \n",
      "Iteration: 5772, Loss:2.29 \n",
      "Iteration: 5773, Loss:2.48 \n",
      "Iteration: 5774, Loss:2.69 \n",
      "Iteration: 5775, Loss:2.18 \n",
      "Iteration: 5776, Loss:2.50 \n",
      "Iteration: 5777, Loss:2.42 \n",
      "Iteration: 5778, Loss:2.88 \n",
      "Iteration: 5779, Loss:2.58 \n",
      "Iteration: 5780, Loss:2.92 \n",
      "Iteration: 5781, Loss:2.51 \n",
      "Iteration: 5782, Loss:2.16 \n",
      "Iteration: 5783, Loss:2.05 \n",
      "Iteration: 5784, Loss:2.40 \n",
      "Iteration: 5785, Loss:2.43 \n",
      "Iteration: 5786, Loss:2.56 \n",
      "Iteration: 5787, Loss:2.68 \n",
      "Iteration: 5788, Loss:2.43 \n",
      "Iteration: 5789, Loss:2.56 \n",
      "Iteration: 5790, Loss:2.64 \n",
      "Iteration: 5791, Loss:2.82 \n",
      "Iteration: 5792, Loss:2.30 \n",
      "Iteration: 5793, Loss:2.27 \n",
      "Iteration: 5794, Loss:2.48 \n",
      "Iteration: 5795, Loss:2.76 \n",
      "Iteration: 5796, Loss:2.43 \n",
      "Iteration: 5797, Loss:2.92 \n",
      "Iteration: 5798, Loss:2.47 \n",
      "Iteration: 5799, Loss:2.81 \n",
      "Iteration: 5800, Loss:2.69 \n",
      "Iteration: 5801, Loss:2.49 \n",
      "Iteration: 5802, Loss:2.35 \n",
      "Iteration: 5803, Loss:2.62 \n",
      "Iteration: 5804, Loss:2.50 \n",
      "Iteration: 5805, Loss:2.68 \n",
      "Iteration: 5806, Loss:2.57 \n",
      "Iteration: 5807, Loss:2.34 \n",
      "Iteration: 5808, Loss:3.01 \n",
      "Iteration: 5809, Loss:2.43 \n",
      "Iteration: 5810, Loss:2.42 \n",
      "Iteration: 5811, Loss:2.58 \n",
      "Iteration: 5812, Loss:2.26 \n",
      "Iteration: 5813, Loss:2.58 \n",
      "Iteration: 5814, Loss:2.46 \n",
      "Iteration: 5815, Loss:2.20 \n",
      "Iteration: 5816, Loss:2.34 \n",
      "Iteration: 5817, Loss:2.63 \n",
      "Iteration: 5818, Loss:2.87 \n",
      "Iteration: 5819, Loss:2.53 \n",
      "Iteration: 5820, Loss:2.20 \n",
      "Iteration: 5821, Loss:2.58 \n",
      "Iteration: 5822, Loss:2.60 \n",
      "Iteration: 5823, Loss:2.58 \n",
      "Iteration: 5824, Loss:2.74 \n",
      "Iteration: 5825, Loss:2.55 \n",
      "Iteration: 5826, Loss:2.49 \n",
      "Iteration: 5827, Loss:2.59 \n",
      "Iteration: 5828, Loss:2.32 \n",
      "Iteration: 5829, Loss:2.31 \n",
      "Iteration: 5830, Loss:2.35 \n",
      "Iteration: 5831, Loss:2.25 \n",
      "Iteration: 5832, Loss:2.30 \n",
      "Iteration: 5833, Loss:1.93 \n",
      "Iteration: 5834, Loss:2.73 \n",
      "Iteration: 5835, Loss:2.52 \n",
      "Iteration: 5836, Loss:2.78 \n",
      "Iteration: 5837, Loss:2.53 \n",
      "Iteration: 5838, Loss:2.54 \n",
      "Iteration: 5839, Loss:2.61 \n",
      "Iteration: 5840, Loss:2.58 \n",
      "Iteration: 5841, Loss:2.76 \n",
      "Iteration: 5842, Loss:2.62 \n",
      "Iteration: 5843, Loss:2.70 \n",
      "Iteration: 5844, Loss:2.71 \n",
      "Iteration: 5845, Loss:2.58 \n",
      "Iteration: 5846, Loss:2.37 \n",
      "Iteration: 5847, Loss:2.64 \n",
      "Iteration: 5848, Loss:2.88 \n",
      "Iteration: 5849, Loss:2.54 \n",
      "Iteration: 5850, Loss:2.52 \n",
      "Iteration: 5851, Loss:2.85 \n",
      "Iteration: 5852, Loss:2.85 \n",
      "Iteration: 5853, Loss:2.54 \n",
      "Iteration: 5854, Loss:2.43 \n",
      "Iteration: 5855, Loss:2.62 \n",
      "Iteration: 5856, Loss:2.29 \n",
      "Iteration: 5857, Loss:2.53 \n",
      "Iteration: 5858, Loss:2.19 \n",
      "Iteration: 5859, Loss:2.57 \n",
      "Iteration: 5860, Loss:2.70 \n",
      "Iteration: 5861, Loss:2.44 \n",
      "Iteration: 5862, Loss:2.54 \n",
      "Iteration: 5863, Loss:2.31 \n",
      "Iteration: 5864, Loss:2.46 \n",
      "Iteration: 5865, Loss:2.50 \n",
      "Iteration: 5866, Loss:2.16 \n",
      "Iteration: 5867, Loss:2.08 \n",
      "Iteration: 5868, Loss:2.56 \n",
      "Iteration: 5869, Loss:2.48 \n",
      "Iteration: 5870, Loss:2.46 \n",
      "Iteration: 5871, Loss:2.28 \n",
      "Iteration: 5872, Loss:2.39 \n",
      "Iteration: 5873, Loss:2.84 \n",
      "Iteration: 5874, Loss:2.69 \n",
      "Iteration: 5875, Loss:3.06 \n",
      "Iteration: 5876, Loss:2.94 \n",
      "Iteration: 5877, Loss:2.18 \n",
      "Iteration: 5878, Loss:2.83 \n",
      "Iteration: 5879, Loss:2.35 \n",
      "Iteration: 5880, Loss:2.52 \n",
      "Iteration: 5881, Loss:2.59 \n",
      "Iteration: 5882, Loss:2.52 \n",
      "Iteration: 5883, Loss:2.43 \n",
      "Iteration: 5884, Loss:2.29 \n",
      "Iteration: 5885, Loss:2.36 \n",
      "Iteration: 5886, Loss:2.56 \n",
      "Iteration: 5887, Loss:2.12 \n",
      "Iteration: 5888, Loss:2.74 \n",
      "Iteration: 5889, Loss:2.56 \n",
      "Iteration: 5890, Loss:2.56 \n",
      "Iteration: 5891, Loss:2.82 \n",
      "Iteration: 5892, Loss:2.63 \n",
      "Iteration: 5893, Loss:2.96 \n",
      "Iteration: 5894, Loss:2.21 \n",
      "Iteration: 5895, Loss:2.64 \n",
      "Iteration: 5896, Loss:2.54 \n",
      "Iteration: 5897, Loss:2.31 \n",
      "Iteration: 5898, Loss:2.82 \n",
      "Iteration: 5899, Loss:2.34 \n",
      "Iteration: 5900, Loss:2.66 \n",
      "Iteration: 5901, Loss:2.66 \n",
      "Iteration: 5902, Loss:2.84 \n",
      "Iteration: 5903, Loss:2.58 \n",
      "Iteration: 5904, Loss:2.71 \n",
      "Iteration: 5905, Loss:2.40 \n",
      "Iteration: 5906, Loss:2.64 \n",
      "Iteration: 5907, Loss:2.68 \n",
      "Iteration: 5908, Loss:2.69 \n",
      "Iteration: 5909, Loss:2.77 \n",
      "Iteration: 5910, Loss:2.69 \n",
      "Iteration: 5911, Loss:2.19 \n",
      "Iteration: 5912, Loss:2.55 \n",
      "Iteration: 5913, Loss:2.76 \n",
      "Iteration: 5914, Loss:2.64 \n",
      "Iteration: 5915, Loss:2.22 \n",
      "Iteration: 5916, Loss:2.14 \n",
      "Iteration: 5917, Loss:2.36 \n",
      "Iteration: 5918, Loss:2.28 \n",
      "Iteration: 5919, Loss:2.59 \n",
      "Iteration: 5920, Loss:2.97 \n",
      "Iteration: 5921, Loss:2.07 \n",
      "Iteration: 5922, Loss:2.44 \n",
      "Iteration: 5923, Loss:2.24 \n",
      "Iteration: 5924, Loss:2.63 \n",
      "Iteration: 5925, Loss:2.61 \n",
      "Iteration: 5926, Loss:2.88 \n",
      "Iteration: 5927, Loss:2.31 \n",
      "Iteration: 5928, Loss:2.47 \n",
      "Iteration: 5929, Loss:2.48 \n",
      "Iteration: 5930, Loss:2.75 \n",
      "Iteration: 5931, Loss:2.16 \n",
      "Iteration: 5932, Loss:3.05 \n",
      "Iteration: 5933, Loss:2.79 \n",
      "Iteration: 5934, Loss:2.58 \n",
      "Iteration: 5935, Loss:2.35 \n",
      "Iteration: 5936, Loss:2.98 \n",
      "Iteration: 5937, Loss:2.87 \n",
      "Iteration: 5938, Loss:2.66 \n",
      "Iteration: 5939, Loss:2.67 \n",
      "Iteration: 5940, Loss:2.13 \n",
      "Iteration: 5941, Loss:2.21 \n",
      "Iteration: 5942, Loss:2.67 \n",
      "Iteration: 5943, Loss:2.45 \n",
      "Iteration: 5944, Loss:2.16 \n",
      "Iteration: 5945, Loss:2.15 \n",
      "Iteration: 5946, Loss:2.61 \n",
      "Iteration: 5947, Loss:2.34 \n",
      "Iteration: 5948, Loss:2.31 \n",
      "Iteration: 5949, Loss:2.75 \n",
      "Iteration: 5950, Loss:2.64 \n",
      "Iteration: 5951, Loss:2.56 \n",
      "Iteration: 5952, Loss:3.00 \n",
      "Iteration: 5953, Loss:2.44 \n",
      "Iteration: 5954, Loss:2.44 \n",
      "Iteration: 5955, Loss:2.40 \n",
      "Iteration: 5956, Loss:2.38 \n",
      "Iteration: 5957, Loss:2.53 \n",
      "Iteration: 5958, Loss:2.95 \n",
      "Iteration: 5959, Loss:2.93 \n",
      "Iteration: 5960, Loss:2.31 \n",
      "Iteration: 5961, Loss:2.50 \n",
      "Iteration: 5962, Loss:2.58 \n",
      "Iteration: 5963, Loss:2.80 \n",
      "Iteration: 5964, Loss:2.36 \n",
      "Iteration: 5965, Loss:2.57 \n",
      "Iteration: 5966, Loss:2.38 \n",
      "Iteration: 5967, Loss:2.59 \n",
      "Iteration: 5968, Loss:2.57 \n",
      "Iteration: 5969, Loss:2.73 \n",
      "Iteration: 5970, Loss:2.37 \n",
      "Iteration: 5971, Loss:2.50 \n",
      "Iteration: 5972, Loss:2.37 \n",
      "Iteration: 5973, Loss:2.31 \n",
      "Iteration: 5974, Loss:2.92 \n",
      "Iteration: 5975, Loss:2.62 \n",
      "Iteration: 5976, Loss:2.71 \n",
      "Iteration: 5977, Loss:1.86 \n",
      "Iteration: 5978, Loss:2.45 \n",
      "Iteration: 5979, Loss:2.79 \n",
      "Iteration: 5980, Loss:2.61 \n",
      "Iteration: 5981, Loss:2.59 \n",
      "Iteration: 5982, Loss:2.43 \n",
      "Iteration: 5983, Loss:2.92 \n",
      "Iteration: 5984, Loss:2.29 \n",
      "Iteration: 5985, Loss:2.31 \n",
      "Iteration: 5986, Loss:2.88 \n",
      "Iteration: 5987, Loss:2.59 \n",
      "Iteration: 5988, Loss:2.61 \n",
      "Iteration: 5989, Loss:2.44 \n",
      "Iteration: 5990, Loss:2.48 \n",
      "Iteration: 5991, Loss:2.31 \n",
      "Iteration: 5992, Loss:2.33 \n",
      "Iteration: 5993, Loss:2.79 \n",
      "Iteration: 5994, Loss:2.94 \n",
      "Iteration: 5995, Loss:2.54 \n",
      "Iteration: 5996, Loss:2.71 \n",
      "Iteration: 5997, Loss:2.73 \n",
      "Iteration: 5998, Loss:2.38 \n",
      "Iteration: 5999, Loss:2.79 \n",
      "Iteration: 6000, Loss:2.47 \n",
      "Iteration: 6001, Loss:2.53 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_6000.ckpt\n",
      "Iteration: 6002, Loss:2.70 \n",
      "Iteration: 6003, Loss:2.43 \n",
      "Iteration: 6004, Loss:2.91 \n",
      "Iteration: 6005, Loss:1.88 \n",
      "Iteration: 6006, Loss:2.44 \n",
      "Iteration: 6007, Loss:2.62 \n",
      "Iteration: 6008, Loss:2.41 \n",
      "Iteration: 6009, Loss:2.69 \n",
      "Iteration: 6010, Loss:2.62 \n",
      "Iteration: 6011, Loss:2.15 \n",
      "Iteration: 6012, Loss:2.88 \n",
      "Iteration: 6013, Loss:2.10 \n",
      "Iteration: 6014, Loss:2.45 \n",
      "Iteration: 6015, Loss:2.66 \n",
      "Iteration: 6016, Loss:2.82 \n",
      "Iteration: 6017, Loss:2.44 \n",
      "Iteration: 6018, Loss:2.95 \n",
      "Iteration: 6019, Loss:2.43 \n",
      "Iteration: 6020, Loss:2.44 \n",
      "Iteration: 6021, Loss:2.12 \n",
      "Iteration: 6022, Loss:2.87 \n",
      "Iteration: 6023, Loss:2.23 \n",
      "Iteration: 6024, Loss:2.34 \n",
      "Iteration: 6025, Loss:2.81 \n",
      "Iteration: 6026, Loss:2.49 \n",
      "Iteration: 6027, Loss:2.29 \n",
      "Iteration: 6028, Loss:2.41 \n",
      "Iteration: 6029, Loss:2.76 \n",
      "Iteration: 6030, Loss:2.17 \n",
      "Iteration: 6031, Loss:2.01 \n",
      "Iteration: 6032, Loss:2.55 \n",
      "Iteration: 6033, Loss:2.19 \n",
      "Iteration: 6034, Loss:2.42 \n",
      "Iteration: 6035, Loss:3.03 \n",
      "Iteration: 6036, Loss:2.62 \n",
      "Iteration: 6037, Loss:2.68 \n",
      "Iteration: 6038, Loss:2.67 \n",
      "Iteration: 6039, Loss:2.51 \n",
      "Iteration: 6040, Loss:2.75 \n",
      "Iteration: 6041, Loss:2.08 \n",
      "Iteration: 6042, Loss:2.11 \n",
      "Iteration: 6043, Loss:2.66 \n",
      "Iteration: 6044, Loss:2.72 \n",
      "Iteration: 6045, Loss:2.81 \n",
      "Iteration: 6046, Loss:2.45 \n",
      "Iteration: 6047, Loss:2.36 \n",
      "Iteration: 6048, Loss:2.05 \n",
      "Iteration: 6049, Loss:2.65 \n",
      "Iteration: 6050, Loss:2.50 \n",
      "Iteration: 6051, Loss:2.37 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6052, Loss:2.82 \n",
      "Iteration: 6053, Loss:2.52 \n",
      "Iteration: 6054, Loss:2.38 \n",
      "Iteration: 6055, Loss:2.77 \n",
      "Iteration: 6056, Loss:2.69 \n",
      "Iteration: 6057, Loss:2.62 \n",
      "Iteration: 6058, Loss:2.74 \n",
      "Iteration: 6059, Loss:2.18 \n",
      "Iteration: 6060, Loss:2.58 \n",
      "Iteration: 6061, Loss:2.84 \n",
      "Iteration: 6062, Loss:2.87 \n",
      "Iteration: 6063, Loss:2.31 \n",
      "Iteration: 6064, Loss:2.72 \n",
      "Iteration: 6065, Loss:2.87 \n",
      "Iteration: 6066, Loss:2.40 \n",
      "Iteration: 6067, Loss:2.54 \n",
      "Iteration: 6068, Loss:2.36 \n",
      "Iteration: 6069, Loss:2.76 \n",
      "Iteration: 6070, Loss:2.86 \n",
      "Iteration: 6071, Loss:2.55 \n",
      "Iteration: 6072, Loss:3.16 \n",
      "Iteration: 6073, Loss:2.62 \n",
      "Iteration: 6074, Loss:2.42 \n",
      "Iteration: 6075, Loss:2.68 \n",
      "Iteration: 6076, Loss:2.15 \n",
      "Iteration: 6077, Loss:2.96 \n",
      "Iteration: 6078, Loss:2.83 \n",
      "Iteration: 6079, Loss:2.70 \n",
      "Iteration: 6080, Loss:2.73 \n",
      "Iteration: 6081, Loss:2.94 \n",
      "Iteration: 6082, Loss:2.48 \n",
      "Iteration: 6083, Loss:2.87 \n",
      "Iteration: 6084, Loss:2.49 \n",
      "Iteration: 6085, Loss:2.67 \n",
      "Iteration: 6086, Loss:2.63 \n",
      "Iteration: 6087, Loss:2.37 \n",
      "Iteration: 6088, Loss:2.49 \n",
      "Iteration: 6089, Loss:2.52 \n",
      "Iteration: 6090, Loss:2.57 \n",
      "Iteration: 6091, Loss:2.86 \n",
      "Iteration: 6092, Loss:2.61 \n",
      "Iteration: 6093, Loss:2.47 \n",
      "Iteration: 6094, Loss:2.58 \n",
      "Iteration: 6095, Loss:2.40 \n",
      "Iteration: 6096, Loss:2.60 \n",
      "Iteration: 6097, Loss:2.44 \n",
      "Iteration: 6098, Loss:2.31 \n",
      "Iteration: 6099, Loss:2.72 \n",
      "Iteration: 6100, Loss:2.72 \n",
      "Iteration: 6101, Loss:2.32 \n",
      "Iteration: 6102, Loss:2.24 \n",
      "Iteration: 6103, Loss:2.48 \n",
      "Iteration: 6104, Loss:2.71 \n",
      "Iteration: 6105, Loss:2.31 \n",
      "Iteration: 6106, Loss:2.60 \n",
      "Iteration: 6107, Loss:3.00 \n",
      "Iteration: 6108, Loss:2.78 \n",
      "Iteration: 6109, Loss:2.29 \n",
      "Iteration: 6110, Loss:2.66 \n",
      "Iteration: 6111, Loss:2.59 \n",
      "Iteration: 6112, Loss:2.23 \n",
      "Iteration: 6113, Loss:2.94 \n",
      "Iteration: 6114, Loss:2.62 \n",
      "Iteration: 6115, Loss:2.47 \n",
      "Iteration: 6116, Loss:2.37 \n",
      "Iteration: 6117, Loss:2.71 \n",
      "Iteration: 6118, Loss:2.58 \n",
      "Iteration: 6119, Loss:2.21 \n",
      "Iteration: 6120, Loss:2.63 \n",
      "Iteration: 6121, Loss:2.38 \n",
      "Iteration: 6122, Loss:2.26 \n",
      "Iteration: 6123, Loss:2.53 \n",
      "Iteration: 6124, Loss:2.37 \n",
      "Iteration: 6125, Loss:2.62 \n",
      "Iteration: 6126, Loss:2.90 \n",
      "Iteration: 6127, Loss:1.88 \n",
      "Iteration: 6128, Loss:2.92 \n",
      "Iteration: 6129, Loss:2.71 \n",
      "Iteration: 6130, Loss:2.59 \n",
      "Iteration: 6131, Loss:2.37 \n",
      "Iteration: 6132, Loss:2.57 \n",
      "Iteration: 6133, Loss:2.43 \n",
      "Iteration: 6134, Loss:2.51 \n",
      "Iteration: 6135, Loss:2.39 \n",
      "Iteration: 6136, Loss:2.71 \n",
      "Iteration: 6137, Loss:2.51 \n",
      "Iteration: 6138, Loss:2.55 \n",
      "Iteration: 6139, Loss:2.78 \n",
      "Iteration: 6140, Loss:2.39 \n",
      "Iteration: 6141, Loss:2.56 \n",
      "Iteration: 6142, Loss:2.75 \n",
      "Iteration: 6143, Loss:2.56 \n",
      "Iteration: 6144, Loss:2.33 \n",
      "Iteration: 6145, Loss:2.37 \n",
      "Iteration: 6146, Loss:2.65 \n",
      "Iteration: 6147, Loss:2.57 \n",
      "Iteration: 6148, Loss:2.25 \n",
      "Iteration: 6149, Loss:2.42 \n",
      "Iteration: 6150, Loss:2.86 \n",
      "Iteration: 6151, Loss:2.56 \n",
      "Iteration: 6152, Loss:2.69 \n",
      "Iteration: 6153, Loss:2.49 \n",
      "Iteration: 6154, Loss:2.35 \n",
      "Iteration: 6155, Loss:2.33 \n",
      "Iteration: 6156, Loss:2.42 \n",
      "Iteration: 6157, Loss:2.87 \n",
      "Iteration: 6158, Loss:2.87 \n",
      "Iteration: 6159, Loss:2.48 \n",
      "Iteration: 6160, Loss:2.23 \n",
      "Iteration: 6161, Loss:2.78 \n",
      "Iteration: 6162, Loss:2.45 \n",
      "Iteration: 6163, Loss:2.86 \n",
      "Iteration: 6164, Loss:2.88 \n",
      "Iteration: 6165, Loss:2.69 \n",
      "Iteration: 6166, Loss:2.12 \n",
      "Iteration: 6167, Loss:2.24 \n",
      "Iteration: 6168, Loss:2.73 \n",
      "Iteration: 6169, Loss:2.54 \n",
      "Iteration: 6170, Loss:2.41 \n",
      "Iteration: 6171, Loss:2.57 \n",
      "Iteration: 6172, Loss:2.64 \n",
      "Iteration: 6173, Loss:2.44 \n",
      "Iteration: 6174, Loss:2.27 \n",
      "Iteration: 6175, Loss:2.55 \n",
      "Iteration: 6176, Loss:2.23 \n",
      "Iteration: 6177, Loss:2.78 \n",
      "Iteration: 6178, Loss:2.56 \n",
      "Iteration: 6179, Loss:2.34 \n",
      "Iteration: 6180, Loss:2.53 \n",
      "Iteration: 6181, Loss:2.40 \n",
      "Iteration: 6182, Loss:2.55 \n",
      "Iteration: 6183, Loss:2.69 \n",
      "Iteration: 6184, Loss:2.79 \n",
      "Iteration: 6185, Loss:2.38 \n",
      "Iteration: 6186, Loss:2.66 \n",
      "Iteration: 6187, Loss:2.33 \n",
      "Iteration: 6188, Loss:2.46 \n",
      "Iteration: 6189, Loss:2.44 \n",
      "Iteration: 6190, Loss:2.51 \n",
      "Iteration: 6191, Loss:2.30 \n",
      "Iteration: 6192, Loss:2.65 \n",
      "Iteration: 6193, Loss:2.62 \n",
      "Iteration: 6194, Loss:2.73 \n",
      "Iteration: 6195, Loss:2.62 \n",
      "Iteration: 6196, Loss:2.56 \n",
      "Iteration: 6197, Loss:2.57 \n",
      "Iteration: 6198, Loss:2.56 \n",
      "Iteration: 6199, Loss:2.73 \n",
      "Iteration: 6200, Loss:2.02 \n",
      "Iteration: 6201, Loss:2.65 \n",
      "Iteration: 6202, Loss:1.91 \n",
      "Iteration: 6203, Loss:2.15 \n",
      "Iteration: 6204, Loss:2.71 \n",
      "Iteration: 6205, Loss:2.56 \n",
      "Iteration: 6206, Loss:2.46 \n",
      "Iteration: 6207, Loss:2.18 \n",
      "Iteration: 6208, Loss:2.92 \n",
      "Iteration: 6209, Loss:2.57 \n",
      "Iteration: 6210, Loss:2.83 \n",
      "Iteration: 6211, Loss:2.45 \n",
      "Iteration: 6212, Loss:2.94 \n",
      "Iteration: 6213, Loss:2.42 \n",
      "Iteration: 6214, Loss:2.86 \n",
      "Iteration: 6215, Loss:2.37 \n",
      "Iteration: 6216, Loss:2.43 \n",
      "Iteration: 6217, Loss:2.88 \n",
      "Iteration: 6218, Loss:2.36 \n",
      "Iteration: 6219, Loss:2.65 \n",
      "Iteration: 6220, Loss:2.29 \n",
      "Iteration: 6221, Loss:2.40 \n",
      "Iteration: 6222, Loss:2.50 \n",
      "Iteration: 6223, Loss:2.39 \n",
      "Iteration: 6224, Loss:2.21 \n",
      "Iteration: 6225, Loss:2.82 \n",
      "Iteration: 6226, Loss:2.63 \n",
      "Iteration: 6227, Loss:2.75 \n",
      "Iteration: 6228, Loss:2.68 \n",
      "Iteration: 6229, Loss:1.92 \n",
      "Iteration: 6230, Loss:2.55 \n",
      "Iteration: 6231, Loss:2.50 \n",
      "Iteration: 6232, Loss:2.66 \n",
      "Iteration: 6233, Loss:2.69 \n",
      "Iteration: 6234, Loss:2.46 \n",
      "Iteration: 6235, Loss:2.64 \n",
      "Iteration: 6236, Loss:2.48 \n",
      "Iteration: 6237, Loss:2.29 \n",
      "Iteration: 6238, Loss:2.93 \n",
      "Iteration: 6239, Loss:2.56 \n",
      "Iteration: 6240, Loss:2.75 \n",
      "Iteration: 6241, Loss:2.64 \n",
      "Iteration: 6242, Loss:2.45 \n",
      "Iteration: 6243, Loss:2.37 \n",
      "Iteration: 6244, Loss:2.66 \n",
      "Iteration: 6245, Loss:2.55 \n",
      "Iteration: 6246, Loss:2.68 \n",
      "Iteration: 6247, Loss:2.18 \n",
      "Iteration: 6248, Loss:2.96 \n",
      "Iteration: 6249, Loss:2.56 \n",
      "Iteration: 6250, Loss:2.49 \n",
      "Iteration: 6251, Loss:2.37 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_6250.ckpt\n",
      "Iteration: 6252, Loss:2.09 \n",
      "Iteration: 6253, Loss:2.53 \n",
      "Iteration: 6254, Loss:2.67 \n",
      "Iteration: 6255, Loss:2.59 \n",
      "Iteration: 6256, Loss:2.47 \n",
      "Iteration: 6257, Loss:2.76 \n",
      "Iteration: 6258, Loss:2.37 \n",
      "Iteration: 6259, Loss:2.53 \n",
      "Iteration: 6260, Loss:2.51 \n",
      "Iteration: 6261, Loss:2.35 \n",
      "Iteration: 6262, Loss:2.44 \n",
      "Iteration: 6263, Loss:2.66 \n",
      "Iteration: 6264, Loss:2.52 \n",
      "Iteration: 6265, Loss:2.09 \n",
      "Iteration: 6266, Loss:2.62 \n",
      "Iteration: 6267, Loss:2.40 \n",
      "Iteration: 6268, Loss:2.84 \n",
      "Iteration: 6269, Loss:2.37 \n",
      "Iteration: 6270, Loss:2.49 \n",
      "Iteration: 6271, Loss:2.49 \n",
      "Iteration: 6272, Loss:2.32 \n",
      "Iteration: 6273, Loss:2.47 \n",
      "Iteration: 6274, Loss:2.35 \n",
      "Iteration: 6275, Loss:2.44 \n",
      "Iteration: 6276, Loss:2.49 \n",
      "Iteration: 6277, Loss:2.22 \n",
      "Iteration: 6278, Loss:2.41 \n",
      "Iteration: 6279, Loss:2.60 \n",
      "Iteration: 6280, Loss:2.52 \n",
      "Iteration: 6281, Loss:2.15 \n",
      "Iteration: 6282, Loss:2.65 \n",
      "Iteration: 6283, Loss:2.52 \n",
      "Iteration: 6284, Loss:2.71 \n",
      "Iteration: 6285, Loss:1.91 \n",
      "Iteration: 6286, Loss:2.59 \n",
      "Iteration: 6287, Loss:2.31 \n",
      "Iteration: 6288, Loss:2.79 \n",
      "Iteration: 6289, Loss:2.83 \n",
      "Iteration: 6290, Loss:2.42 \n",
      "Iteration: 6291, Loss:2.91 \n",
      "Iteration: 6292, Loss:2.74 \n",
      "Iteration: 6293, Loss:2.69 \n",
      "Iteration: 6294, Loss:2.22 \n",
      "Iteration: 6295, Loss:1.96 \n",
      "Iteration: 6296, Loss:2.47 \n",
      "Iteration: 6297, Loss:2.65 \n",
      "Iteration: 6298, Loss:2.31 \n",
      "Iteration: 6299, Loss:2.29 \n",
      "Iteration: 6300, Loss:2.49 \n",
      "Iteration: 6301, Loss:2.73 \n",
      "Iteration: 6302, Loss:2.21 \n",
      "Iteration: 6303, Loss:2.69 \n",
      "Iteration: 6304, Loss:2.58 \n",
      "Iteration: 6305, Loss:2.40 \n",
      "Iteration: 6306, Loss:2.75 \n",
      "Iteration: 6307, Loss:2.67 \n",
      "Iteration: 6308, Loss:2.81 \n",
      "Iteration: 6309, Loss:1.96 \n",
      "Iteration: 6310, Loss:3.13 \n",
      "Iteration: 6311, Loss:3.03 \n",
      "Iteration: 6312, Loss:2.69 \n",
      "Iteration: 6313, Loss:2.39 \n",
      "Iteration: 6314, Loss:2.73 \n",
      "Iteration: 6315, Loss:2.52 \n",
      "Iteration: 6316, Loss:2.74 \n",
      "Iteration: 6317, Loss:2.57 \n",
      "Iteration: 6318, Loss:2.72 \n",
      "Iteration: 6319, Loss:2.71 \n",
      "Iteration: 6320, Loss:2.50 \n",
      "Iteration: 6321, Loss:2.05 \n",
      "Iteration: 6322, Loss:2.54 \n",
      "Iteration: 6323, Loss:2.98 \n",
      "Iteration: 6324, Loss:2.57 \n",
      "Iteration: 6325, Loss:2.91 \n",
      "Iteration: 6326, Loss:2.55 \n",
      "Iteration: 6327, Loss:2.52 \n",
      "Iteration: 6328, Loss:2.89 \n",
      "Iteration: 6329, Loss:2.72 \n",
      "Iteration: 6330, Loss:2.51 \n",
      "Iteration: 6331, Loss:2.85 \n",
      "Iteration: 6332, Loss:2.70 \n",
      "Iteration: 6333, Loss:2.35 \n",
      "Iteration: 6334, Loss:2.55 \n",
      "Iteration: 6335, Loss:2.74 \n",
      "Iteration: 6336, Loss:2.72 \n",
      "Iteration: 6337, Loss:2.20 \n",
      "Iteration: 6338, Loss:2.98 \n",
      "Iteration: 6339, Loss:2.92 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6340, Loss:2.36 \n",
      "Iteration: 6341, Loss:2.71 \n",
      "Iteration: 6342, Loss:2.07 \n",
      "Iteration: 6343, Loss:2.26 \n",
      "Iteration: 6344, Loss:2.52 \n",
      "Iteration: 6345, Loss:2.60 \n",
      "Iteration: 6346, Loss:2.22 \n",
      "Iteration: 6347, Loss:2.68 \n",
      "Iteration: 6348, Loss:2.86 \n",
      "Iteration: 6349, Loss:2.70 \n",
      "Iteration: 6350, Loss:2.50 \n",
      "Iteration: 6351, Loss:2.59 \n",
      "Iteration: 6352, Loss:2.56 \n",
      "Iteration: 6353, Loss:2.34 \n",
      "Iteration: 6354, Loss:2.40 \n",
      "Iteration: 6355, Loss:2.24 \n",
      "Iteration: 6356, Loss:2.27 \n",
      "Iteration: 6357, Loss:2.47 \n",
      "Iteration: 6358, Loss:3.00 \n",
      "Iteration: 6359, Loss:2.68 \n",
      "Iteration: 6360, Loss:2.72 \n",
      "Iteration: 6361, Loss:2.83 \n",
      "Iteration: 6362, Loss:2.33 \n",
      "Iteration: 6363, Loss:2.72 \n",
      "Iteration: 6364, Loss:2.92 \n",
      "Iteration: 6365, Loss:2.87 \n",
      "Iteration: 6366, Loss:2.27 \n",
      "Iteration: 6367, Loss:2.72 \n",
      "Iteration: 6368, Loss:2.52 \n",
      "Iteration: 6369, Loss:2.96 \n",
      "Iteration: 6370, Loss:2.06 \n",
      "Iteration: 6371, Loss:2.29 \n",
      "Iteration: 6372, Loss:3.01 \n",
      "Iteration: 6373, Loss:2.75 \n",
      "Iteration: 6374, Loss:2.07 \n",
      "Iteration: 6375, Loss:2.56 \n",
      "Iteration: 6376, Loss:3.00 \n",
      "Iteration: 6377, Loss:2.57 \n",
      "Iteration: 6378, Loss:2.52 \n",
      "Iteration: 6379, Loss:2.39 \n",
      "Iteration: 6380, Loss:2.64 \n",
      "Iteration: 6381, Loss:2.71 \n",
      "Iteration: 6382, Loss:2.49 \n",
      "Iteration: 6383, Loss:2.10 \n",
      "Iteration: 6384, Loss:2.47 \n",
      "Iteration: 6385, Loss:2.50 \n",
      "Iteration: 6386, Loss:2.66 \n",
      "Iteration: 6387, Loss:2.82 \n",
      "Iteration: 6388, Loss:2.43 \n",
      "Iteration: 6389, Loss:2.79 \n",
      "Iteration: 6390, Loss:2.52 \n",
      "Iteration: 6391, Loss:2.37 \n",
      "Iteration: 6392, Loss:2.57 \n",
      "Iteration: 6393, Loss:2.74 \n",
      "Iteration: 6394, Loss:2.09 \n",
      "Iteration: 6395, Loss:2.66 \n",
      "Iteration: 6396, Loss:2.60 \n",
      "Iteration: 6397, Loss:2.77 \n",
      "Iteration: 6398, Loss:2.34 \n",
      "Iteration: 6399, Loss:2.33 \n",
      "Iteration: 6400, Loss:2.44 \n",
      "Iteration: 6401, Loss:2.78 \n",
      "Iteration: 6402, Loss:2.67 \n",
      "Iteration: 6403, Loss:2.86 \n",
      "Iteration: 6404, Loss:2.58 \n",
      "Iteration: 6405, Loss:2.58 \n",
      "Iteration: 6406, Loss:2.63 \n",
      "Iteration: 6407, Loss:2.63 \n",
      "Iteration: 6408, Loss:2.34 \n",
      "Iteration: 6409, Loss:2.51 \n",
      "Iteration: 6410, Loss:2.58 \n",
      "Iteration: 6411, Loss:2.79 \n",
      "Iteration: 6412, Loss:2.47 \n",
      "Iteration: 6413, Loss:2.78 \n",
      "Iteration: 6414, Loss:2.57 \n",
      "Iteration: 6415, Loss:2.24 \n",
      "Iteration: 6416, Loss:2.49 \n",
      "Iteration: 6417, Loss:2.59 \n",
      "Iteration: 6418, Loss:2.78 \n",
      "Iteration: 6419, Loss:2.40 \n",
      "Iteration: 6420, Loss:2.48 \n",
      "Iteration: 6421, Loss:2.43 \n",
      "Iteration: 6422, Loss:2.22 \n",
      "Iteration: 6423, Loss:2.55 \n",
      "Iteration: 6424, Loss:2.51 \n",
      "Iteration: 6425, Loss:2.26 \n",
      "Iteration: 6426, Loss:2.77 \n",
      "Iteration: 6427, Loss:2.84 \n",
      "Iteration: 6428, Loss:2.30 \n",
      "Iteration: 6429, Loss:2.58 \n",
      "Iteration: 6430, Loss:2.73 \n",
      "Iteration: 6431, Loss:2.44 \n",
      "Iteration: 6432, Loss:2.65 \n",
      "Iteration: 6433, Loss:2.84 \n",
      "Iteration: 6434, Loss:2.44 \n",
      "Iteration: 6435, Loss:2.81 \n",
      "Iteration: 6436, Loss:2.24 \n",
      "Iteration: 6437, Loss:2.40 \n",
      "Iteration: 6438, Loss:2.67 \n",
      "Iteration: 6439, Loss:2.57 \n",
      "Iteration: 6440, Loss:2.83 \n",
      "Iteration: 6441, Loss:2.63 \n",
      "Iteration: 6442, Loss:2.22 \n",
      "Iteration: 6443, Loss:2.43 \n",
      "Iteration: 6444, Loss:2.13 \n",
      "Iteration: 6445, Loss:2.65 \n",
      "Iteration: 6446, Loss:2.70 \n",
      "Iteration: 6447, Loss:2.79 \n",
      "Iteration: 6448, Loss:2.62 \n",
      "Iteration: 6449, Loss:2.53 \n",
      "Iteration: 6450, Loss:2.61 \n",
      "Iteration: 6451, Loss:2.71 \n",
      "Iteration: 6452, Loss:2.69 \n",
      "Iteration: 6453, Loss:2.85 \n",
      "Iteration: 6454, Loss:2.31 \n",
      "Iteration: 6455, Loss:2.69 \n",
      "Iteration: 6456, Loss:2.83 \n",
      "Iteration: 6457, Loss:2.38 \n",
      "Iteration: 6458, Loss:2.48 \n",
      "Iteration: 6459, Loss:2.92 \n",
      "Iteration: 6460, Loss:2.09 \n",
      "Iteration: 6461, Loss:2.47 \n",
      "Iteration: 6462, Loss:2.28 \n",
      "Iteration: 6463, Loss:2.44 \n",
      "Iteration: 6464, Loss:2.45 \n",
      "Iteration: 6465, Loss:2.75 \n",
      "Iteration: 6466, Loss:2.84 \n",
      "Iteration: 6467, Loss:2.56 \n",
      "Iteration: 6468, Loss:2.69 \n",
      "Iteration: 6469, Loss:2.41 \n",
      "Iteration: 6470, Loss:2.88 \n",
      "Iteration: 6471, Loss:2.37 \n",
      "Iteration: 6472, Loss:2.52 \n",
      "Iteration: 6473, Loss:2.42 \n",
      "Iteration: 6474, Loss:2.45 \n",
      "Iteration: 6475, Loss:2.12 \n",
      "Iteration: 6476, Loss:2.38 \n",
      "Iteration: 6477, Loss:2.55 \n",
      "Iteration: 6478, Loss:2.86 \n",
      "Iteration: 6479, Loss:3.06 \n",
      "Iteration: 6480, Loss:2.54 \n",
      "Iteration: 6481, Loss:2.49 \n",
      "Iteration: 6482, Loss:2.63 \n",
      "Iteration: 6483, Loss:2.38 \n",
      "Iteration: 6484, Loss:2.64 \n",
      "Iteration: 6485, Loss:2.61 \n",
      "Iteration: 6486, Loss:2.52 \n",
      "Iteration: 6487, Loss:2.64 \n",
      "Iteration: 6488, Loss:2.69 \n",
      "Iteration: 6489, Loss:2.71 \n",
      "Iteration: 6490, Loss:2.65 \n",
      "Iteration: 6491, Loss:2.71 \n",
      "Iteration: 6492, Loss:2.52 \n",
      "Iteration: 6493, Loss:2.61 \n",
      "Iteration: 6494, Loss:2.55 \n",
      "Iteration: 6495, Loss:2.51 \n",
      "Iteration: 6496, Loss:2.75 \n",
      "Iteration: 6497, Loss:2.55 \n",
      "Iteration: 6498, Loss:2.59 \n",
      "Iteration: 6499, Loss:2.59 \n",
      "Iteration: 6500, Loss:2.63 \n",
      "Iteration: 6501, Loss:2.26 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_6500.ckpt\n",
      "Iteration: 6502, Loss:2.47 \n",
      "Iteration: 6503, Loss:2.64 \n",
      "Iteration: 6504, Loss:2.59 \n",
      "Iteration: 6505, Loss:2.09 \n",
      "Iteration: 6506, Loss:2.89 \n",
      "Iteration: 6507, Loss:2.51 \n",
      "Iteration: 6508, Loss:2.45 \n",
      "Iteration: 6509, Loss:2.36 \n",
      "Iteration: 6510, Loss:2.65 \n",
      "Iteration: 6511, Loss:2.19 \n",
      "Iteration: 6512, Loss:2.47 \n",
      "Iteration: 6513, Loss:2.85 \n",
      "Iteration: 6514, Loss:2.51 \n",
      "Iteration: 6515, Loss:2.48 \n",
      "Iteration: 6516, Loss:2.46 \n",
      "Iteration: 6517, Loss:2.20 \n",
      "Iteration: 6518, Loss:2.83 \n",
      "Iteration: 6519, Loss:2.34 \n",
      "Iteration: 6520, Loss:3.07 \n",
      "Iteration: 6521, Loss:2.43 \n",
      "Iteration: 6522, Loss:2.60 \n",
      "Iteration: 6523, Loss:2.52 \n",
      "Iteration: 6524, Loss:2.76 \n",
      "Iteration: 6525, Loss:2.61 \n",
      "Iteration: 6526, Loss:2.61 \n",
      "Iteration: 6527, Loss:2.61 \n",
      "Iteration: 6528, Loss:2.76 \n",
      "Iteration: 6529, Loss:2.54 \n",
      "Iteration: 6530, Loss:2.35 \n",
      "Iteration: 6531, Loss:2.55 \n",
      "Iteration: 6532, Loss:2.72 \n",
      "Iteration: 6533, Loss:3.12 \n",
      "Iteration: 6534, Loss:2.44 \n",
      "Iteration: 6535, Loss:2.78 \n",
      "Iteration: 6536, Loss:2.14 \n",
      "Iteration: 6537, Loss:2.60 \n",
      "Iteration: 6538, Loss:2.67 \n",
      "Iteration: 6539, Loss:2.52 \n",
      "Iteration: 6540, Loss:2.71 \n",
      "Iteration: 6541, Loss:2.29 \n",
      "Iteration: 6542, Loss:2.49 \n",
      "Iteration: 6543, Loss:2.81 \n",
      "Iteration: 6544, Loss:2.34 \n",
      "Iteration: 6545, Loss:2.39 \n",
      "Iteration: 6546, Loss:2.36 \n",
      "Iteration: 6547, Loss:2.34 \n",
      "Iteration: 6548, Loss:2.22 \n",
      "Iteration: 6549, Loss:2.47 \n",
      "Iteration: 6550, Loss:2.57 \n",
      "Iteration: 6551, Loss:2.61 \n",
      "Iteration: 6552, Loss:2.40 \n",
      "Iteration: 6553, Loss:2.54 \n",
      "Iteration: 6554, Loss:2.40 \n",
      "Iteration: 6555, Loss:2.52 \n",
      "Iteration: 6556, Loss:2.17 \n",
      "Iteration: 6557, Loss:2.44 \n",
      "Iteration: 6558, Loss:2.41 \n",
      "Iteration: 6559, Loss:2.82 \n",
      "Iteration: 6560, Loss:2.97 \n",
      "Iteration: 6561, Loss:2.00 \n",
      "Iteration: 6562, Loss:2.65 \n",
      "Iteration: 6563, Loss:2.45 \n",
      "Iteration: 6564, Loss:2.92 \n",
      "Iteration: 6565, Loss:2.35 \n",
      "Iteration: 6566, Loss:2.74 \n",
      "Iteration: 6567, Loss:2.88 \n",
      "Iteration: 6568, Loss:2.59 \n",
      "Iteration: 6569, Loss:2.52 \n",
      "Iteration: 6570, Loss:2.47 \n",
      "Iteration: 6571, Loss:2.65 \n",
      "Iteration: 6572, Loss:2.81 \n",
      "Iteration: 6573, Loss:2.16 \n",
      "Iteration: 6574, Loss:1.73 \n",
      "Iteration: 6575, Loss:2.55 \n",
      "Iteration: 6576, Loss:2.38 \n",
      "Iteration: 6577, Loss:2.33 \n",
      "Iteration: 6578, Loss:2.17 \n",
      "Iteration: 6579, Loss:2.21 \n",
      "Iteration: 6580, Loss:2.76 \n",
      "Iteration: 6581, Loss:2.37 \n",
      "Iteration: 6582, Loss:2.60 \n",
      "Iteration: 6583, Loss:2.72 \n",
      "Iteration: 6584, Loss:2.44 \n",
      "Iteration: 6585, Loss:2.50 \n",
      "Iteration: 6586, Loss:2.58 \n",
      "Iteration: 6587, Loss:2.81 \n",
      "Iteration: 6588, Loss:2.43 \n",
      "Iteration: 6589, Loss:2.61 \n",
      "Iteration: 6590, Loss:2.43 \n",
      "Iteration: 6591, Loss:2.58 \n",
      "Iteration: 6592, Loss:2.22 \n",
      "Iteration: 6593, Loss:2.32 \n",
      "Iteration: 6594, Loss:2.70 \n",
      "Iteration: 6595, Loss:2.36 \n",
      "Iteration: 6596, Loss:2.55 \n",
      "Iteration: 6597, Loss:2.68 \n",
      "Iteration: 6598, Loss:2.53 \n",
      "Iteration: 6599, Loss:2.72 \n",
      "Iteration: 6600, Loss:2.35 \n",
      "Iteration: 6601, Loss:2.28 \n",
      "Iteration: 6602, Loss:2.99 \n",
      "Iteration: 6603, Loss:2.74 \n",
      "Iteration: 6604, Loss:2.41 \n",
      "Iteration: 6605, Loss:2.42 \n",
      "Iteration: 6606, Loss:2.81 \n",
      "Iteration: 6607, Loss:2.46 \n",
      "Iteration: 6608, Loss:2.63 \n",
      "Iteration: 6609, Loss:2.35 \n",
      "Iteration: 6610, Loss:2.82 \n",
      "Iteration: 6611, Loss:2.55 \n",
      "Iteration: 6612, Loss:2.38 \n",
      "Iteration: 6613, Loss:2.78 \n",
      "Iteration: 6614, Loss:2.84 \n",
      "Iteration: 6615, Loss:2.47 \n",
      "Iteration: 6616, Loss:2.07 \n",
      "Iteration: 6617, Loss:2.63 \n",
      "Iteration: 6618, Loss:2.58 \n",
      "Iteration: 6619, Loss:2.38 \n",
      "Iteration: 6620, Loss:2.89 \n",
      "Iteration: 6621, Loss:2.62 \n",
      "Iteration: 6622, Loss:2.59 \n",
      "Iteration: 6623, Loss:2.83 \n",
      "Iteration: 6624, Loss:2.36 \n",
      "Iteration: 6625, Loss:2.87 \n",
      "Iteration: 6626, Loss:2.46 \n",
      "Iteration: 6627, Loss:2.26 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6628, Loss:2.24 \n",
      "Iteration: 6629, Loss:2.57 \n",
      "Iteration: 6630, Loss:2.52 \n",
      "Iteration: 6631, Loss:2.86 \n",
      "Iteration: 6632, Loss:2.70 \n",
      "Iteration: 6633, Loss:2.21 \n",
      "Iteration: 6634, Loss:2.67 \n",
      "Iteration: 6635, Loss:2.94 \n",
      "Iteration: 6636, Loss:2.59 \n",
      "Iteration: 6637, Loss:2.20 \n",
      "Iteration: 6638, Loss:2.61 \n",
      "Iteration: 6639, Loss:2.48 \n",
      "Iteration: 6640, Loss:2.71 \n",
      "Iteration: 6641, Loss:2.21 \n",
      "Iteration: 6642, Loss:2.59 \n",
      "Iteration: 6643, Loss:2.53 \n",
      "Iteration: 6644, Loss:2.25 \n",
      "Iteration: 6645, Loss:3.08 \n",
      "Iteration: 6646, Loss:2.73 \n",
      "Iteration: 6647, Loss:2.83 \n",
      "Iteration: 6648, Loss:2.58 \n",
      "Iteration: 6649, Loss:2.44 \n",
      "Iteration: 6650, Loss:2.09 \n",
      "Iteration: 6651, Loss:2.53 \n",
      "Iteration: 6652, Loss:2.73 \n",
      "Iteration: 6653, Loss:2.76 \n",
      "Iteration: 6654, Loss:2.21 \n",
      "Iteration: 6655, Loss:2.46 \n",
      "Iteration: 6656, Loss:2.63 \n",
      "Iteration: 6657, Loss:2.64 \n",
      "Iteration: 6658, Loss:2.10 \n",
      "Iteration: 6659, Loss:2.52 \n",
      "Iteration: 6660, Loss:2.40 \n",
      "Iteration: 6661, Loss:2.32 \n",
      "Iteration: 6662, Loss:2.51 \n",
      "Iteration: 6663, Loss:2.34 \n",
      "Iteration: 6664, Loss:2.23 \n",
      "Iteration: 6665, Loss:2.41 \n",
      "Iteration: 6666, Loss:2.29 \n",
      "Iteration: 6667, Loss:2.52 \n",
      "Iteration: 6668, Loss:1.98 \n",
      "Iteration: 6669, Loss:2.81 \n",
      "Iteration: 6670, Loss:2.61 \n",
      "Iteration: 6671, Loss:2.56 \n",
      "Iteration: 6672, Loss:2.96 \n",
      "Iteration: 6673, Loss:2.49 \n",
      "Iteration: 6674, Loss:2.66 \n",
      "Iteration: 6675, Loss:2.36 \n",
      "Iteration: 6676, Loss:2.62 \n",
      "Iteration: 6677, Loss:2.55 \n",
      "Iteration: 6678, Loss:2.58 \n",
      "Iteration: 6679, Loss:2.45 \n",
      "Iteration: 6680, Loss:1.90 \n",
      "Iteration: 6681, Loss:2.84 \n",
      "Iteration: 6682, Loss:2.81 \n",
      "Iteration: 6683, Loss:3.03 \n",
      "Iteration: 6684, Loss:2.73 \n",
      "Iteration: 6685, Loss:2.32 \n",
      "Iteration: 6686, Loss:2.74 \n",
      "Iteration: 6687, Loss:2.30 \n",
      "Iteration: 6688, Loss:2.75 \n",
      "Iteration: 6689, Loss:2.52 \n",
      "Iteration: 6690, Loss:2.46 \n",
      "Iteration: 6691, Loss:2.54 \n",
      "Iteration: 6692, Loss:2.49 \n",
      "Iteration: 6693, Loss:2.67 \n",
      "Iteration: 6694, Loss:1.87 \n",
      "Iteration: 6695, Loss:2.90 \n",
      "Iteration: 6696, Loss:2.92 \n",
      "Iteration: 6697, Loss:2.81 \n",
      "Iteration: 6698, Loss:2.79 \n",
      "Iteration: 6699, Loss:2.67 \n",
      "Iteration: 6700, Loss:2.47 \n",
      "Iteration: 6701, Loss:2.57 \n",
      "Iteration: 6702, Loss:2.30 \n",
      "Iteration: 6703, Loss:2.79 \n",
      "Iteration: 6704, Loss:2.30 \n",
      "Iteration: 6705, Loss:2.42 \n",
      "Iteration: 6706, Loss:2.20 \n",
      "Iteration: 6707, Loss:2.88 \n",
      "Iteration: 6708, Loss:1.85 \n",
      "Iteration: 6709, Loss:2.40 \n",
      "Iteration: 6710, Loss:1.99 \n",
      "Iteration: 6711, Loss:2.75 \n",
      "Iteration: 6712, Loss:2.29 \n",
      "Iteration: 6713, Loss:2.58 \n",
      "Iteration: 6714, Loss:2.37 \n",
      "Iteration: 6715, Loss:2.69 \n",
      "Iteration: 6716, Loss:2.73 \n",
      "Iteration: 6717, Loss:2.36 \n",
      "Iteration: 6718, Loss:2.60 \n",
      "Iteration: 6719, Loss:2.43 \n",
      "Iteration: 6720, Loss:2.99 \n",
      "Iteration: 6721, Loss:2.22 \n",
      "Iteration: 6722, Loss:2.78 \n",
      "Iteration: 6723, Loss:2.79 \n",
      "Iteration: 6724, Loss:2.71 \n",
      "Iteration: 6725, Loss:2.82 \n",
      "Iteration: 6726, Loss:2.47 \n",
      "Iteration: 6727, Loss:2.47 \n",
      "Iteration: 6728, Loss:2.55 \n",
      "Iteration: 6729, Loss:2.41 \n",
      "Iteration: 6730, Loss:2.51 \n",
      "Iteration: 6731, Loss:2.66 \n",
      "Iteration: 6732, Loss:2.90 \n",
      "Iteration: 6733, Loss:2.43 \n",
      "Iteration: 6734, Loss:2.17 \n",
      "Iteration: 6735, Loss:2.34 \n",
      "Iteration: 6736, Loss:2.65 \n",
      "Iteration: 6737, Loss:2.42 \n",
      "Iteration: 6738, Loss:2.70 \n",
      "Iteration: 6739, Loss:2.66 \n",
      "Iteration: 6740, Loss:2.49 \n",
      "Iteration: 6741, Loss:1.99 \n",
      "Iteration: 6742, Loss:2.51 \n",
      "Iteration: 6743, Loss:2.90 \n",
      "Iteration: 6744, Loss:2.38 \n",
      "Iteration: 6745, Loss:2.39 \n",
      "Iteration: 6746, Loss:2.32 \n",
      "Iteration: 6747, Loss:2.81 \n",
      "Iteration: 6748, Loss:2.54 \n",
      "Iteration: 6749, Loss:2.81 \n",
      "Iteration: 6750, Loss:2.88 \n",
      "Iteration: 6751, Loss:2.66 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_6750.ckpt\n",
      "Iteration: 6752, Loss:2.52 \n",
      "Iteration: 6753, Loss:2.78 \n",
      "Iteration: 6754, Loss:2.56 \n",
      "Iteration: 6755, Loss:2.27 \n",
      "Iteration: 6756, Loss:2.53 \n",
      "Iteration: 6757, Loss:2.64 \n",
      "Iteration: 6758, Loss:2.44 \n",
      "Iteration: 6759, Loss:2.33 \n",
      "Iteration: 6760, Loss:2.50 \n",
      "Iteration: 6761, Loss:2.82 \n",
      "Iteration: 6762, Loss:2.39 \n",
      "Iteration: 6763, Loss:2.46 \n",
      "Iteration: 6764, Loss:1.98 \n",
      "Iteration: 6765, Loss:2.28 \n",
      "Iteration: 6766, Loss:2.74 \n",
      "Iteration: 6767, Loss:2.91 \n",
      "Iteration: 6768, Loss:2.60 \n",
      "Iteration: 6769, Loss:2.29 \n",
      "Iteration: 6770, Loss:2.50 \n",
      "Iteration: 6771, Loss:2.69 \n",
      "Iteration: 6772, Loss:2.66 \n",
      "Iteration: 6773, Loss:2.22 \n",
      "Iteration: 6774, Loss:2.84 \n",
      "Iteration: 6775, Loss:2.62 \n",
      "Iteration: 6776, Loss:2.23 \n",
      "Iteration: 6777, Loss:2.51 \n",
      "Iteration: 6778, Loss:2.79 \n",
      "Iteration: 6779, Loss:2.44 \n",
      "Iteration: 6780, Loss:2.81 \n",
      "Iteration: 6781, Loss:2.53 \n",
      "Iteration: 6782, Loss:2.80 \n",
      "Iteration: 6783, Loss:2.60 \n",
      "Iteration: 6784, Loss:2.47 \n",
      "Iteration: 6785, Loss:2.77 \n",
      "Iteration: 6786, Loss:2.56 \n",
      "Iteration: 6787, Loss:2.85 \n",
      "Iteration: 6788, Loss:2.68 \n",
      "Iteration: 6789, Loss:2.27 \n",
      "Iteration: 6790, Loss:2.81 \n",
      "Iteration: 6791, Loss:2.52 \n",
      "Iteration: 6792, Loss:2.78 \n",
      "Iteration: 6793, Loss:2.27 \n",
      "Iteration: 6794, Loss:3.04 \n",
      "Iteration: 6795, Loss:2.24 \n",
      "Iteration: 6796, Loss:2.63 \n",
      "Iteration: 6797, Loss:2.73 \n",
      "Iteration: 6798, Loss:2.33 \n",
      "Iteration: 6799, Loss:2.42 \n",
      "Iteration: 6800, Loss:2.52 \n",
      "Iteration: 6801, Loss:2.83 \n",
      "Iteration: 6802, Loss:2.45 \n",
      "Iteration: 6803, Loss:2.60 \n",
      "Iteration: 6804, Loss:2.66 \n",
      "Iteration: 6805, Loss:2.55 \n",
      "Iteration: 6806, Loss:2.65 \n",
      "Iteration: 6807, Loss:2.49 \n",
      "Iteration: 6808, Loss:2.43 \n",
      "Iteration: 6809, Loss:2.10 \n",
      "Iteration: 6810, Loss:2.30 \n",
      "Iteration: 6811, Loss:2.97 \n",
      "Iteration: 6812, Loss:2.07 \n",
      "Iteration: 6813, Loss:2.21 \n",
      "Iteration: 6814, Loss:2.45 \n",
      "Iteration: 6815, Loss:2.93 \n",
      "Iteration: 6816, Loss:2.49 \n",
      "Iteration: 6817, Loss:2.36 \n",
      "Iteration: 6818, Loss:2.51 \n",
      "Iteration: 6819, Loss:2.71 \n",
      "Iteration: 6820, Loss:2.60 \n",
      "Iteration: 6821, Loss:2.81 \n",
      "Iteration: 6822, Loss:2.49 \n",
      "Iteration: 6823, Loss:2.48 \n",
      "Iteration: 6824, Loss:2.57 \n",
      "Iteration: 6825, Loss:2.47 \n",
      "Iteration: 6826, Loss:2.57 \n",
      "Iteration: 6827, Loss:2.55 \n",
      "Iteration: 6828, Loss:2.61 \n",
      "Iteration: 6829, Loss:2.60 \n",
      "Iteration: 6830, Loss:2.45 \n",
      "Iteration: 6831, Loss:2.76 \n",
      "Iteration: 6832, Loss:2.39 \n",
      "Iteration: 6833, Loss:2.76 \n",
      "Iteration: 6834, Loss:2.60 \n",
      "Iteration: 6835, Loss:2.78 \n",
      "Iteration: 6836, Loss:2.36 \n",
      "Iteration: 6837, Loss:2.40 \n",
      "Iteration: 6838, Loss:2.44 \n",
      "Iteration: 6839, Loss:2.89 \n",
      "Iteration: 6840, Loss:2.55 \n",
      "Iteration: 6841, Loss:2.35 \n",
      "Iteration: 6842, Loss:2.89 \n",
      "Iteration: 6843, Loss:2.41 \n",
      "Iteration: 6844, Loss:2.68 \n",
      "Iteration: 6845, Loss:2.71 \n",
      "Iteration: 6846, Loss:2.69 \n",
      "Iteration: 6847, Loss:2.58 \n",
      "Iteration: 6848, Loss:2.62 \n",
      "Iteration: 6849, Loss:2.14 \n",
      "Iteration: 6850, Loss:2.59 \n",
      "Iteration: 6851, Loss:2.91 \n",
      "Iteration: 6852, Loss:2.70 \n",
      "Iteration: 6853, Loss:2.19 \n",
      "Iteration: 6854, Loss:2.55 \n",
      "Iteration: 6855, Loss:2.47 \n",
      "Iteration: 6856, Loss:2.65 \n",
      "Iteration: 6857, Loss:2.44 \n",
      "Iteration: 6858, Loss:2.66 \n",
      "Iteration: 6859, Loss:2.42 \n",
      "Iteration: 6860, Loss:2.30 \n",
      "Iteration: 6861, Loss:2.16 \n",
      "Iteration: 6862, Loss:2.42 \n",
      "Iteration: 6863, Loss:2.78 \n",
      "Iteration: 6864, Loss:2.29 \n",
      "Iteration: 6865, Loss:2.23 \n",
      "Iteration: 6866, Loss:2.77 \n",
      "Iteration: 6867, Loss:2.29 \n",
      "Iteration: 6868, Loss:2.45 \n",
      "Iteration: 6869, Loss:2.46 \n",
      "Iteration: 6870, Loss:2.62 \n",
      "Iteration: 6871, Loss:2.47 \n",
      "Iteration: 6872, Loss:2.38 \n",
      "Iteration: 6873, Loss:2.28 \n",
      "Iteration: 6874, Loss:2.26 \n",
      "Iteration: 6875, Loss:2.48 \n",
      "Iteration: 6876, Loss:2.46 \n",
      "Iteration: 6877, Loss:2.78 \n",
      "Iteration: 6878, Loss:2.70 \n",
      "Iteration: 6879, Loss:2.83 \n",
      "Iteration: 6880, Loss:2.83 \n",
      "Iteration: 6881, Loss:2.82 \n",
      "Iteration: 6882, Loss:2.91 \n",
      "Iteration: 6883, Loss:3.13 \n",
      "Iteration: 6884, Loss:2.84 \n",
      "Iteration: 6885, Loss:2.06 \n",
      "Iteration: 6886, Loss:2.24 \n",
      "Iteration: 6887, Loss:2.43 \n",
      "Iteration: 6888, Loss:2.74 \n",
      "Iteration: 6889, Loss:2.87 \n",
      "Iteration: 6890, Loss:2.39 \n",
      "Iteration: 6891, Loss:2.92 \n",
      "Iteration: 6892, Loss:2.33 \n",
      "Iteration: 6893, Loss:2.67 \n",
      "Iteration: 6894, Loss:2.50 \n",
      "Iteration: 6895, Loss:2.66 \n",
      "Iteration: 6896, Loss:2.75 \n",
      "Iteration: 6897, Loss:2.29 \n",
      "Iteration: 6898, Loss:2.40 \n",
      "Iteration: 6899, Loss:2.30 \n",
      "Iteration: 6900, Loss:2.36 \n",
      "Iteration: 6901, Loss:2.07 \n",
      "Iteration: 6902, Loss:2.37 \n",
      "Iteration: 6903, Loss:2.55 \n",
      "Iteration: 6904, Loss:2.51 \n",
      "Iteration: 6905, Loss:2.71 \n",
      "Iteration: 6906, Loss:2.84 \n",
      "Iteration: 6907, Loss:2.48 \n",
      "Iteration: 6908, Loss:2.45 \n",
      "Iteration: 6909, Loss:2.77 \n",
      "Iteration: 6910, Loss:2.56 \n",
      "Iteration: 6911, Loss:2.73 \n",
      "Iteration: 6912, Loss:2.50 \n",
      "Iteration: 6913, Loss:2.60 \n",
      "Iteration: 6914, Loss:2.45 \n",
      "Iteration: 6915, Loss:2.51 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 6916, Loss:2.33 \n",
      "Iteration: 6917, Loss:2.50 \n",
      "Iteration: 6918, Loss:2.25 \n",
      "Iteration: 6919, Loss:2.90 \n",
      "Iteration: 6920, Loss:2.84 \n",
      "Iteration: 6921, Loss:2.75 \n",
      "Iteration: 6922, Loss:2.08 \n",
      "Iteration: 6923, Loss:2.63 \n",
      "Iteration: 6924, Loss:2.50 \n",
      "Iteration: 6925, Loss:2.28 \n",
      "Iteration: 6926, Loss:2.26 \n",
      "Iteration: 6927, Loss:2.29 \n",
      "Iteration: 6928, Loss:2.06 \n",
      "Iteration: 6929, Loss:2.90 \n",
      "Iteration: 6930, Loss:2.53 \n",
      "Iteration: 6931, Loss:3.02 \n",
      "Iteration: 6932, Loss:2.77 \n",
      "Iteration: 6933, Loss:2.13 \n",
      "Iteration: 6934, Loss:2.40 \n",
      "Iteration: 6935, Loss:2.55 \n",
      "Iteration: 6936, Loss:2.77 \n",
      "Iteration: 6937, Loss:2.71 \n",
      "Iteration: 6938, Loss:2.23 \n",
      "Iteration: 6939, Loss:2.37 \n",
      "Iteration: 6940, Loss:2.41 \n",
      "Iteration: 6941, Loss:2.67 \n",
      "Iteration: 6942, Loss:2.68 \n",
      "Iteration: 6943, Loss:2.92 \n",
      "Iteration: 6944, Loss:3.02 \n",
      "Iteration: 6945, Loss:2.26 \n",
      "Iteration: 6946, Loss:2.59 \n",
      "Iteration: 6947, Loss:2.24 \n",
      "Iteration: 6948, Loss:2.68 \n",
      "Iteration: 6949, Loss:2.87 \n",
      "Iteration: 6950, Loss:2.81 \n",
      "Iteration: 6951, Loss:2.35 \n",
      "Iteration: 6952, Loss:2.25 \n",
      "Iteration: 6953, Loss:2.16 \n",
      "Iteration: 6954, Loss:2.81 \n",
      "Iteration: 6955, Loss:2.70 \n",
      "Iteration: 6956, Loss:2.41 \n",
      "Iteration: 6957, Loss:2.72 \n",
      "Iteration: 6958, Loss:2.45 \n",
      "Iteration: 6959, Loss:2.78 \n",
      "Iteration: 6960, Loss:2.01 \n",
      "Iteration: 6961, Loss:2.77 \n",
      "Iteration: 6962, Loss:2.75 \n",
      "Iteration: 6963, Loss:2.51 \n",
      "Iteration: 6964, Loss:2.37 \n",
      "Iteration: 6965, Loss:2.35 \n",
      "Iteration: 6966, Loss:2.52 \n",
      "Iteration: 6967, Loss:2.60 \n",
      "Iteration: 6968, Loss:2.74 \n",
      "Iteration: 6969, Loss:2.60 \n",
      "Iteration: 6970, Loss:2.63 \n",
      "Iteration: 6971, Loss:2.77 \n",
      "Iteration: 6972, Loss:2.60 \n",
      "Iteration: 6973, Loss:2.81 \n",
      "Iteration: 6974, Loss:3.01 \n",
      "Iteration: 6975, Loss:2.20 \n",
      "Iteration: 6976, Loss:2.08 \n",
      "Iteration: 6977, Loss:2.65 \n",
      "Iteration: 6978, Loss:2.43 \n",
      "Iteration: 6979, Loss:2.41 \n",
      "Iteration: 6980, Loss:2.49 \n",
      "Iteration: 6981, Loss:2.18 \n",
      "Iteration: 6982, Loss:2.34 \n",
      "Iteration: 6983, Loss:2.89 \n",
      "Iteration: 6984, Loss:2.84 \n",
      "Iteration: 6985, Loss:2.12 \n",
      "Iteration: 6986, Loss:2.72 \n",
      "Iteration: 6987, Loss:2.67 \n",
      "Iteration: 6988, Loss:2.06 \n",
      "Iteration: 6989, Loss:2.47 \n",
      "Iteration: 6990, Loss:1.83 \n",
      "Iteration: 6991, Loss:2.77 \n",
      "Iteration: 6992, Loss:2.95 \n",
      "Iteration: 6993, Loss:2.53 \n",
      "Iteration: 6994, Loss:2.17 \n",
      "Iteration: 6995, Loss:2.47 \n",
      "Iteration: 6996, Loss:2.61 \n",
      "Iteration: 6997, Loss:2.83 \n",
      "Iteration: 6998, Loss:2.77 \n",
      "Iteration: 6999, Loss:2.68 \n",
      "Iteration: 7000, Loss:2.71 \n",
      "Iteration: 7001, Loss:2.69 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_7000.ckpt\n",
      "Iteration: 7002, Loss:2.73 \n",
      "Iteration: 7003, Loss:2.71 \n",
      "Iteration: 7004, Loss:2.52 \n",
      "Iteration: 7005, Loss:2.49 \n",
      "Iteration: 7006, Loss:2.81 \n",
      "Iteration: 7007, Loss:2.29 \n",
      "Iteration: 7008, Loss:2.87 \n",
      "Iteration: 7009, Loss:2.51 \n",
      "Iteration: 7010, Loss:2.20 \n",
      "Iteration: 7011, Loss:2.55 \n",
      "Iteration: 7012, Loss:2.42 \n",
      "Iteration: 7013, Loss:2.60 \n",
      "Iteration: 7014, Loss:2.26 \n",
      "Iteration: 7015, Loss:2.54 \n",
      "Iteration: 7016, Loss:2.67 \n",
      "Iteration: 7017, Loss:2.89 \n",
      "Iteration: 7018, Loss:2.59 \n",
      "Iteration: 7019, Loss:2.57 \n",
      "Iteration: 7020, Loss:2.71 \n",
      "Iteration: 7021, Loss:2.82 \n",
      "Iteration: 7022, Loss:2.10 \n",
      "Iteration: 7023, Loss:2.58 \n",
      "Iteration: 7024, Loss:2.56 \n",
      "Iteration: 7025, Loss:2.54 \n",
      "Iteration: 7026, Loss:2.21 \n",
      "Iteration: 7027, Loss:2.42 \n",
      "Iteration: 7028, Loss:2.81 \n",
      "Iteration: 7029, Loss:2.49 \n",
      "Iteration: 7030, Loss:2.64 \n",
      "Iteration: 7031, Loss:2.41 \n",
      "Iteration: 7032, Loss:2.52 \n",
      "Iteration: 7033, Loss:2.48 \n",
      "Iteration: 7034, Loss:2.18 \n",
      "Iteration: 7035, Loss:2.46 \n",
      "Iteration: 7036, Loss:2.53 \n",
      "Iteration: 7037, Loss:2.18 \n",
      "Iteration: 7038, Loss:2.40 \n",
      "Iteration: 7039, Loss:2.81 \n",
      "Iteration: 7040, Loss:2.64 \n",
      "Iteration: 7041, Loss:2.44 \n",
      "Iteration: 7042, Loss:2.57 \n",
      "Iteration: 7043, Loss:2.52 \n",
      "Iteration: 7044, Loss:2.73 \n",
      "Iteration: 7045, Loss:2.78 \n",
      "Iteration: 7046, Loss:2.68 \n",
      "Iteration: 7047, Loss:2.51 \n",
      "Iteration: 7048, Loss:1.93 \n",
      "Iteration: 7049, Loss:2.73 \n",
      "Iteration: 7050, Loss:2.41 \n",
      "Iteration: 7051, Loss:2.68 \n",
      "Iteration: 7052, Loss:2.47 \n",
      "Iteration: 7053, Loss:2.71 \n",
      "Iteration: 7054, Loss:2.12 \n",
      "Iteration: 7055, Loss:2.58 \n",
      "Iteration: 7056, Loss:2.09 \n",
      "Iteration: 7057, Loss:2.57 \n",
      "Iteration: 7058, Loss:2.68 \n",
      "Iteration: 7059, Loss:2.70 \n",
      "Iteration: 7060, Loss:2.03 \n",
      "Iteration: 7061, Loss:2.12 \n",
      "Iteration: 7062, Loss:2.66 \n",
      "Iteration: 7063, Loss:2.62 \n",
      "Iteration: 7064, Loss:2.69 \n",
      "Iteration: 7065, Loss:2.29 \n",
      "Iteration: 7066, Loss:2.38 \n",
      "Iteration: 7067, Loss:2.56 \n",
      "Iteration: 7068, Loss:3.02 \n",
      "Iteration: 7069, Loss:2.69 \n",
      "Iteration: 7070, Loss:2.45 \n",
      "Iteration: 7071, Loss:2.76 \n",
      "Iteration: 7072, Loss:2.56 \n",
      "Iteration: 7073, Loss:2.56 \n",
      "Iteration: 7074, Loss:2.75 \n",
      "Iteration: 7075, Loss:2.21 \n",
      "Iteration: 7076, Loss:2.52 \n",
      "Iteration: 7077, Loss:2.55 \n",
      "Iteration: 7078, Loss:2.46 \n",
      "Iteration: 7079, Loss:2.07 \n",
      "Iteration: 7080, Loss:2.73 \n",
      "Iteration: 7081, Loss:2.66 \n",
      "Iteration: 7082, Loss:2.93 \n",
      "Iteration: 7083, Loss:2.31 \n",
      "Iteration: 7084, Loss:2.86 \n",
      "Iteration: 7085, Loss:2.89 \n",
      "Iteration: 7086, Loss:2.69 \n",
      "Iteration: 7087, Loss:2.64 \n",
      "Iteration: 7088, Loss:2.81 \n",
      "Iteration: 7089, Loss:2.53 \n",
      "Iteration: 7090, Loss:2.58 \n",
      "Iteration: 7091, Loss:2.28 \n",
      "Iteration: 7092, Loss:2.68 \n",
      "Iteration: 7093, Loss:2.08 \n",
      "Iteration: 7094, Loss:2.79 \n",
      "Iteration: 7095, Loss:2.82 \n",
      "Iteration: 7096, Loss:2.98 \n",
      "Iteration: 7097, Loss:2.20 \n",
      "Iteration: 7098, Loss:2.67 \n",
      "Iteration: 7099, Loss:2.85 \n",
      "Iteration: 7100, Loss:2.72 \n",
      "Iteration: 7101, Loss:2.31 \n",
      "Iteration: 7102, Loss:2.62 \n",
      "Iteration: 7103, Loss:2.58 \n",
      "Iteration: 7104, Loss:2.46 \n",
      "Iteration: 7105, Loss:2.42 \n",
      "Iteration: 7106, Loss:2.55 \n",
      "Iteration: 7107, Loss:2.74 \n",
      "Iteration: 7108, Loss:2.65 \n",
      "Iteration: 7109, Loss:2.58 \n",
      "Iteration: 7110, Loss:2.85 \n",
      "Iteration: 7111, Loss:2.82 \n",
      "Iteration: 7112, Loss:2.48 \n",
      "Iteration: 7113, Loss:2.53 \n",
      "Iteration: 7114, Loss:2.51 \n",
      "Iteration: 7115, Loss:2.54 \n",
      "Iteration: 7116, Loss:2.83 \n",
      "Iteration: 7117, Loss:2.75 \n",
      "Iteration: 7118, Loss:2.75 \n",
      "Iteration: 7119, Loss:2.37 \n",
      "Iteration: 7120, Loss:2.95 \n",
      "Iteration: 7121, Loss:2.12 \n",
      "Iteration: 7122, Loss:2.19 \n",
      "Iteration: 7123, Loss:2.39 \n",
      "Iteration: 7124, Loss:2.59 \n",
      "Iteration: 7125, Loss:2.95 \n",
      "Iteration: 7126, Loss:2.17 \n",
      "Iteration: 7127, Loss:2.06 \n",
      "Iteration: 7128, Loss:2.25 \n",
      "Iteration: 7129, Loss:2.53 \n",
      "Iteration: 7130, Loss:2.34 \n",
      "Iteration: 7131, Loss:2.84 \n",
      "Iteration: 7132, Loss:2.38 \n",
      "Iteration: 7133, Loss:2.29 \n",
      "Iteration: 7134, Loss:2.22 \n",
      "Iteration: 7135, Loss:2.70 \n",
      "Iteration: 7136, Loss:2.32 \n",
      "Iteration: 7137, Loss:2.72 \n",
      "Iteration: 7138, Loss:2.61 \n",
      "Iteration: 7139, Loss:2.67 \n",
      "Iteration: 7140, Loss:2.46 \n",
      "Iteration: 7141, Loss:2.86 \n",
      "Iteration: 7142, Loss:2.59 \n",
      "Iteration: 7143, Loss:2.95 \n",
      "Iteration: 7144, Loss:2.38 \n",
      "Iteration: 7145, Loss:2.48 \n",
      "Iteration: 7146, Loss:2.37 \n",
      "Iteration: 7147, Loss:2.91 \n",
      "Iteration: 7148, Loss:2.57 \n",
      "Iteration: 7149, Loss:2.38 \n",
      "Iteration: 7150, Loss:2.09 \n",
      "Iteration: 7151, Loss:3.06 \n",
      "Iteration: 7152, Loss:2.54 \n",
      "Iteration: 7153, Loss:2.27 \n",
      "Iteration: 7154, Loss:2.32 \n",
      "Iteration: 7155, Loss:2.25 \n",
      "Iteration: 7156, Loss:2.68 \n",
      "Iteration: 7157, Loss:2.66 \n",
      "Iteration: 7158, Loss:2.53 \n",
      "Iteration: 7159, Loss:2.22 \n",
      "Iteration: 7160, Loss:2.61 \n",
      "Iteration: 7161, Loss:2.29 \n",
      "Iteration: 7162, Loss:3.03 \n",
      "Iteration: 7163, Loss:2.52 \n",
      "Iteration: 7164, Loss:2.53 \n",
      "Iteration: 7165, Loss:2.41 \n",
      "Iteration: 7166, Loss:2.47 \n",
      "Iteration: 7167, Loss:2.47 \n",
      "Iteration: 7168, Loss:2.68 \n",
      "Iteration: 7169, Loss:2.80 \n",
      "Iteration: 7170, Loss:2.61 \n",
      "Iteration: 7171, Loss:2.41 \n",
      "Iteration: 7172, Loss:2.50 \n",
      "Iteration: 7173, Loss:2.81 \n",
      "Iteration: 7174, Loss:2.50 \n",
      "Iteration: 7175, Loss:2.51 \n",
      "Iteration: 7176, Loss:2.75 \n",
      "Iteration: 7177, Loss:2.11 \n",
      "Iteration: 7178, Loss:2.59 \n",
      "Iteration: 7179, Loss:3.07 \n",
      "Iteration: 7180, Loss:2.39 \n",
      "Iteration: 7181, Loss:2.34 \n",
      "Iteration: 7182, Loss:2.37 \n",
      "Iteration: 7183, Loss:2.45 \n",
      "Iteration: 7184, Loss:2.27 \n",
      "Iteration: 7185, Loss:2.80 \n",
      "Iteration: 7186, Loss:2.56 \n",
      "Iteration: 7187, Loss:2.27 \n",
      "Iteration: 7188, Loss:2.51 \n",
      "Iteration: 7189, Loss:2.07 \n",
      "Iteration: 7190, Loss:2.44 \n",
      "Iteration: 7191, Loss:2.77 \n",
      "Iteration: 7192, Loss:2.54 \n",
      "Iteration: 7193, Loss:2.57 \n",
      "Iteration: 7194, Loss:2.25 \n",
      "Iteration: 7195, Loss:2.75 \n",
      "Iteration: 7196, Loss:2.53 \n",
      "Iteration: 7197, Loss:2.78 \n",
      "Iteration: 7198, Loss:2.47 \n",
      "Iteration: 7199, Loss:2.61 \n",
      "Iteration: 7200, Loss:2.19 \n",
      "Iteration: 7201, Loss:2.57 \n",
      "Iteration: 7202, Loss:2.96 \n",
      "Iteration: 7203, Loss:2.37 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7204, Loss:2.37 \n",
      "Iteration: 7205, Loss:2.51 \n",
      "Iteration: 7206, Loss:2.62 \n",
      "Iteration: 7207, Loss:2.38 \n",
      "Iteration: 7208, Loss:2.73 \n",
      "Iteration: 7209, Loss:2.76 \n",
      "Iteration: 7210, Loss:2.92 \n",
      "Iteration: 7211, Loss:2.60 \n",
      "Iteration: 7212, Loss:2.27 \n",
      "Iteration: 7213, Loss:2.55 \n",
      "Iteration: 7214, Loss:2.29 \n",
      "Iteration: 7215, Loss:2.87 \n",
      "Iteration: 7216, Loss:2.63 \n",
      "Iteration: 7217, Loss:2.55 \n",
      "Iteration: 7218, Loss:1.92 \n",
      "Iteration: 7219, Loss:2.53 \n",
      "Iteration: 7220, Loss:2.61 \n",
      "Iteration: 7221, Loss:2.53 \n",
      "Iteration: 7222, Loss:2.05 \n",
      "Iteration: 7223, Loss:2.41 \n",
      "Iteration: 7224, Loss:2.19 \n",
      "Iteration: 7225, Loss:2.63 \n",
      "Iteration: 7226, Loss:2.57 \n",
      "Iteration: 7227, Loss:2.74 \n",
      "Iteration: 7228, Loss:2.58 \n",
      "Iteration: 7229, Loss:2.46 \n",
      "Iteration: 7230, Loss:2.75 \n",
      "Iteration: 7231, Loss:2.78 \n",
      "Iteration: 7232, Loss:2.57 \n",
      "Iteration: 7233, Loss:2.57 \n",
      "Iteration: 7234, Loss:2.43 \n",
      "Iteration: 7235, Loss:2.91 \n",
      "Iteration: 7236, Loss:2.53 \n",
      "Iteration: 7237, Loss:2.66 \n",
      "Iteration: 7238, Loss:2.36 \n",
      "Iteration: 7239, Loss:2.27 \n",
      "Iteration: 7240, Loss:2.42 \n",
      "Iteration: 7241, Loss:2.35 \n",
      "Iteration: 7242, Loss:2.90 \n",
      "Iteration: 7243, Loss:2.42 \n",
      "Iteration: 7244, Loss:2.79 \n",
      "Iteration: 7245, Loss:2.66 \n",
      "Iteration: 7246, Loss:2.75 \n",
      "Iteration: 7247, Loss:2.80 \n",
      "Iteration: 7248, Loss:2.50 \n",
      "Iteration: 7249, Loss:2.90 \n",
      "Iteration: 7250, Loss:2.68 \n",
      "Iteration: 7251, Loss:2.75 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_7250.ckpt\n",
      "Iteration: 7252, Loss:2.34 \n",
      "Iteration: 7253, Loss:2.45 \n",
      "Iteration: 7254, Loss:2.33 \n",
      "Iteration: 7255, Loss:2.24 \n",
      "Iteration: 7256, Loss:2.87 \n",
      "Iteration: 7257, Loss:2.55 \n",
      "Iteration: 7258, Loss:2.57 \n",
      "Iteration: 7259, Loss:2.53 \n",
      "Iteration: 7260, Loss:2.46 \n",
      "Iteration: 7261, Loss:2.58 \n",
      "Iteration: 7262, Loss:2.51 \n",
      "Iteration: 7263, Loss:2.34 \n",
      "Iteration: 7264, Loss:2.68 \n",
      "Iteration: 7265, Loss:2.67 \n",
      "Iteration: 7266, Loss:2.56 \n",
      "Iteration: 7267, Loss:2.34 \n",
      "Iteration: 7268, Loss:2.58 \n",
      "Iteration: 7269, Loss:2.63 \n",
      "Iteration: 7270, Loss:2.37 \n",
      "Iteration: 7271, Loss:2.50 \n",
      "Iteration: 7272, Loss:2.70 \n",
      "Iteration: 7273, Loss:2.66 \n",
      "Iteration: 7274, Loss:2.59 \n",
      "Iteration: 7275, Loss:2.79 \n",
      "Iteration: 7276, Loss:2.32 \n",
      "Iteration: 7277, Loss:2.49 \n",
      "Iteration: 7278, Loss:2.47 \n",
      "Iteration: 7279, Loss:2.32 \n",
      "Iteration: 7280, Loss:2.51 \n",
      "Iteration: 7281, Loss:2.12 \n",
      "Iteration: 7282, Loss:2.36 \n",
      "Iteration: 7283, Loss:2.55 \n",
      "Iteration: 7284, Loss:2.42 \n",
      "Iteration: 7285, Loss:2.66 \n",
      "Iteration: 7286, Loss:2.65 \n",
      "Iteration: 7287, Loss:2.57 \n",
      "Iteration: 7288, Loss:2.88 \n",
      "Iteration: 7289, Loss:2.53 \n",
      "Iteration: 7290, Loss:2.75 \n",
      "Iteration: 7291, Loss:2.49 \n",
      "Iteration: 7292, Loss:1.90 \n",
      "Iteration: 7293, Loss:2.58 \n",
      "Iteration: 7294, Loss:2.66 \n",
      "Iteration: 7295, Loss:2.63 \n",
      "Iteration: 7296, Loss:2.47 \n",
      "Iteration: 7297, Loss:2.43 \n",
      "Iteration: 7298, Loss:2.23 \n",
      "Iteration: 7299, Loss:2.55 \n",
      "Iteration: 7300, Loss:2.62 \n",
      "Iteration: 7301, Loss:2.79 \n",
      "Iteration: 7302, Loss:2.32 \n",
      "Iteration: 7303, Loss:2.27 \n",
      "Iteration: 7304, Loss:2.76 \n",
      "Iteration: 7305, Loss:2.85 \n",
      "Iteration: 7306, Loss:2.58 \n",
      "Iteration: 7307, Loss:2.56 \n",
      "Iteration: 7308, Loss:2.65 \n",
      "Iteration: 7309, Loss:2.54 \n",
      "Iteration: 7310, Loss:3.05 \n",
      "Iteration: 7311, Loss:2.69 \n",
      "Iteration: 7312, Loss:2.73 \n",
      "Iteration: 7313, Loss:2.31 \n",
      "Iteration: 7314, Loss:2.31 \n",
      "Iteration: 7315, Loss:2.36 \n",
      "Iteration: 7316, Loss:2.64 \n",
      "Iteration: 7317, Loss:2.68 \n",
      "Iteration: 7318, Loss:2.35 \n",
      "Iteration: 7319, Loss:2.37 \n",
      "Iteration: 7320, Loss:2.60 \n",
      "Iteration: 7321, Loss:2.77 \n",
      "Iteration: 7322, Loss:2.73 \n",
      "Iteration: 7323, Loss:2.59 \n",
      "Iteration: 7324, Loss:2.83 \n",
      "Iteration: 7325, Loss:2.73 \n",
      "Iteration: 7326, Loss:2.68 \n",
      "Iteration: 7327, Loss:2.84 \n",
      "Iteration: 7328, Loss:2.34 \n",
      "Iteration: 7329, Loss:2.72 \n",
      "Iteration: 7330, Loss:2.42 \n",
      "Iteration: 7331, Loss:2.86 \n",
      "Iteration: 7332, Loss:2.42 \n",
      "Iteration: 7333, Loss:2.58 \n",
      "Iteration: 7334, Loss:2.47 \n",
      "Iteration: 7335, Loss:2.37 \n",
      "Iteration: 7336, Loss:2.21 \n",
      "Iteration: 7337, Loss:2.11 \n",
      "Iteration: 7338, Loss:2.37 \n",
      "Iteration: 7339, Loss:2.01 \n",
      "Iteration: 7340, Loss:2.43 \n",
      "Iteration: 7341, Loss:2.18 \n",
      "Iteration: 7342, Loss:2.13 \n",
      "Iteration: 7343, Loss:2.33 \n",
      "Iteration: 7344, Loss:2.89 \n",
      "Iteration: 7345, Loss:2.34 \n",
      "Iteration: 7346, Loss:2.77 \n",
      "Iteration: 7347, Loss:2.51 \n",
      "Iteration: 7348, Loss:2.83 \n",
      "Iteration: 7349, Loss:2.28 \n",
      "Iteration: 7350, Loss:2.43 \n",
      "Iteration: 7351, Loss:2.59 \n",
      "Iteration: 7352, Loss:2.58 \n",
      "Iteration: 7353, Loss:2.88 \n",
      "Iteration: 7354, Loss:2.40 \n",
      "Iteration: 7355, Loss:2.60 \n",
      "Iteration: 7356, Loss:2.62 \n",
      "Iteration: 7357, Loss:2.32 \n",
      "Iteration: 7358, Loss:2.54 \n",
      "Iteration: 7359, Loss:2.00 \n",
      "Iteration: 7360, Loss:2.59 \n",
      "Iteration: 7361, Loss:2.62 \n",
      "Iteration: 7362, Loss:2.39 \n",
      "Iteration: 7363, Loss:2.49 \n",
      "Iteration: 7364, Loss:3.05 \n",
      "Iteration: 7365, Loss:2.84 \n",
      "Iteration: 7366, Loss:2.48 \n",
      "Iteration: 7367, Loss:2.52 \n",
      "Iteration: 7368, Loss:2.63 \n",
      "Iteration: 7369, Loss:3.08 \n",
      "Iteration: 7370, Loss:2.57 \n",
      "Iteration: 7371, Loss:2.27 \n",
      "Iteration: 7372, Loss:2.32 \n",
      "Iteration: 7373, Loss:2.74 \n",
      "Iteration: 7374, Loss:2.88 \n",
      "Iteration: 7375, Loss:2.63 \n",
      "Iteration: 7376, Loss:2.46 \n",
      "Iteration: 7377, Loss:2.33 \n",
      "Iteration: 7378, Loss:2.61 \n",
      "Iteration: 7379, Loss:2.57 \n",
      "Iteration: 7380, Loss:2.56 \n",
      "Iteration: 7381, Loss:3.03 \n",
      "Iteration: 7382, Loss:2.66 \n",
      "Iteration: 7383, Loss:1.94 \n",
      "Iteration: 7384, Loss:2.33 \n",
      "Iteration: 7385, Loss:2.49 \n",
      "Iteration: 7386, Loss:2.96 \n",
      "Iteration: 7387, Loss:2.74 \n",
      "Iteration: 7388, Loss:2.79 \n",
      "Iteration: 7389, Loss:2.09 \n",
      "Iteration: 7390, Loss:2.58 \n",
      "Iteration: 7391, Loss:2.59 \n",
      "Iteration: 7392, Loss:2.62 \n",
      "Iteration: 7393, Loss:2.05 \n",
      "Iteration: 7394, Loss:2.62 \n",
      "Iteration: 7395, Loss:2.36 \n",
      "Iteration: 7396, Loss:2.56 \n",
      "Iteration: 7397, Loss:2.21 \n",
      "Iteration: 7398, Loss:2.78 \n",
      "Iteration: 7399, Loss:2.36 \n",
      "Iteration: 7400, Loss:2.65 \n",
      "Iteration: 7401, Loss:2.69 \n",
      "Iteration: 7402, Loss:2.90 \n",
      "Iteration: 7403, Loss:2.99 \n",
      "Iteration: 7404, Loss:2.15 \n",
      "Iteration: 7405, Loss:2.79 \n",
      "Iteration: 7406, Loss:2.50 \n",
      "Iteration: 7407, Loss:2.68 \n",
      "Iteration: 7408, Loss:2.64 \n",
      "Iteration: 7409, Loss:2.56 \n",
      "Iteration: 7410, Loss:2.22 \n",
      "Iteration: 7411, Loss:2.50 \n",
      "Iteration: 7412, Loss:2.89 \n",
      "Iteration: 7413, Loss:2.79 \n",
      "Iteration: 7414, Loss:2.44 \n",
      "Iteration: 7415, Loss:2.43 \n",
      "Iteration: 7416, Loss:2.50 \n",
      "Iteration: 7417, Loss:2.49 \n",
      "Iteration: 7418, Loss:2.82 \n",
      "Iteration: 7419, Loss:2.32 \n",
      "Iteration: 7420, Loss:2.77 \n",
      "Iteration: 7421, Loss:2.45 \n",
      "Iteration: 7422, Loss:2.80 \n",
      "Iteration: 7423, Loss:2.52 \n",
      "Iteration: 7424, Loss:2.58 \n",
      "Iteration: 7425, Loss:2.39 \n",
      "Iteration: 7426, Loss:2.45 \n",
      "Iteration: 7427, Loss:2.49 \n",
      "Iteration: 7428, Loss:2.42 \n",
      "Iteration: 7429, Loss:2.94 \n",
      "Iteration: 7430, Loss:2.73 \n",
      "Iteration: 7431, Loss:2.10 \n",
      "Iteration: 7432, Loss:2.63 \n",
      "Iteration: 7433, Loss:2.66 \n",
      "Iteration: 7434, Loss:2.23 \n",
      "Iteration: 7435, Loss:2.81 \n",
      "Iteration: 7436, Loss:2.67 \n",
      "Iteration: 7437, Loss:2.45 \n",
      "Iteration: 7438, Loss:2.73 \n",
      "Iteration: 7439, Loss:2.73 \n",
      "Iteration: 7440, Loss:2.22 \n",
      "Iteration: 7441, Loss:2.33 \n",
      "Iteration: 7442, Loss:2.32 \n",
      "Iteration: 7443, Loss:2.92 \n",
      "Iteration: 7444, Loss:2.78 \n",
      "Iteration: 7445, Loss:1.93 \n",
      "Iteration: 7446, Loss:2.79 \n",
      "Iteration: 7447, Loss:2.28 \n",
      "Iteration: 7448, Loss:2.74 \n",
      "Iteration: 7449, Loss:2.62 \n",
      "Iteration: 7450, Loss:2.27 \n",
      "Iteration: 7451, Loss:3.04 \n",
      "Iteration: 7452, Loss:2.34 \n",
      "Iteration: 7453, Loss:2.49 \n",
      "Iteration: 7454, Loss:2.39 \n",
      "Iteration: 7455, Loss:2.63 \n",
      "Iteration: 7456, Loss:2.58 \n",
      "Iteration: 7457, Loss:2.60 \n",
      "Iteration: 7458, Loss:3.05 \n",
      "Iteration: 7459, Loss:2.64 \n",
      "Iteration: 7460, Loss:2.44 \n",
      "Iteration: 7461, Loss:2.55 \n",
      "Iteration: 7462, Loss:2.65 \n",
      "Iteration: 7463, Loss:2.50 \n",
      "Iteration: 7464, Loss:2.60 \n",
      "Iteration: 7465, Loss:2.43 \n",
      "Iteration: 7466, Loss:2.10 \n",
      "Iteration: 7467, Loss:2.62 \n",
      "Iteration: 7468, Loss:2.86 \n",
      "Iteration: 7469, Loss:2.65 \n",
      "Iteration: 7470, Loss:2.62 \n",
      "Iteration: 7471, Loss:2.59 \n",
      "Iteration: 7472, Loss:2.53 \n",
      "Iteration: 7473, Loss:2.42 \n",
      "Iteration: 7474, Loss:2.82 \n",
      "Iteration: 7475, Loss:2.21 \n",
      "Iteration: 7476, Loss:2.39 \n",
      "Iteration: 7477, Loss:2.79 \n",
      "Iteration: 7478, Loss:2.07 \n",
      "Iteration: 7479, Loss:2.32 \n",
      "Iteration: 7480, Loss:2.42 \n",
      "Iteration: 7481, Loss:2.29 \n",
      "Iteration: 7482, Loss:2.66 \n",
      "Iteration: 7483, Loss:2.13 \n",
      "Iteration: 7484, Loss:2.51 \n",
      "Iteration: 7485, Loss:2.34 \n",
      "Iteration: 7486, Loss:2.30 \n",
      "Iteration: 7487, Loss:2.48 \n",
      "Iteration: 7488, Loss:2.93 \n",
      "Iteration: 7489, Loss:2.26 \n",
      "Iteration: 7490, Loss:2.41 \n",
      "Iteration: 7491, Loss:2.71 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7492, Loss:2.45 \n",
      "Iteration: 7493, Loss:2.82 \n",
      "Iteration: 7494, Loss:2.43 \n",
      "Iteration: 7495, Loss:2.27 \n",
      "Iteration: 7496, Loss:2.30 \n",
      "Iteration: 7497, Loss:2.58 \n",
      "Iteration: 7498, Loss:2.43 \n",
      "Iteration: 7499, Loss:2.76 \n",
      "Iteration: 7500, Loss:2.68 \n",
      "Iteration: 7501, Loss:2.16 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_7500.ckpt\n",
      "Iteration: 7502, Loss:2.27 \n",
      "Iteration: 7503, Loss:2.16 \n",
      "Iteration: 7504, Loss:2.78 \n",
      "Iteration: 7505, Loss:2.69 \n",
      "Iteration: 7506, Loss:2.62 \n",
      "Iteration: 7507, Loss:2.22 \n",
      "Iteration: 7508, Loss:2.79 \n",
      "Iteration: 7509, Loss:2.86 \n",
      "Iteration: 7510, Loss:2.67 \n",
      "Iteration: 7511, Loss:2.75 \n",
      "Iteration: 7512, Loss:2.43 \n",
      "Iteration: 7513, Loss:2.64 \n",
      "Iteration: 7514, Loss:2.64 \n",
      "Iteration: 7515, Loss:2.23 \n",
      "Iteration: 7516, Loss:2.60 \n",
      "Iteration: 7517, Loss:2.14 \n",
      "Iteration: 7518, Loss:2.61 \n",
      "Iteration: 7519, Loss:2.14 \n",
      "Iteration: 7520, Loss:2.80 \n",
      "Iteration: 7521, Loss:2.73 \n",
      "Iteration: 7522, Loss:2.89 \n",
      "Iteration: 7523, Loss:2.45 \n",
      "Iteration: 7524, Loss:2.25 \n",
      "Iteration: 7525, Loss:2.39 \n",
      "Iteration: 7526, Loss:2.69 \n",
      "Iteration: 7527, Loss:2.47 \n",
      "Iteration: 7528, Loss:2.89 \n",
      "Iteration: 7529, Loss:2.55 \n",
      "Iteration: 7530, Loss:2.81 \n",
      "Iteration: 7531, Loss:2.19 \n",
      "Iteration: 7532, Loss:2.49 \n",
      "Iteration: 7533, Loss:2.63 \n",
      "Iteration: 7534, Loss:2.91 \n",
      "Iteration: 7535, Loss:2.15 \n",
      "Iteration: 7536, Loss:2.81 \n",
      "Iteration: 7537, Loss:2.49 \n",
      "Iteration: 7538, Loss:2.44 \n",
      "Iteration: 7539, Loss:2.47 \n",
      "Iteration: 7540, Loss:2.99 \n",
      "Iteration: 7541, Loss:2.38 \n",
      "Iteration: 7542, Loss:2.19 \n",
      "Iteration: 7543, Loss:2.42 \n",
      "Iteration: 7544, Loss:2.39 \n",
      "Iteration: 7545, Loss:2.57 \n",
      "Iteration: 7546, Loss:2.83 \n",
      "Iteration: 7547, Loss:2.85 \n",
      "Iteration: 7548, Loss:2.35 \n",
      "Iteration: 7549, Loss:2.44 \n",
      "Iteration: 7550, Loss:2.83 \n",
      "Iteration: 7551, Loss:2.83 \n",
      "Iteration: 7552, Loss:2.71 \n",
      "Iteration: 7553, Loss:2.79 \n",
      "Iteration: 7554, Loss:2.66 \n",
      "Iteration: 7555, Loss:2.57 \n",
      "Iteration: 7556, Loss:2.20 \n",
      "Iteration: 7557, Loss:2.29 \n",
      "Iteration: 7558, Loss:3.00 \n",
      "Iteration: 7559, Loss:2.57 \n",
      "Iteration: 7560, Loss:2.64 \n",
      "Iteration: 7561, Loss:2.46 \n",
      "Iteration: 7562, Loss:2.48 \n",
      "Iteration: 7563, Loss:2.79 \n",
      "Iteration: 7564, Loss:2.63 \n",
      "Iteration: 7565, Loss:2.65 \n",
      "Iteration: 7566, Loss:1.90 \n",
      "Iteration: 7567, Loss:2.34 \n",
      "Iteration: 7568, Loss:2.71 \n",
      "Iteration: 7569, Loss:2.69 \n",
      "Iteration: 7570, Loss:2.96 \n",
      "Iteration: 7571, Loss:2.35 \n",
      "Iteration: 7572, Loss:2.26 \n",
      "Iteration: 7573, Loss:2.59 \n",
      "Iteration: 7574, Loss:2.31 \n",
      "Iteration: 7575, Loss:2.25 \n",
      "Iteration: 7576, Loss:2.19 \n",
      "Iteration: 7577, Loss:2.48 \n",
      "Iteration: 7578, Loss:2.73 \n",
      "Iteration: 7579, Loss:2.67 \n",
      "Iteration: 7580, Loss:2.53 \n",
      "Iteration: 7581, Loss:2.71 \n",
      "Iteration: 7582, Loss:2.34 \n",
      "Iteration: 7583, Loss:2.76 \n",
      "Iteration: 7584, Loss:2.24 \n",
      "Iteration: 7585, Loss:2.81 \n",
      "Iteration: 7586, Loss:3.01 \n",
      "Iteration: 7587, Loss:2.58 \n",
      "Iteration: 7588, Loss:2.72 \n",
      "Iteration: 7589, Loss:2.51 \n",
      "Iteration: 7590, Loss:2.50 \n",
      "Iteration: 7591, Loss:2.13 \n",
      "Iteration: 7592, Loss:2.59 \n",
      "Iteration: 7593, Loss:3.10 \n",
      "Iteration: 7594, Loss:2.48 \n",
      "Iteration: 7595, Loss:2.43 \n",
      "Iteration: 7596, Loss:2.62 \n",
      "Iteration: 7597, Loss:2.69 \n",
      "Iteration: 7598, Loss:2.75 \n",
      "Iteration: 7599, Loss:2.62 \n",
      "Iteration: 7600, Loss:2.39 \n",
      "Iteration: 7601, Loss:2.36 \n",
      "Iteration: 7602, Loss:2.34 \n",
      "Iteration: 7603, Loss:2.28 \n",
      "Iteration: 7604, Loss:2.43 \n",
      "Iteration: 7605, Loss:2.58 \n",
      "Iteration: 7606, Loss:2.28 \n",
      "Iteration: 7607, Loss:2.69 \n",
      "Iteration: 7608, Loss:2.23 \n",
      "Iteration: 7609, Loss:2.52 \n",
      "Iteration: 7610, Loss:2.23 \n",
      "Iteration: 7611, Loss:2.12 \n",
      "Iteration: 7612, Loss:2.73 \n",
      "Iteration: 7613, Loss:2.29 \n",
      "Iteration: 7614, Loss:2.40 \n",
      "Iteration: 7615, Loss:2.90 \n",
      "Iteration: 7616, Loss:2.18 \n",
      "Iteration: 7617, Loss:2.60 \n",
      "Iteration: 7618, Loss:2.48 \n",
      "Iteration: 7619, Loss:2.58 \n",
      "Iteration: 7620, Loss:2.45 \n",
      "Iteration: 7621, Loss:2.72 \n",
      "Iteration: 7622, Loss:2.01 \n",
      "Iteration: 7623, Loss:2.73 \n",
      "Iteration: 7624, Loss:2.93 \n",
      "Iteration: 7625, Loss:2.55 \n",
      "Iteration: 7626, Loss:2.39 \n",
      "Iteration: 7627, Loss:2.24 \n",
      "Iteration: 7628, Loss:2.47 \n",
      "Iteration: 7629, Loss:2.95 \n",
      "Iteration: 7630, Loss:2.56 \n",
      "Iteration: 7631, Loss:2.89 \n",
      "Iteration: 7632, Loss:2.66 \n",
      "Iteration: 7633, Loss:2.48 \n",
      "Iteration: 7634, Loss:2.97 \n",
      "Iteration: 7635, Loss:2.81 \n",
      "Iteration: 7636, Loss:2.44 \n",
      "Iteration: 7637, Loss:2.85 \n",
      "Iteration: 7638, Loss:2.74 \n",
      "Iteration: 7639, Loss:2.43 \n",
      "Iteration: 7640, Loss:2.20 \n",
      "Iteration: 7641, Loss:2.88 \n",
      "Iteration: 7642, Loss:2.65 \n",
      "Iteration: 7643, Loss:2.70 \n",
      "Iteration: 7644, Loss:2.44 \n",
      "Iteration: 7645, Loss:2.47 \n",
      "Iteration: 7646, Loss:2.33 \n",
      "Iteration: 7647, Loss:2.91 \n",
      "Iteration: 7648, Loss:2.20 \n",
      "Iteration: 7649, Loss:2.19 \n",
      "Iteration: 7650, Loss:2.82 \n",
      "Iteration: 7651, Loss:2.53 \n",
      "Iteration: 7652, Loss:2.37 \n",
      "Iteration: 7653, Loss:2.74 \n",
      "Iteration: 7654, Loss:2.74 \n",
      "Iteration: 7655, Loss:2.81 \n",
      "Iteration: 7656, Loss:2.85 \n",
      "Iteration: 7657, Loss:2.69 \n",
      "Iteration: 7658, Loss:2.24 \n",
      "Iteration: 7659, Loss:2.71 \n",
      "Iteration: 7660, Loss:2.68 \n",
      "Iteration: 7661, Loss:2.19 \n",
      "Iteration: 7662, Loss:2.46 \n",
      "Iteration: 7663, Loss:2.67 \n",
      "Iteration: 7664, Loss:2.45 \n",
      "Iteration: 7665, Loss:2.41 \n",
      "Iteration: 7666, Loss:2.54 \n",
      "Iteration: 7667, Loss:2.23 \n",
      "Iteration: 7668, Loss:2.52 \n",
      "Iteration: 7669, Loss:2.89 \n",
      "Iteration: 7670, Loss:2.61 \n",
      "Iteration: 7671, Loss:2.69 \n",
      "Iteration: 7672, Loss:2.40 \n",
      "Iteration: 7673, Loss:2.65 \n",
      "Iteration: 7674, Loss:2.25 \n",
      "Iteration: 7675, Loss:2.37 \n",
      "Iteration: 7676, Loss:2.65 \n",
      "Iteration: 7677, Loss:2.48 \n",
      "Iteration: 7678, Loss:2.63 \n",
      "Iteration: 7679, Loss:2.71 \n",
      "Iteration: 7680, Loss:2.68 \n",
      "Iteration: 7681, Loss:2.61 \n",
      "Iteration: 7682, Loss:2.11 \n",
      "Iteration: 7683, Loss:2.55 \n",
      "Iteration: 7684, Loss:2.88 \n",
      "Iteration: 7685, Loss:2.84 \n",
      "Iteration: 7686, Loss:2.73 \n",
      "Iteration: 7687, Loss:2.49 \n",
      "Iteration: 7688, Loss:2.73 \n",
      "Iteration: 7689, Loss:2.59 \n",
      "Iteration: 7690, Loss:2.51 \n",
      "Iteration: 7691, Loss:2.58 \n",
      "Iteration: 7692, Loss:2.66 \n",
      "Iteration: 7693, Loss:2.42 \n",
      "Iteration: 7694, Loss:2.84 \n",
      "Iteration: 7695, Loss:2.40 \n",
      "Iteration: 7696, Loss:2.67 \n",
      "Iteration: 7697, Loss:2.33 \n",
      "Iteration: 7698, Loss:2.86 \n",
      "Iteration: 7699, Loss:2.41 \n",
      "Iteration: 7700, Loss:2.10 \n",
      "Iteration: 7701, Loss:2.58 \n",
      "Iteration: 7702, Loss:2.62 \n",
      "Iteration: 7703, Loss:2.28 \n",
      "Iteration: 7704, Loss:2.55 \n",
      "Iteration: 7705, Loss:2.24 \n",
      "Iteration: 7706, Loss:2.46 \n",
      "Iteration: 7707, Loss:2.35 \n",
      "Iteration: 7708, Loss:2.67 \n",
      "Iteration: 7709, Loss:2.86 \n",
      "Iteration: 7710, Loss:2.62 \n",
      "Iteration: 7711, Loss:2.51 \n",
      "Iteration: 7712, Loss:2.55 \n",
      "Iteration: 7713, Loss:2.97 \n",
      "Iteration: 7714, Loss:2.01 \n",
      "Iteration: 7715, Loss:2.35 \n",
      "Iteration: 7716, Loss:2.47 \n",
      "Iteration: 7717, Loss:2.45 \n",
      "Iteration: 7718, Loss:2.88 \n",
      "Iteration: 7719, Loss:2.55 \n",
      "Iteration: 7720, Loss:2.84 \n",
      "Iteration: 7721, Loss:2.29 \n",
      "Iteration: 7722, Loss:2.68 \n",
      "Iteration: 7723, Loss:2.70 \n",
      "Iteration: 7724, Loss:2.69 \n",
      "Iteration: 7725, Loss:2.71 \n",
      "Iteration: 7726, Loss:2.54 \n",
      "Iteration: 7727, Loss:2.48 \n",
      "Iteration: 7728, Loss:2.43 \n",
      "Iteration: 7729, Loss:2.57 \n",
      "Iteration: 7730, Loss:2.94 \n",
      "Iteration: 7731, Loss:2.41 \n",
      "Iteration: 7732, Loss:2.81 \n",
      "Iteration: 7733, Loss:2.74 \n",
      "Iteration: 7734, Loss:2.67 \n",
      "Iteration: 7735, Loss:2.17 \n",
      "Iteration: 7736, Loss:2.74 \n",
      "Iteration: 7737, Loss:2.82 \n",
      "Iteration: 7738, Loss:2.33 \n",
      "Iteration: 7739, Loss:2.81 \n",
      "Iteration: 7740, Loss:2.60 \n",
      "Iteration: 7741, Loss:1.87 \n",
      "Iteration: 7742, Loss:2.27 \n",
      "Iteration: 7743, Loss:2.39 \n",
      "Iteration: 7744, Loss:2.50 \n",
      "Iteration: 7745, Loss:2.56 \n",
      "Iteration: 7746, Loss:2.37 \n",
      "Iteration: 7747, Loss:2.40 \n",
      "Iteration: 7748, Loss:2.62 \n",
      "Iteration: 7749, Loss:2.61 \n",
      "Iteration: 7750, Loss:2.58 \n",
      "Iteration: 7751, Loss:2.75 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_7750.ckpt\n",
      "Iteration: 7752, Loss:2.57 \n",
      "Iteration: 7753, Loss:2.47 \n",
      "Iteration: 7754, Loss:2.53 \n",
      "Iteration: 7755, Loss:2.50 \n",
      "Iteration: 7756, Loss:2.27 \n",
      "Iteration: 7757, Loss:2.05 \n",
      "Iteration: 7758, Loss:2.55 \n",
      "Iteration: 7759, Loss:2.67 \n",
      "Iteration: 7760, Loss:2.46 \n",
      "Iteration: 7761, Loss:2.63 \n",
      "Iteration: 7762, Loss:2.14 \n",
      "Iteration: 7763, Loss:2.01 \n",
      "Iteration: 7764, Loss:2.57 \n",
      "Iteration: 7765, Loss:2.55 \n",
      "Iteration: 7766, Loss:2.04 \n",
      "Iteration: 7767, Loss:2.78 \n",
      "Iteration: 7768, Loss:2.18 \n",
      "Iteration: 7769, Loss:2.72 \n",
      "Iteration: 7770, Loss:2.62 \n",
      "Iteration: 7771, Loss:2.48 \n",
      "Iteration: 7772, Loss:1.98 \n",
      "Iteration: 7773, Loss:2.88 \n",
      "Iteration: 7774, Loss:2.56 \n",
      "Iteration: 7775, Loss:2.60 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 7776, Loss:2.76 \n",
      "Iteration: 7777, Loss:2.70 \n",
      "Iteration: 7778, Loss:2.91 \n",
      "Iteration: 7779, Loss:2.65 \n",
      "Iteration: 7780, Loss:2.47 \n",
      "Iteration: 7781, Loss:2.69 \n",
      "Iteration: 7782, Loss:2.24 \n",
      "Iteration: 7783, Loss:2.53 \n",
      "Iteration: 7784, Loss:2.87 \n",
      "Iteration: 7785, Loss:2.56 \n",
      "Iteration: 7786, Loss:2.81 \n",
      "Iteration: 7787, Loss:2.34 \n",
      "Iteration: 7788, Loss:2.55 \n",
      "Iteration: 7789, Loss:2.67 \n",
      "Iteration: 7790, Loss:2.96 \n",
      "Iteration: 7791, Loss:2.32 \n",
      "Iteration: 7792, Loss:2.79 \n",
      "Iteration: 7793, Loss:2.53 \n",
      "Iteration: 7794, Loss:2.60 \n",
      "Iteration: 7795, Loss:2.48 \n",
      "Iteration: 7796, Loss:2.41 \n",
      "Iteration: 7797, Loss:3.06 \n",
      "Iteration: 7798, Loss:2.93 \n",
      "Iteration: 7799, Loss:2.66 \n",
      "Iteration: 7800, Loss:2.45 \n",
      "Iteration: 7801, Loss:2.55 \n",
      "Iteration: 7802, Loss:2.67 \n",
      "Iteration: 7803, Loss:2.94 \n",
      "Iteration: 7804, Loss:2.37 \n",
      "Iteration: 7805, Loss:2.34 \n",
      "Iteration: 7806, Loss:2.79 \n",
      "Iteration: 7807, Loss:2.90 \n",
      "Iteration: 7808, Loss:2.96 \n",
      "Iteration: 7809, Loss:2.95 \n",
      "Iteration: 7810, Loss:2.50 \n",
      "Iteration: 7811, Loss:2.65 \n",
      "Iteration: 7812, Loss:2.86 \n",
      "Iteration: 7813, Loss:2.59 \n",
      "Iteration: 7814, Loss:2.52 \n",
      "Iteration: 7815, Loss:2.53 \n",
      "Iteration: 7816, Loss:2.67 \n",
      "Iteration: 7817, Loss:2.50 \n",
      "Iteration: 7818, Loss:2.36 \n",
      "Iteration: 7819, Loss:2.50 \n",
      "Iteration: 7820, Loss:2.56 \n",
      "Iteration: 7821, Loss:2.19 \n",
      "Iteration: 7822, Loss:2.54 \n",
      "Iteration: 7823, Loss:2.36 \n",
      "Iteration: 7824, Loss:2.35 \n",
      "Iteration: 7825, Loss:2.64 \n",
      "Iteration: 7826, Loss:2.34 \n",
      "Iteration: 7827, Loss:2.38 \n",
      "Iteration: 7828, Loss:2.48 \n",
      "Iteration: 7829, Loss:2.41 \n",
      "Iteration: 7830, Loss:2.45 \n",
      "Iteration: 7831, Loss:2.45 \n",
      "Iteration: 7832, Loss:2.53 \n",
      "Iteration: 7833, Loss:2.50 \n",
      "Iteration: 7834, Loss:2.66 \n",
      "Iteration: 7835, Loss:2.63 \n",
      "Iteration: 7836, Loss:2.47 \n",
      "Iteration: 7837, Loss:2.42 \n",
      "Iteration: 7838, Loss:2.77 \n",
      "Iteration: 7839, Loss:2.55 \n",
      "Iteration: 7840, Loss:2.86 \n",
      "Iteration: 7841, Loss:2.84 \n",
      "Iteration: 7842, Loss:2.40 \n",
      "Iteration: 7843, Loss:2.67 \n",
      "Iteration: 7844, Loss:2.26 \n",
      "Iteration: 7845, Loss:2.76 \n",
      "Iteration: 7846, Loss:2.90 \n",
      "Iteration: 7847, Loss:2.80 \n",
      "Iteration: 7848, Loss:2.54 \n",
      "Iteration: 7849, Loss:2.87 \n",
      "Iteration: 7850, Loss:2.19 \n",
      "Iteration: 7851, Loss:2.99 \n",
      "Iteration: 7852, Loss:2.77 \n",
      "Iteration: 7853, Loss:2.69 \n",
      "Iteration: 7854, Loss:2.54 \n",
      "Iteration: 7855, Loss:2.35 \n",
      "Iteration: 7856, Loss:2.33 \n",
      "Iteration: 7857, Loss:2.45 \n",
      "Iteration: 7858, Loss:2.48 \n",
      "Iteration: 7859, Loss:2.21 \n",
      "Iteration: 7860, Loss:2.36 \n",
      "Iteration: 7861, Loss:2.51 \n",
      "Iteration: 7862, Loss:2.27 \n",
      "Iteration: 7863, Loss:2.46 \n",
      "Iteration: 7864, Loss:2.38 \n",
      "Iteration: 7865, Loss:2.41 \n",
      "Iteration: 7866, Loss:2.28 \n",
      "Iteration: 7867, Loss:2.22 \n",
      "Iteration: 7868, Loss:2.54 \n",
      "Iteration: 7869, Loss:2.65 \n",
      "Iteration: 7870, Loss:2.68 \n",
      "Iteration: 7871, Loss:2.92 \n",
      "Iteration: 7872, Loss:2.80 \n",
      "Iteration: 7873, Loss:2.58 \n",
      "Iteration: 7874, Loss:2.65 \n",
      "Iteration: 7875, Loss:2.53 \n",
      "Iteration: 7876, Loss:2.80 \n",
      "Iteration: 7877, Loss:2.59 \n",
      "Iteration: 7878, Loss:2.48 \n",
      "Iteration: 7879, Loss:2.32 \n",
      "Iteration: 7880, Loss:2.39 \n",
      "Iteration: 7881, Loss:2.53 \n",
      "Iteration: 7882, Loss:2.68 \n",
      "Iteration: 7883, Loss:2.42 \n",
      "Iteration: 7884, Loss:2.44 \n",
      "Iteration: 7885, Loss:2.78 \n",
      "Iteration: 7886, Loss:2.58 \n",
      "Iteration: 7887, Loss:2.11 \n",
      "Iteration: 7888, Loss:2.41 \n",
      "Iteration: 7889, Loss:2.81 \n",
      "Iteration: 7890, Loss:2.50 \n",
      "Iteration: 7891, Loss:2.49 \n",
      "Iteration: 7892, Loss:2.36 \n",
      "Iteration: 7893, Loss:2.71 \n",
      "Iteration: 7894, Loss:2.35 \n",
      "Iteration: 7895, Loss:2.37 \n",
      "Iteration: 7896, Loss:2.13 \n",
      "Iteration: 7897, Loss:2.77 \n",
      "Iteration: 7898, Loss:2.62 \n",
      "Iteration: 7899, Loss:2.24 \n",
      "Iteration: 7900, Loss:2.25 \n",
      "Iteration: 7901, Loss:2.36 \n",
      "Iteration: 7902, Loss:2.57 \n",
      "Iteration: 7903, Loss:2.58 \n",
      "Iteration: 7904, Loss:2.78 \n",
      "Iteration: 7905, Loss:2.33 \n",
      "Iteration: 7906, Loss:2.33 \n",
      "Iteration: 7907, Loss:2.54 \n",
      "Iteration: 7908, Loss:2.78 \n",
      "Iteration: 7909, Loss:2.68 \n",
      "Iteration: 7910, Loss:2.30 \n",
      "Iteration: 7911, Loss:2.54 \n",
      "Iteration: 7912, Loss:1.95 \n",
      "Iteration: 7913, Loss:2.95 \n",
      "Iteration: 7914, Loss:2.72 \n",
      "Iteration: 7915, Loss:2.35 \n",
      "Iteration: 7916, Loss:2.64 \n",
      "Iteration: 7917, Loss:2.55 \n",
      "Iteration: 7918, Loss:2.85 \n",
      "Iteration: 7919, Loss:2.52 \n",
      "Iteration: 7920, Loss:2.46 \n",
      "Iteration: 7921, Loss:2.57 \n",
      "Iteration: 7922, Loss:2.93 \n",
      "Iteration: 7923, Loss:2.79 \n",
      "Iteration: 7924, Loss:2.67 \n",
      "Iteration: 7925, Loss:2.20 \n",
      "Iteration: 7926, Loss:2.90 \n",
      "Iteration: 7927, Loss:2.33 \n",
      "Iteration: 7928, Loss:2.58 \n",
      "Iteration: 7929, Loss:2.21 \n",
      "Iteration: 7930, Loss:2.47 \n",
      "Iteration: 7931, Loss:2.21 \n",
      "Iteration: 7932, Loss:2.86 \n",
      "Iteration: 7933, Loss:2.68 \n",
      "Iteration: 7934, Loss:2.20 \n",
      "Iteration: 7935, Loss:2.27 \n",
      "Iteration: 7936, Loss:2.18 \n",
      "Iteration: 7937, Loss:2.15 \n",
      "Iteration: 7938, Loss:1.89 \n",
      "Iteration: 7939, Loss:2.42 \n",
      "Iteration: 7940, Loss:2.89 \n",
      "Iteration: 7941, Loss:2.82 \n",
      "Iteration: 7942, Loss:2.59 \n",
      "Iteration: 7943, Loss:2.82 \n",
      "Iteration: 7944, Loss:2.48 \n",
      "Iteration: 7945, Loss:2.08 \n",
      "Iteration: 7946, Loss:3.04 \n",
      "Iteration: 7947, Loss:2.64 \n",
      "Iteration: 7948, Loss:2.89 \n",
      "Iteration: 7949, Loss:2.10 \n",
      "Iteration: 7950, Loss:2.72 \n",
      "Iteration: 7951, Loss:2.65 \n",
      "Iteration: 7952, Loss:2.52 \n",
      "Iteration: 7953, Loss:2.30 \n",
      "Iteration: 7954, Loss:1.97 \n",
      "Iteration: 7955, Loss:2.10 \n",
      "Iteration: 7956, Loss:2.46 \n",
      "Iteration: 7957, Loss:2.75 \n",
      "Iteration: 7958, Loss:2.69 \n",
      "Iteration: 7959, Loss:2.31 \n",
      "Iteration: 7960, Loss:2.85 \n",
      "Iteration: 7961, Loss:2.47 \n",
      "Iteration: 7962, Loss:2.83 \n",
      "Iteration: 7963, Loss:2.50 \n",
      "Iteration: 7964, Loss:2.63 \n",
      "Iteration: 7965, Loss:2.24 \n",
      "Iteration: 7966, Loss:2.27 \n",
      "Iteration: 7967, Loss:2.61 \n",
      "Iteration: 7968, Loss:2.69 \n",
      "Iteration: 7969, Loss:2.33 \n",
      "Iteration: 7970, Loss:2.98 \n",
      "Iteration: 7971, Loss:2.36 \n",
      "Iteration: 7972, Loss:2.34 \n",
      "Iteration: 7973, Loss:2.56 \n",
      "Iteration: 7974, Loss:2.44 \n",
      "Iteration: 7975, Loss:2.80 \n",
      "Iteration: 7976, Loss:2.49 \n",
      "Iteration: 7977, Loss:2.63 \n",
      "Iteration: 7978, Loss:2.38 \n",
      "Iteration: 7979, Loss:2.07 \n",
      "Iteration: 7980, Loss:2.16 \n",
      "Iteration: 7981, Loss:2.77 \n",
      "Iteration: 7982, Loss:2.35 \n",
      "Iteration: 7983, Loss:2.62 \n",
      "Iteration: 7984, Loss:2.15 \n",
      "Iteration: 7985, Loss:2.56 \n",
      "Iteration: 7986, Loss:2.56 \n",
      "Iteration: 7987, Loss:2.58 \n",
      "Iteration: 7988, Loss:2.54 \n",
      "Iteration: 7989, Loss:2.58 \n",
      "Iteration: 7990, Loss:3.10 \n",
      "Iteration: 7991, Loss:2.72 \n",
      "Iteration: 7992, Loss:2.46 \n",
      "Iteration: 7993, Loss:2.74 \n",
      "Iteration: 7994, Loss:3.28 \n",
      "Iteration: 7995, Loss:2.65 \n",
      "Iteration: 7996, Loss:1.94 \n",
      "Iteration: 7997, Loss:2.71 \n",
      "Iteration: 7998, Loss:2.72 \n",
      "Iteration: 7999, Loss:2.97 \n",
      "Iteration: 8000, Loss:2.18 \n",
      "Iteration: 8001, Loss:2.02 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_8000.ckpt\n",
      "Iteration: 8002, Loss:2.56 \n",
      "Iteration: 8003, Loss:2.24 \n",
      "Iteration: 8004, Loss:2.61 \n",
      "Iteration: 8005, Loss:2.46 \n",
      "Iteration: 8006, Loss:2.49 \n",
      "Iteration: 8007, Loss:2.71 \n",
      "Iteration: 8008, Loss:2.28 \n",
      "Iteration: 8009, Loss:2.63 \n",
      "Iteration: 8010, Loss:2.34 \n",
      "Iteration: 8011, Loss:2.49 \n",
      "Iteration: 8012, Loss:2.78 \n",
      "Iteration: 8013, Loss:2.46 \n",
      "Iteration: 8014, Loss:2.39 \n",
      "Iteration: 8015, Loss:2.49 \n",
      "Iteration: 8016, Loss:2.58 \n",
      "Iteration: 8017, Loss:2.71 \n",
      "Iteration: 8018, Loss:2.40 \n",
      "Iteration: 8019, Loss:2.67 \n",
      "Iteration: 8020, Loss:2.37 \n",
      "Iteration: 8021, Loss:2.49 \n",
      "Iteration: 8022, Loss:2.44 \n",
      "Iteration: 8023, Loss:2.63 \n",
      "Iteration: 8024, Loss:2.44 \n",
      "Iteration: 8025, Loss:2.57 \n",
      "Iteration: 8026, Loss:2.31 \n",
      "Iteration: 8027, Loss:3.04 \n",
      "Iteration: 8028, Loss:2.74 \n",
      "Iteration: 8029, Loss:2.61 \n",
      "Iteration: 8030, Loss:2.80 \n",
      "Iteration: 8031, Loss:2.71 \n",
      "Iteration: 8032, Loss:2.32 \n",
      "Iteration: 8033, Loss:2.54 \n",
      "Iteration: 8034, Loss:2.48 \n",
      "Iteration: 8035, Loss:2.22 \n",
      "Iteration: 8036, Loss:2.49 \n",
      "Iteration: 8037, Loss:2.58 \n",
      "Iteration: 8038, Loss:2.71 \n",
      "Iteration: 8039, Loss:2.63 \n",
      "Iteration: 8040, Loss:2.53 \n",
      "Iteration: 8041, Loss:2.91 \n",
      "Iteration: 8042, Loss:2.64 \n",
      "Iteration: 8043, Loss:2.45 \n",
      "Iteration: 8044, Loss:2.64 \n",
      "Iteration: 8045, Loss:2.54 \n",
      "Iteration: 8046, Loss:2.43 \n",
      "Iteration: 8047, Loss:2.79 \n",
      "Iteration: 8048, Loss:2.47 \n",
      "Iteration: 8049, Loss:2.67 \n",
      "Iteration: 8050, Loss:2.20 \n",
      "Iteration: 8051, Loss:2.37 \n",
      "Iteration: 8052, Loss:2.84 \n",
      "Iteration: 8053, Loss:2.55 \n",
      "Iteration: 8054, Loss:2.63 \n",
      "Iteration: 8055, Loss:2.31 \n",
      "Iteration: 8056, Loss:2.64 \n",
      "Iteration: 8057, Loss:2.49 \n",
      "Iteration: 8058, Loss:2.47 \n",
      "Iteration: 8059, Loss:1.89 \n",
      "Iteration: 8060, Loss:2.22 \n",
      "Iteration: 8061, Loss:3.09 \n",
      "Iteration: 8062, Loss:2.84 \n",
      "Iteration: 8063, Loss:2.43 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8064, Loss:2.40 \n",
      "Iteration: 8065, Loss:2.40 \n",
      "Iteration: 8066, Loss:2.46 \n",
      "Iteration: 8067, Loss:2.38 \n",
      "Iteration: 8068, Loss:2.67 \n",
      "Iteration: 8069, Loss:2.38 \n",
      "Iteration: 8070, Loss:2.57 \n",
      "Iteration: 8071, Loss:2.76 \n",
      "Iteration: 8072, Loss:2.53 \n",
      "Iteration: 8073, Loss:2.90 \n",
      "Iteration: 8074, Loss:2.61 \n",
      "Iteration: 8075, Loss:2.55 \n",
      "Iteration: 8076, Loss:2.58 \n",
      "Iteration: 8077, Loss:2.68 \n",
      "Iteration: 8078, Loss:2.22 \n",
      "Iteration: 8079, Loss:2.73 \n",
      "Iteration: 8080, Loss:2.31 \n",
      "Iteration: 8081, Loss:2.51 \n",
      "Iteration: 8082, Loss:2.65 \n",
      "Iteration: 8083, Loss:2.63 \n",
      "Iteration: 8084, Loss:2.89 \n",
      "Iteration: 8085, Loss:2.91 \n",
      "Iteration: 8086, Loss:2.79 \n",
      "Iteration: 8087, Loss:2.38 \n",
      "Iteration: 8088, Loss:2.19 \n",
      "Iteration: 8089, Loss:2.54 \n",
      "Iteration: 8090, Loss:2.57 \n",
      "Iteration: 8091, Loss:2.30 \n",
      "Iteration: 8092, Loss:2.53 \n",
      "Iteration: 8093, Loss:2.79 \n",
      "Iteration: 8094, Loss:2.24 \n",
      "Iteration: 8095, Loss:2.44 \n",
      "Iteration: 8096, Loss:2.63 \n",
      "Iteration: 8097, Loss:2.88 \n",
      "Iteration: 8098, Loss:2.45 \n",
      "Iteration: 8099, Loss:2.66 \n",
      "Iteration: 8100, Loss:2.71 \n",
      "Iteration: 8101, Loss:2.48 \n",
      "Iteration: 8102, Loss:2.71 \n",
      "Iteration: 8103, Loss:2.76 \n",
      "Iteration: 8104, Loss:2.02 \n",
      "Iteration: 8105, Loss:2.78 \n",
      "Iteration: 8106, Loss:2.69 \n",
      "Iteration: 8107, Loss:2.15 \n",
      "Iteration: 8108, Loss:2.90 \n",
      "Iteration: 8109, Loss:2.37 \n",
      "Iteration: 8110, Loss:2.72 \n",
      "Iteration: 8111, Loss:2.54 \n",
      "Iteration: 8112, Loss:2.76 \n",
      "Iteration: 8113, Loss:2.60 \n",
      "Iteration: 8114, Loss:2.32 \n",
      "Iteration: 8115, Loss:2.69 \n",
      "Iteration: 8116, Loss:2.35 \n",
      "Iteration: 8117, Loss:2.76 \n",
      "Iteration: 8118, Loss:2.55 \n",
      "Iteration: 8119, Loss:2.33 \n",
      "Iteration: 8120, Loss:1.90 \n",
      "Iteration: 8121, Loss:2.52 \n",
      "Iteration: 8122, Loss:2.98 \n",
      "Iteration: 8123, Loss:2.64 \n",
      "Iteration: 8124, Loss:2.48 \n",
      "Iteration: 8125, Loss:2.77 \n",
      "Iteration: 8126, Loss:2.81 \n",
      "Iteration: 8127, Loss:3.10 \n",
      "Iteration: 8128, Loss:2.84 \n",
      "Iteration: 8129, Loss:2.58 \n",
      "Iteration: 8130, Loss:2.42 \n",
      "Iteration: 8131, Loss:2.81 \n",
      "Iteration: 8132, Loss:2.55 \n",
      "Iteration: 8133, Loss:2.50 \n",
      "Iteration: 8134, Loss:2.76 \n",
      "Iteration: 8135, Loss:2.35 \n",
      "Iteration: 8136, Loss:2.94 \n",
      "Iteration: 8137, Loss:2.41 \n",
      "Iteration: 8138, Loss:2.92 \n",
      "Iteration: 8139, Loss:2.46 \n",
      "Iteration: 8140, Loss:2.45 \n",
      "Iteration: 8141, Loss:3.07 \n",
      "Iteration: 8142, Loss:2.54 \n",
      "Iteration: 8143, Loss:2.41 \n",
      "Iteration: 8144, Loss:2.90 \n",
      "Iteration: 8145, Loss:2.40 \n",
      "Iteration: 8146, Loss:2.50 \n",
      "Iteration: 8147, Loss:2.59 \n",
      "Iteration: 8148, Loss:2.75 \n",
      "Iteration: 8149, Loss:2.63 \n",
      "Iteration: 8150, Loss:2.39 \n",
      "Iteration: 8151, Loss:2.57 \n",
      "Iteration: 8152, Loss:2.74 \n",
      "Iteration: 8153, Loss:2.64 \n",
      "Iteration: 8154, Loss:2.27 \n",
      "Iteration: 8155, Loss:2.72 \n",
      "Iteration: 8156, Loss:2.39 \n",
      "Iteration: 8157, Loss:2.14 \n",
      "Iteration: 8158, Loss:2.78 \n",
      "Iteration: 8159, Loss:2.72 \n",
      "Iteration: 8160, Loss:2.62 \n",
      "Iteration: 8161, Loss:2.70 \n",
      "Iteration: 8162, Loss:2.44 \n",
      "Iteration: 8163, Loss:2.71 \n",
      "Iteration: 8164, Loss:2.55 \n",
      "Iteration: 8165, Loss:2.43 \n",
      "Iteration: 8166, Loss:2.94 \n",
      "Iteration: 8167, Loss:2.22 \n",
      "Iteration: 8168, Loss:2.64 \n",
      "Iteration: 8169, Loss:2.58 \n",
      "Iteration: 8170, Loss:2.80 \n",
      "Iteration: 8171, Loss:2.38 \n",
      "Iteration: 8172, Loss:2.64 \n",
      "Iteration: 8173, Loss:2.38 \n",
      "Iteration: 8174, Loss:2.45 \n",
      "Iteration: 8175, Loss:2.59 \n",
      "Iteration: 8176, Loss:2.22 \n",
      "Iteration: 8177, Loss:2.33 \n",
      "Iteration: 8178, Loss:1.94 \n",
      "Iteration: 8179, Loss:2.42 \n",
      "Iteration: 8180, Loss:2.70 \n",
      "Iteration: 8181, Loss:2.64 \n",
      "Iteration: 8182, Loss:2.11 \n",
      "Iteration: 8183, Loss:2.38 \n",
      "Iteration: 8184, Loss:2.50 \n",
      "Iteration: 8185, Loss:2.68 \n",
      "Iteration: 8186, Loss:2.49 \n",
      "Iteration: 8187, Loss:2.44 \n",
      "Iteration: 8188, Loss:2.66 \n",
      "Iteration: 8189, Loss:2.43 \n",
      "Iteration: 8190, Loss:2.76 \n",
      "Iteration: 8191, Loss:2.47 \n",
      "Iteration: 8192, Loss:1.92 \n",
      "Iteration: 8193, Loss:2.38 \n",
      "Iteration: 8194, Loss:2.56 \n",
      "Iteration: 8195, Loss:2.33 \n",
      "Iteration: 8196, Loss:2.74 \n",
      "Iteration: 8197, Loss:2.46 \n",
      "Iteration: 8198, Loss:2.91 \n",
      "Iteration: 8199, Loss:2.36 \n",
      "Iteration: 8200, Loss:2.73 \n",
      "Iteration: 8201, Loss:2.66 \n",
      "Iteration: 8202, Loss:2.64 \n",
      "Iteration: 8203, Loss:2.78 \n",
      "Iteration: 8204, Loss:2.36 \n",
      "Iteration: 8205, Loss:2.29 \n",
      "Iteration: 8206, Loss:2.42 \n",
      "Iteration: 8207, Loss:2.09 \n",
      "Iteration: 8208, Loss:2.15 \n",
      "Iteration: 8209, Loss:2.73 \n",
      "Iteration: 8210, Loss:2.24 \n",
      "Iteration: 8211, Loss:2.29 \n",
      "Iteration: 8212, Loss:2.44 \n",
      "Iteration: 8213, Loss:2.33 \n",
      "Iteration: 8214, Loss:2.45 \n",
      "Iteration: 8215, Loss:2.94 \n",
      "Iteration: 8216, Loss:2.64 \n",
      "Iteration: 8217, Loss:2.37 \n",
      "Iteration: 8218, Loss:2.44 \n",
      "Iteration: 8219, Loss:2.61 \n",
      "Iteration: 8220, Loss:2.49 \n",
      "Iteration: 8221, Loss:2.86 \n",
      "Iteration: 8222, Loss:2.36 \n",
      "Iteration: 8223, Loss:2.60 \n",
      "Iteration: 8224, Loss:2.24 \n",
      "Iteration: 8225, Loss:2.21 \n",
      "Iteration: 8226, Loss:2.94 \n",
      "Iteration: 8227, Loss:2.60 \n",
      "Iteration: 8228, Loss:2.93 \n",
      "Iteration: 8229, Loss:3.15 \n",
      "Iteration: 8230, Loss:2.45 \n",
      "Iteration: 8231, Loss:2.49 \n",
      "Iteration: 8232, Loss:2.48 \n",
      "Iteration: 8233, Loss:2.17 \n",
      "Iteration: 8234, Loss:2.45 \n",
      "Iteration: 8235, Loss:2.21 \n",
      "Iteration: 8236, Loss:2.93 \n",
      "Iteration: 8237, Loss:2.52 \n",
      "Iteration: 8238, Loss:2.23 \n",
      "Iteration: 8239, Loss:2.89 \n",
      "Iteration: 8240, Loss:2.67 \n",
      "Iteration: 8241, Loss:2.90 \n",
      "Iteration: 8242, Loss:2.30 \n",
      "Iteration: 8243, Loss:2.67 \n",
      "Iteration: 8244, Loss:2.81 \n",
      "Iteration: 8245, Loss:2.65 \n",
      "Iteration: 8246, Loss:2.40 \n",
      "Iteration: 8247, Loss:2.62 \n",
      "Iteration: 8248, Loss:2.56 \n",
      "Iteration: 8249, Loss:2.70 \n",
      "Iteration: 8250, Loss:2.58 \n",
      "Iteration: 8251, Loss:2.63 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_8250.ckpt\n",
      "Iteration: 8252, Loss:2.36 \n",
      "Iteration: 8253, Loss:2.54 \n",
      "Iteration: 8254, Loss:2.53 \n",
      "Iteration: 8255, Loss:2.78 \n",
      "Iteration: 8256, Loss:2.28 \n",
      "Iteration: 8257, Loss:2.52 \n",
      "Iteration: 8258, Loss:2.76 \n",
      "Iteration: 8259, Loss:3.04 \n",
      "Iteration: 8260, Loss:2.48 \n",
      "Iteration: 8261, Loss:2.59 \n",
      "Iteration: 8262, Loss:2.51 \n",
      "Iteration: 8263, Loss:2.50 \n",
      "Iteration: 8264, Loss:2.65 \n",
      "Iteration: 8265, Loss:2.39 \n",
      "Iteration: 8266, Loss:2.09 \n",
      "Iteration: 8267, Loss:2.59 \n",
      "Iteration: 8268, Loss:2.55 \n",
      "Iteration: 8269, Loss:2.93 \n",
      "Iteration: 8270, Loss:2.57 \n",
      "Iteration: 8271, Loss:2.46 \n",
      "Iteration: 8272, Loss:2.43 \n",
      "Iteration: 8273, Loss:2.85 \n",
      "Iteration: 8274, Loss:2.69 \n",
      "Iteration: 8275, Loss:2.25 \n",
      "Iteration: 8276, Loss:2.59 \n",
      "Iteration: 8277, Loss:2.55 \n",
      "Iteration: 8278, Loss:2.86 \n",
      "Iteration: 8279, Loss:2.59 \n",
      "Iteration: 8280, Loss:2.53 \n",
      "Iteration: 8281, Loss:2.63 \n",
      "Iteration: 8282, Loss:2.36 \n",
      "Iteration: 8283, Loss:2.66 \n",
      "Iteration: 8284, Loss:2.60 \n",
      "Iteration: 8285, Loss:2.95 \n",
      "Iteration: 8286, Loss:2.54 \n",
      "Iteration: 8287, Loss:2.41 \n",
      "Iteration: 8288, Loss:2.86 \n",
      "Iteration: 8289, Loss:2.69 \n",
      "Iteration: 8290, Loss:2.90 \n",
      "Iteration: 8291, Loss:2.54 \n",
      "Iteration: 8292, Loss:1.89 \n",
      "Iteration: 8293, Loss:2.21 \n",
      "Iteration: 8294, Loss:2.85 \n",
      "Iteration: 8295, Loss:2.65 \n",
      "Iteration: 8296, Loss:2.44 \n",
      "Iteration: 8297, Loss:2.61 \n",
      "Iteration: 8298, Loss:2.64 \n",
      "Iteration: 8299, Loss:2.29 \n",
      "Iteration: 8300, Loss:2.37 \n",
      "Iteration: 8301, Loss:2.27 \n",
      "Iteration: 8302, Loss:2.42 \n",
      "Iteration: 8303, Loss:2.44 \n",
      "Iteration: 8304, Loss:2.56 \n",
      "Iteration: 8305, Loss:2.87 \n",
      "Iteration: 8306, Loss:2.66 \n",
      "Iteration: 8307, Loss:2.58 \n",
      "Iteration: 8308, Loss:2.18 \n",
      "Iteration: 8309, Loss:2.78 \n",
      "Iteration: 8310, Loss:2.28 \n",
      "Iteration: 8311, Loss:2.48 \n",
      "Iteration: 8312, Loss:2.44 \n",
      "Iteration: 8313, Loss:2.31 \n",
      "Iteration: 8314, Loss:2.75 \n",
      "Iteration: 8315, Loss:2.42 \n",
      "Iteration: 8316, Loss:2.43 \n",
      "Iteration: 8317, Loss:2.62 \n",
      "Iteration: 8318, Loss:2.62 \n",
      "Iteration: 8319, Loss:2.88 \n",
      "Iteration: 8320, Loss:2.38 \n",
      "Iteration: 8321, Loss:3.02 \n",
      "Iteration: 8322, Loss:2.36 \n",
      "Iteration: 8323, Loss:2.46 \n",
      "Iteration: 8324, Loss:2.79 \n",
      "Iteration: 8325, Loss:2.63 \n",
      "Iteration: 8326, Loss:2.16 \n",
      "Iteration: 8327, Loss:2.50 \n",
      "Iteration: 8328, Loss:2.76 \n",
      "Iteration: 8329, Loss:2.57 \n",
      "Iteration: 8330, Loss:2.57 \n",
      "Iteration: 8331, Loss:2.81 \n",
      "Iteration: 8332, Loss:2.73 \n",
      "Iteration: 8333, Loss:2.75 \n",
      "Iteration: 8334, Loss:2.15 \n",
      "Iteration: 8335, Loss:2.51 \n",
      "Iteration: 8336, Loss:2.51 \n",
      "Iteration: 8337, Loss:2.33 \n",
      "Iteration: 8338, Loss:2.62 \n",
      "Iteration: 8339, Loss:2.75 \n",
      "Iteration: 8340, Loss:2.24 \n",
      "Iteration: 8341, Loss:2.84 \n",
      "Iteration: 8342, Loss:2.14 \n",
      "Iteration: 8343, Loss:2.63 \n",
      "Iteration: 8344, Loss:2.50 \n",
      "Iteration: 8345, Loss:2.59 \n",
      "Iteration: 8346, Loss:2.41 \n",
      "Iteration: 8347, Loss:2.71 \n",
      "Iteration: 8348, Loss:2.62 \n",
      "Iteration: 8349, Loss:2.22 \n",
      "Iteration: 8350, Loss:2.64 \n",
      "Iteration: 8351, Loss:2.91 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8352, Loss:2.30 \n",
      "Iteration: 8353, Loss:2.97 \n",
      "Iteration: 8354, Loss:2.67 \n",
      "Iteration: 8355, Loss:2.59 \n",
      "Iteration: 8356, Loss:2.45 \n",
      "Iteration: 8357, Loss:2.37 \n",
      "Iteration: 8358, Loss:2.43 \n",
      "Iteration: 8359, Loss:2.80 \n",
      "Iteration: 8360, Loss:2.29 \n",
      "Iteration: 8361, Loss:2.65 \n",
      "Iteration: 8362, Loss:2.67 \n",
      "Iteration: 8363, Loss:2.38 \n",
      "Iteration: 8364, Loss:2.13 \n",
      "Iteration: 8365, Loss:2.75 \n",
      "Iteration: 8366, Loss:3.00 \n",
      "Iteration: 8367, Loss:2.79 \n",
      "Iteration: 8368, Loss:2.49 \n",
      "Iteration: 8369, Loss:2.58 \n",
      "Iteration: 8370, Loss:2.44 \n",
      "Iteration: 8371, Loss:2.30 \n",
      "Iteration: 8372, Loss:2.56 \n",
      "Iteration: 8373, Loss:2.77 \n",
      "Iteration: 8374, Loss:1.96 \n",
      "Iteration: 8375, Loss:2.86 \n",
      "Iteration: 8376, Loss:2.28 \n",
      "Iteration: 8377, Loss:2.59 \n",
      "Iteration: 8378, Loss:2.41 \n",
      "Iteration: 8379, Loss:2.24 \n",
      "Iteration: 8380, Loss:2.74 \n",
      "Iteration: 8381, Loss:2.65 \n",
      "Iteration: 8382, Loss:2.40 \n",
      "Iteration: 8383, Loss:2.54 \n",
      "Iteration: 8384, Loss:2.59 \n",
      "Iteration: 8385, Loss:2.25 \n",
      "Iteration: 8386, Loss:2.16 \n",
      "Iteration: 8387, Loss:2.51 \n",
      "Iteration: 8388, Loss:2.46 \n",
      "Iteration: 8389, Loss:2.92 \n",
      "Iteration: 8390, Loss:2.75 \n",
      "Iteration: 8391, Loss:2.51 \n",
      "Iteration: 8392, Loss:2.44 \n",
      "Iteration: 8393, Loss:2.71 \n",
      "Iteration: 8394, Loss:2.90 \n",
      "Iteration: 8395, Loss:2.53 \n",
      "Iteration: 8396, Loss:2.31 \n",
      "Iteration: 8397, Loss:2.77 \n",
      "Iteration: 8398, Loss:2.22 \n",
      "Iteration: 8399, Loss:2.25 \n",
      "Iteration: 8400, Loss:2.47 \n",
      "Iteration: 8401, Loss:2.08 \n",
      "Iteration: 8402, Loss:2.62 \n",
      "Iteration: 8403, Loss:2.75 \n",
      "Iteration: 8404, Loss:2.63 \n",
      "Iteration: 8405, Loss:2.70 \n",
      "Iteration: 8406, Loss:2.69 \n",
      "Iteration: 8407, Loss:2.61 \n",
      "Iteration: 8408, Loss:2.26 \n",
      "Iteration: 8409, Loss:2.57 \n",
      "Iteration: 8410, Loss:2.72 \n",
      "Iteration: 8411, Loss:2.55 \n",
      "Iteration: 8412, Loss:2.72 \n",
      "Iteration: 8413, Loss:2.75 \n",
      "Iteration: 8414, Loss:2.66 \n",
      "Iteration: 8415, Loss:2.44 \n",
      "Iteration: 8416, Loss:2.54 \n",
      "Iteration: 8417, Loss:2.26 \n",
      "Iteration: 8418, Loss:2.48 \n",
      "Iteration: 8419, Loss:2.56 \n",
      "Iteration: 8420, Loss:2.61 \n",
      "Iteration: 8421, Loss:2.22 \n",
      "Iteration: 8422, Loss:2.37 \n",
      "Iteration: 8423, Loss:2.43 \n",
      "Iteration: 8424, Loss:2.68 \n",
      "Iteration: 8425, Loss:2.70 \n",
      "Iteration: 8426, Loss:2.39 \n",
      "Iteration: 8427, Loss:2.82 \n",
      "Iteration: 8428, Loss:2.66 \n",
      "Iteration: 8429, Loss:2.96 \n",
      "Iteration: 8430, Loss:2.75 \n",
      "Iteration: 8431, Loss:2.44 \n",
      "Iteration: 8432, Loss:2.47 \n",
      "Iteration: 8433, Loss:2.27 \n",
      "Iteration: 8434, Loss:2.74 \n",
      "Iteration: 8435, Loss:2.65 \n",
      "Iteration: 8436, Loss:2.55 \n",
      "Iteration: 8437, Loss:2.53 \n",
      "Iteration: 8438, Loss:2.82 \n",
      "Iteration: 8439, Loss:2.47 \n",
      "Iteration: 8440, Loss:2.78 \n",
      "Iteration: 8441, Loss:2.48 \n",
      "Iteration: 8442, Loss:2.57 \n",
      "Iteration: 8443, Loss:2.53 \n",
      "Iteration: 8444, Loss:2.21 \n",
      "Iteration: 8445, Loss:2.48 \n",
      "Iteration: 8446, Loss:2.22 \n",
      "Iteration: 8447, Loss:2.67 \n",
      "Iteration: 8448, Loss:2.38 \n",
      "Iteration: 8449, Loss:2.69 \n",
      "Iteration: 8450, Loss:2.72 \n",
      "Iteration: 8451, Loss:2.56 \n",
      "Iteration: 8452, Loss:2.69 \n",
      "Iteration: 8453, Loss:2.40 \n",
      "Iteration: 8454, Loss:2.28 \n",
      "Iteration: 8455, Loss:2.89 \n",
      "Iteration: 8456, Loss:2.51 \n",
      "Iteration: 8457, Loss:2.47 \n",
      "Iteration: 8458, Loss:1.86 \n",
      "Iteration: 8459, Loss:2.58 \n",
      "Iteration: 8460, Loss:2.41 \n",
      "Iteration: 8461, Loss:2.50 \n",
      "Iteration: 8462, Loss:2.43 \n",
      "Iteration: 8463, Loss:2.71 \n",
      "Iteration: 8464, Loss:2.21 \n",
      "Iteration: 8465, Loss:1.99 \n",
      "Iteration: 8466, Loss:2.81 \n",
      "Iteration: 8467, Loss:2.69 \n",
      "Iteration: 8468, Loss:3.00 \n",
      "Iteration: 8469, Loss:2.80 \n",
      "Iteration: 8470, Loss:2.42 \n",
      "Iteration: 8471, Loss:2.62 \n",
      "Iteration: 8472, Loss:2.76 \n",
      "Iteration: 8473, Loss:2.48 \n",
      "Iteration: 8474, Loss:2.45 \n",
      "Iteration: 8475, Loss:2.66 \n",
      "Iteration: 8476, Loss:2.64 \n",
      "Iteration: 8477, Loss:2.65 \n",
      "Iteration: 8478, Loss:2.66 \n",
      "Iteration: 8479, Loss:2.35 \n",
      "Iteration: 8480, Loss:2.95 \n",
      "Iteration: 8481, Loss:2.52 \n",
      "Iteration: 8482, Loss:2.40 \n",
      "Iteration: 8483, Loss:2.04 \n",
      "Iteration: 8484, Loss:2.45 \n",
      "Iteration: 8485, Loss:2.85 \n",
      "Iteration: 8486, Loss:2.49 \n",
      "Iteration: 8487, Loss:2.46 \n",
      "Iteration: 8488, Loss:2.63 \n",
      "Iteration: 8489, Loss:2.75 \n",
      "Iteration: 8490, Loss:2.43 \n",
      "Iteration: 8491, Loss:2.84 \n",
      "Iteration: 8492, Loss:2.20 \n",
      "Iteration: 8493, Loss:2.41 \n",
      "Iteration: 8494, Loss:2.58 \n",
      "Iteration: 8495, Loss:2.42 \n",
      "Iteration: 8496, Loss:2.28 \n",
      "Iteration: 8497, Loss:2.17 \n",
      "Iteration: 8498, Loss:2.39 \n",
      "Iteration: 8499, Loss:2.47 \n",
      "Iteration: 8500, Loss:2.65 \n",
      "Iteration: 8501, Loss:2.64 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_8500.ckpt\n",
      "Iteration: 8502, Loss:2.81 \n",
      "Iteration: 8503, Loss:2.55 \n",
      "Iteration: 8504, Loss:2.78 \n",
      "Iteration: 8505, Loss:2.42 \n",
      "Iteration: 8506, Loss:2.37 \n",
      "Iteration: 8507, Loss:2.31 \n",
      "Iteration: 8508, Loss:2.42 \n",
      "Iteration: 8509, Loss:2.46 \n",
      "Iteration: 8510, Loss:2.74 \n",
      "Iteration: 8511, Loss:2.69 \n",
      "Iteration: 8512, Loss:2.24 \n",
      "Iteration: 8513, Loss:3.08 \n",
      "Iteration: 8514, Loss:2.54 \n",
      "Iteration: 8515, Loss:2.63 \n",
      "Iteration: 8516, Loss:2.48 \n",
      "Iteration: 8517, Loss:2.69 \n",
      "Iteration: 8518, Loss:2.69 \n",
      "Iteration: 8519, Loss:2.55 \n",
      "Iteration: 8520, Loss:2.55 \n",
      "Iteration: 8521, Loss:2.72 \n",
      "Iteration: 8522, Loss:2.97 \n",
      "Iteration: 8523, Loss:2.72 \n",
      "Iteration: 8524, Loss:2.85 \n",
      "Iteration: 8525, Loss:2.76 \n",
      "Iteration: 8526, Loss:2.60 \n",
      "Iteration: 8527, Loss:2.36 \n",
      "Iteration: 8528, Loss:2.83 \n",
      "Iteration: 8529, Loss:2.33 \n",
      "Iteration: 8530, Loss:2.72 \n",
      "Iteration: 8531, Loss:2.13 \n",
      "Iteration: 8532, Loss:2.46 \n",
      "Iteration: 8533, Loss:2.34 \n",
      "Iteration: 8534, Loss:2.50 \n",
      "Iteration: 8535, Loss:2.72 \n",
      "Iteration: 8536, Loss:2.24 \n",
      "Iteration: 8537, Loss:2.72 \n",
      "Iteration: 8538, Loss:2.63 \n",
      "Iteration: 8539, Loss:2.27 \n",
      "Iteration: 8540, Loss:2.66 \n",
      "Iteration: 8541, Loss:2.32 \n",
      "Iteration: 8542, Loss:2.47 \n",
      "Iteration: 8543, Loss:2.69 \n",
      "Iteration: 8544, Loss:2.87 \n",
      "Iteration: 8545, Loss:2.86 \n",
      "Iteration: 8546, Loss:2.69 \n",
      "Iteration: 8547, Loss:2.35 \n",
      "Iteration: 8548, Loss:2.37 \n",
      "Iteration: 8549, Loss:2.55 \n",
      "Iteration: 8550, Loss:2.73 \n",
      "Iteration: 8551, Loss:2.64 \n",
      "Iteration: 8552, Loss:2.35 \n",
      "Iteration: 8553, Loss:2.77 \n",
      "Iteration: 8554, Loss:2.68 \n",
      "Iteration: 8555, Loss:2.47 \n",
      "Iteration: 8556, Loss:2.46 \n",
      "Iteration: 8557, Loss:2.10 \n",
      "Iteration: 8558, Loss:2.64 \n",
      "Iteration: 8559, Loss:2.07 \n",
      "Iteration: 8560, Loss:2.69 \n",
      "Iteration: 8561, Loss:2.54 \n",
      "Iteration: 8562, Loss:2.22 \n",
      "Iteration: 8563, Loss:2.59 \n",
      "Iteration: 8564, Loss:2.73 \n",
      "Iteration: 8565, Loss:2.63 \n",
      "Iteration: 8566, Loss:2.44 \n",
      "Iteration: 8567, Loss:2.64 \n",
      "Iteration: 8568, Loss:2.81 \n",
      "Iteration: 8569, Loss:2.20 \n",
      "Iteration: 8570, Loss:2.44 \n",
      "Iteration: 8571, Loss:2.34 \n",
      "Iteration: 8572, Loss:2.32 \n",
      "Iteration: 8573, Loss:2.62 \n",
      "Iteration: 8574, Loss:2.67 \n",
      "Iteration: 8575, Loss:2.70 \n",
      "Iteration: 8576, Loss:2.98 \n",
      "Iteration: 8577, Loss:2.54 \n",
      "Iteration: 8578, Loss:2.49 \n",
      "Iteration: 8579, Loss:2.34 \n",
      "Iteration: 8580, Loss:2.68 \n",
      "Iteration: 8581, Loss:2.14 \n",
      "Iteration: 8582, Loss:2.41 \n",
      "Iteration: 8583, Loss:2.77 \n",
      "Iteration: 8584, Loss:2.40 \n",
      "Iteration: 8585, Loss:2.64 \n",
      "Iteration: 8586, Loss:2.21 \n",
      "Iteration: 8587, Loss:2.73 \n",
      "Iteration: 8588, Loss:2.38 \n",
      "Iteration: 8589, Loss:2.63 \n",
      "Iteration: 8590, Loss:2.55 \n",
      "Iteration: 8591, Loss:2.24 \n",
      "Iteration: 8592, Loss:2.54 \n",
      "Iteration: 8593, Loss:2.75 \n",
      "Iteration: 8594, Loss:2.30 \n",
      "Iteration: 8595, Loss:2.17 \n",
      "Iteration: 8596, Loss:2.47 \n",
      "Iteration: 8597, Loss:2.23 \n",
      "Iteration: 8598, Loss:2.60 \n",
      "Iteration: 8599, Loss:2.96 \n",
      "Iteration: 8600, Loss:2.43 \n",
      "Iteration: 8601, Loss:2.71 \n",
      "Iteration: 8602, Loss:2.88 \n",
      "Iteration: 8603, Loss:2.37 \n",
      "Iteration: 8604, Loss:2.44 \n",
      "Iteration: 8605, Loss:2.40 \n",
      "Iteration: 8606, Loss:2.46 \n",
      "Iteration: 8607, Loss:2.35 \n",
      "Iteration: 8608, Loss:2.75 \n",
      "Iteration: 8609, Loss:2.40 \n",
      "Iteration: 8610, Loss:2.79 \n",
      "Iteration: 8611, Loss:2.54 \n",
      "Iteration: 8612, Loss:2.55 \n",
      "Iteration: 8613, Loss:2.61 \n",
      "Iteration: 8614, Loss:2.67 \n",
      "Iteration: 8615, Loss:2.73 \n",
      "Iteration: 8616, Loss:2.09 \n",
      "Iteration: 8617, Loss:2.65 \n",
      "Iteration: 8618, Loss:2.50 \n",
      "Iteration: 8619, Loss:2.56 \n",
      "Iteration: 8620, Loss:2.47 \n",
      "Iteration: 8621, Loss:2.85 \n",
      "Iteration: 8622, Loss:2.37 \n",
      "Iteration: 8623, Loss:2.82 \n",
      "Iteration: 8624, Loss:2.99 \n",
      "Iteration: 8625, Loss:2.44 \n",
      "Iteration: 8626, Loss:2.20 \n",
      "Iteration: 8627, Loss:2.52 \n",
      "Iteration: 8628, Loss:2.58 \n",
      "Iteration: 8629, Loss:2.68 \n",
      "Iteration: 8630, Loss:2.63 \n",
      "Iteration: 8631, Loss:2.77 \n",
      "Iteration: 8632, Loss:2.62 \n",
      "Iteration: 8633, Loss:2.98 \n",
      "Iteration: 8634, Loss:2.46 \n",
      "Iteration: 8635, Loss:2.48 \n",
      "Iteration: 8636, Loss:2.76 \n",
      "Iteration: 8637, Loss:2.66 \n",
      "Iteration: 8638, Loss:2.78 \n",
      "Iteration: 8639, Loss:2.47 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8640, Loss:2.28 \n",
      "Iteration: 8641, Loss:2.53 \n",
      "Iteration: 8642, Loss:2.70 \n",
      "Iteration: 8643, Loss:2.80 \n",
      "Iteration: 8644, Loss:2.88 \n",
      "Iteration: 8645, Loss:2.95 \n",
      "Iteration: 8646, Loss:2.67 \n",
      "Iteration: 8647, Loss:2.33 \n",
      "Iteration: 8648, Loss:2.98 \n",
      "Iteration: 8649, Loss:2.88 \n",
      "Iteration: 8650, Loss:2.84 \n",
      "Iteration: 8651, Loss:3.07 \n",
      "Iteration: 8652, Loss:2.52 \n",
      "Iteration: 8653, Loss:2.74 \n",
      "Iteration: 8654, Loss:2.39 \n",
      "Iteration: 8655, Loss:2.41 \n",
      "Iteration: 8656, Loss:2.73 \n",
      "Iteration: 8657, Loss:2.84 \n",
      "Iteration: 8658, Loss:2.49 \n",
      "Iteration: 8659, Loss:2.57 \n",
      "Iteration: 8660, Loss:2.83 \n",
      "Iteration: 8661, Loss:2.62 \n",
      "Iteration: 8662, Loss:2.68 \n",
      "Iteration: 8663, Loss:2.83 \n",
      "Iteration: 8664, Loss:2.32 \n",
      "Iteration: 8665, Loss:2.63 \n",
      "Iteration: 8666, Loss:2.22 \n",
      "Iteration: 8667, Loss:2.39 \n",
      "Iteration: 8668, Loss:2.49 \n",
      "Iteration: 8669, Loss:2.75 \n",
      "Iteration: 8670, Loss:2.84 \n",
      "Iteration: 8671, Loss:2.10 \n",
      "Iteration: 8672, Loss:2.58 \n",
      "Iteration: 8673, Loss:2.47 \n",
      "Iteration: 8674, Loss:2.42 \n",
      "Iteration: 8675, Loss:2.71 \n",
      "Iteration: 8676, Loss:2.70 \n",
      "Iteration: 8677, Loss:2.75 \n",
      "Iteration: 8678, Loss:2.81 \n",
      "Iteration: 8679, Loss:2.64 \n",
      "Iteration: 8680, Loss:2.57 \n",
      "Iteration: 8681, Loss:1.92 \n",
      "Iteration: 8682, Loss:2.71 \n",
      "Iteration: 8683, Loss:2.40 \n",
      "Iteration: 8684, Loss:2.52 \n",
      "Iteration: 8685, Loss:2.44 \n",
      "Iteration: 8686, Loss:2.72 \n",
      "Iteration: 8687, Loss:2.55 \n",
      "Iteration: 8688, Loss:2.21 \n",
      "Iteration: 8689, Loss:2.56 \n",
      "Iteration: 8690, Loss:2.10 \n",
      "Iteration: 8691, Loss:2.39 \n",
      "Iteration: 8692, Loss:2.55 \n",
      "Iteration: 8693, Loss:2.79 \n",
      "Iteration: 8694, Loss:2.57 \n",
      "Iteration: 8695, Loss:2.24 \n",
      "Iteration: 8696, Loss:2.44 \n",
      "Iteration: 8697, Loss:2.83 \n",
      "Iteration: 8698, Loss:2.86 \n",
      "Iteration: 8699, Loss:2.54 \n",
      "Iteration: 8700, Loss:2.72 \n",
      "Iteration: 8701, Loss:2.62 \n",
      "Iteration: 8702, Loss:2.58 \n",
      "Iteration: 8703, Loss:2.62 \n",
      "Iteration: 8704, Loss:2.32 \n",
      "Iteration: 8705, Loss:2.64 \n",
      "Iteration: 8706, Loss:2.39 \n",
      "Iteration: 8707, Loss:2.32 \n",
      "Iteration: 8708, Loss:2.64 \n",
      "Iteration: 8709, Loss:2.65 \n",
      "Iteration: 8710, Loss:2.48 \n",
      "Iteration: 8711, Loss:2.84 \n",
      "Iteration: 8712, Loss:2.75 \n",
      "Iteration: 8713, Loss:2.47 \n",
      "Iteration: 8714, Loss:2.61 \n",
      "Iteration: 8715, Loss:2.94 \n",
      "Iteration: 8716, Loss:2.60 \n",
      "Iteration: 8717, Loss:2.57 \n",
      "Iteration: 8718, Loss:2.51 \n",
      "Iteration: 8719, Loss:2.45 \n",
      "Iteration: 8720, Loss:2.66 \n",
      "Iteration: 8721, Loss:2.41 \n",
      "Iteration: 8722, Loss:2.28 \n",
      "Iteration: 8723, Loss:2.51 \n",
      "Iteration: 8724, Loss:2.78 \n",
      "Iteration: 8725, Loss:2.41 \n",
      "Iteration: 8726, Loss:2.66 \n",
      "Iteration: 8727, Loss:2.37 \n",
      "Iteration: 8728, Loss:2.75 \n",
      "Iteration: 8729, Loss:2.65 \n",
      "Iteration: 8730, Loss:2.86 \n",
      "Iteration: 8731, Loss:2.78 \n",
      "Iteration: 8732, Loss:2.34 \n",
      "Iteration: 8733, Loss:2.46 \n",
      "Iteration: 8734, Loss:2.32 \n",
      "Iteration: 8735, Loss:2.66 \n",
      "Iteration: 8736, Loss:2.89 \n",
      "Iteration: 8737, Loss:2.45 \n",
      "Iteration: 8738, Loss:2.76 \n",
      "Iteration: 8739, Loss:2.56 \n",
      "Iteration: 8740, Loss:2.66 \n",
      "Iteration: 8741, Loss:2.44 \n",
      "Iteration: 8742, Loss:2.40 \n",
      "Iteration: 8743, Loss:2.66 \n",
      "Iteration: 8744, Loss:2.32 \n",
      "Iteration: 8745, Loss:2.45 \n",
      "Iteration: 8746, Loss:2.20 \n",
      "Iteration: 8747, Loss:2.55 \n",
      "Iteration: 8748, Loss:2.75 \n",
      "Iteration: 8749, Loss:2.64 \n",
      "Iteration: 8750, Loss:2.55 \n",
      "Iteration: 8751, Loss:2.70 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_8750.ckpt\n",
      "Iteration: 8752, Loss:2.65 \n",
      "Iteration: 8753, Loss:2.11 \n",
      "Iteration: 8754, Loss:2.34 \n",
      "Iteration: 8755, Loss:2.38 \n",
      "Iteration: 8756, Loss:2.89 \n",
      "Iteration: 8757, Loss:2.73 \n",
      "Iteration: 8758, Loss:2.89 \n",
      "Iteration: 8759, Loss:2.27 \n",
      "Iteration: 8760, Loss:2.15 \n",
      "Iteration: 8761, Loss:2.42 \n",
      "Iteration: 8762, Loss:2.24 \n",
      "Iteration: 8763, Loss:2.50 \n",
      "Iteration: 8764, Loss:1.91 \n",
      "Iteration: 8765, Loss:2.90 \n",
      "Iteration: 8766, Loss:2.34 \n",
      "Iteration: 8767, Loss:2.78 \n",
      "Iteration: 8768, Loss:2.74 \n",
      "Iteration: 8769, Loss:2.49 \n",
      "Iteration: 8770, Loss:2.79 \n",
      "Iteration: 8771, Loss:2.46 \n",
      "Iteration: 8772, Loss:2.96 \n",
      "Iteration: 8773, Loss:2.39 \n",
      "Iteration: 8774, Loss:2.12 \n",
      "Iteration: 8775, Loss:2.31 \n",
      "Iteration: 8776, Loss:2.74 \n",
      "Iteration: 8777, Loss:2.51 \n",
      "Iteration: 8778, Loss:2.09 \n",
      "Iteration: 8779, Loss:2.39 \n",
      "Iteration: 8780, Loss:2.73 \n",
      "Iteration: 8781, Loss:2.69 \n",
      "Iteration: 8782, Loss:2.55 \n",
      "Iteration: 8783, Loss:2.28 \n",
      "Iteration: 8784, Loss:2.02 \n",
      "Iteration: 8785, Loss:2.42 \n",
      "Iteration: 8786, Loss:2.47 \n",
      "Iteration: 8787, Loss:2.25 \n",
      "Iteration: 8788, Loss:2.54 \n",
      "Iteration: 8789, Loss:2.23 \n",
      "Iteration: 8790, Loss:2.25 \n",
      "Iteration: 8791, Loss:2.92 \n",
      "Iteration: 8792, Loss:2.28 \n",
      "Iteration: 8793, Loss:1.91 \n",
      "Iteration: 8794, Loss:2.27 \n",
      "Iteration: 8795, Loss:2.54 \n",
      "Iteration: 8796, Loss:2.46 \n",
      "Iteration: 8797, Loss:2.76 \n",
      "Iteration: 8798, Loss:2.26 \n",
      "Iteration: 8799, Loss:2.46 \n",
      "Iteration: 8800, Loss:2.51 \n",
      "Iteration: 8801, Loss:2.48 \n",
      "Iteration: 8802, Loss:2.35 \n",
      "Iteration: 8803, Loss:2.28 \n",
      "Iteration: 8804, Loss:2.65 \n",
      "Iteration: 8805, Loss:2.57 \n",
      "Iteration: 8806, Loss:2.40 \n",
      "Iteration: 8807, Loss:2.61 \n",
      "Iteration: 8808, Loss:1.85 \n",
      "Iteration: 8809, Loss:2.65 \n",
      "Iteration: 8810, Loss:2.29 \n",
      "Iteration: 8811, Loss:2.31 \n",
      "Iteration: 8812, Loss:2.57 \n",
      "Iteration: 8813, Loss:2.51 \n",
      "Iteration: 8814, Loss:2.46 \n",
      "Iteration: 8815, Loss:2.60 \n",
      "Iteration: 8816, Loss:2.41 \n",
      "Iteration: 8817, Loss:2.38 \n",
      "Iteration: 8818, Loss:2.75 \n",
      "Iteration: 8819, Loss:2.35 \n",
      "Iteration: 8820, Loss:2.44 \n",
      "Iteration: 8821, Loss:2.68 \n",
      "Iteration: 8822, Loss:2.98 \n",
      "Iteration: 8823, Loss:2.64 \n",
      "Iteration: 8824, Loss:2.78 \n",
      "Iteration: 8825, Loss:2.79 \n",
      "Iteration: 8826, Loss:2.38 \n",
      "Iteration: 8827, Loss:2.96 \n",
      "Iteration: 8828, Loss:2.07 \n",
      "Iteration: 8829, Loss:2.65 \n",
      "Iteration: 8830, Loss:2.72 \n",
      "Iteration: 8831, Loss:2.72 \n",
      "Iteration: 8832, Loss:2.54 \n",
      "Iteration: 8833, Loss:2.48 \n",
      "Iteration: 8834, Loss:2.23 \n",
      "Iteration: 8835, Loss:2.44 \n",
      "Iteration: 8836, Loss:2.48 \n",
      "Iteration: 8837, Loss:2.84 \n",
      "Iteration: 8838, Loss:2.21 \n",
      "Iteration: 8839, Loss:2.66 \n",
      "Iteration: 8840, Loss:2.69 \n",
      "Iteration: 8841, Loss:2.74 \n",
      "Iteration: 8842, Loss:2.71 \n",
      "Iteration: 8843, Loss:2.08 \n",
      "Iteration: 8844, Loss:2.73 \n",
      "Iteration: 8845, Loss:2.64 \n",
      "Iteration: 8846, Loss:2.25 \n",
      "Iteration: 8847, Loss:2.96 \n",
      "Iteration: 8848, Loss:2.87 \n",
      "Iteration: 8849, Loss:2.68 \n",
      "Iteration: 8850, Loss:2.67 \n",
      "Iteration: 8851, Loss:2.87 \n",
      "Iteration: 8852, Loss:2.36 \n",
      "Iteration: 8853, Loss:2.33 \n",
      "Iteration: 8854, Loss:2.19 \n",
      "Iteration: 8855, Loss:2.66 \n",
      "Iteration: 8856, Loss:3.02 \n",
      "Iteration: 8857, Loss:2.53 \n",
      "Iteration: 8858, Loss:2.15 \n",
      "Iteration: 8859, Loss:2.12 \n",
      "Iteration: 8860, Loss:2.51 \n",
      "Iteration: 8861, Loss:2.79 \n",
      "Iteration: 8862, Loss:2.56 \n",
      "Iteration: 8863, Loss:1.87 \n",
      "Iteration: 8864, Loss:2.32 \n",
      "Iteration: 8865, Loss:2.14 \n",
      "Iteration: 8866, Loss:2.75 \n",
      "Iteration: 8867, Loss:2.78 \n",
      "Iteration: 8868, Loss:2.63 \n",
      "Iteration: 8869, Loss:2.80 \n",
      "Iteration: 8870, Loss:2.50 \n",
      "Iteration: 8871, Loss:2.45 \n",
      "Iteration: 8872, Loss:2.83 \n",
      "Iteration: 8873, Loss:2.35 \n",
      "Iteration: 8874, Loss:2.69 \n",
      "Iteration: 8875, Loss:2.44 \n",
      "Iteration: 8876, Loss:2.49 \n",
      "Iteration: 8877, Loss:2.50 \n",
      "Iteration: 8878, Loss:2.38 \n",
      "Iteration: 8879, Loss:2.47 \n",
      "Iteration: 8880, Loss:2.62 \n",
      "Iteration: 8881, Loss:2.81 \n",
      "Iteration: 8882, Loss:2.33 \n",
      "Iteration: 8883, Loss:2.66 \n",
      "Iteration: 8884, Loss:2.40 \n",
      "Iteration: 8885, Loss:1.89 \n",
      "Iteration: 8886, Loss:2.82 \n",
      "Iteration: 8887, Loss:2.19 \n",
      "Iteration: 8888, Loss:2.38 \n",
      "Iteration: 8889, Loss:2.34 \n",
      "Iteration: 8890, Loss:2.44 \n",
      "Iteration: 8891, Loss:2.43 \n",
      "Iteration: 8892, Loss:2.71 \n",
      "Iteration: 8893, Loss:2.78 \n",
      "Iteration: 8894, Loss:2.66 \n",
      "Iteration: 8895, Loss:2.48 \n",
      "Iteration: 8896, Loss:2.40 \n",
      "Iteration: 8897, Loss:2.44 \n",
      "Iteration: 8898, Loss:2.53 \n",
      "Iteration: 8899, Loss:2.67 \n",
      "Iteration: 8900, Loss:2.87 \n",
      "Iteration: 8901, Loss:2.49 \n",
      "Iteration: 8902, Loss:2.62 \n",
      "Iteration: 8903, Loss:3.05 \n",
      "Iteration: 8904, Loss:2.64 \n",
      "Iteration: 8905, Loss:2.67 \n",
      "Iteration: 8906, Loss:2.53 \n",
      "Iteration: 8907, Loss:2.63 \n",
      "Iteration: 8908, Loss:2.97 \n",
      "Iteration: 8909, Loss:2.61 \n",
      "Iteration: 8910, Loss:2.87 \n",
      "Iteration: 8911, Loss:2.27 \n",
      "Iteration: 8912, Loss:2.59 \n",
      "Iteration: 8913, Loss:2.53 \n",
      "Iteration: 8914, Loss:2.57 \n",
      "Iteration: 8915, Loss:2.79 \n",
      "Iteration: 8916, Loss:2.63 \n",
      "Iteration: 8917, Loss:2.05 \n",
      "Iteration: 8918, Loss:2.70 \n",
      "Iteration: 8919, Loss:2.40 \n",
      "Iteration: 8920, Loss:2.53 \n",
      "Iteration: 8921, Loss:2.38 \n",
      "Iteration: 8922, Loss:2.69 \n",
      "Iteration: 8923, Loss:2.44 \n",
      "Iteration: 8924, Loss:2.49 \n",
      "Iteration: 8925, Loss:2.89 \n",
      "Iteration: 8926, Loss:2.62 \n",
      "Iteration: 8927, Loss:2.63 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 8928, Loss:2.52 \n",
      "Iteration: 8929, Loss:2.56 \n",
      "Iteration: 8930, Loss:3.01 \n",
      "Iteration: 8931, Loss:2.55 \n",
      "Iteration: 8932, Loss:2.84 \n",
      "Iteration: 8933, Loss:2.78 \n",
      "Iteration: 8934, Loss:2.47 \n",
      "Iteration: 8935, Loss:2.39 \n",
      "Iteration: 8936, Loss:2.51 \n",
      "Iteration: 8937, Loss:2.82 \n",
      "Iteration: 8938, Loss:2.67 \n",
      "Iteration: 8939, Loss:2.25 \n",
      "Iteration: 8940, Loss:2.46 \n",
      "Iteration: 8941, Loss:2.46 \n",
      "Iteration: 8942, Loss:2.76 \n",
      "Iteration: 8943, Loss:2.77 \n",
      "Iteration: 8944, Loss:2.74 \n",
      "Iteration: 8945, Loss:2.43 \n",
      "Iteration: 8946, Loss:2.65 \n",
      "Iteration: 8947, Loss:2.08 \n",
      "Iteration: 8948, Loss:2.71 \n",
      "Iteration: 8949, Loss:2.24 \n",
      "Iteration: 8950, Loss:2.66 \n",
      "Iteration: 8951, Loss:2.39 \n",
      "Iteration: 8952, Loss:2.33 \n",
      "Iteration: 8953, Loss:2.30 \n",
      "Iteration: 8954, Loss:2.29 \n",
      "Iteration: 8955, Loss:2.52 \n",
      "Iteration: 8956, Loss:2.70 \n",
      "Iteration: 8957, Loss:2.60 \n",
      "Iteration: 8958, Loss:2.45 \n",
      "Iteration: 8959, Loss:2.30 \n",
      "Iteration: 8960, Loss:2.45 \n",
      "Iteration: 8961, Loss:2.63 \n",
      "Iteration: 8962, Loss:2.66 \n",
      "Iteration: 8963, Loss:2.68 \n",
      "Iteration: 8964, Loss:2.85 \n",
      "Iteration: 8965, Loss:2.21 \n",
      "Iteration: 8966, Loss:2.44 \n",
      "Iteration: 8967, Loss:2.70 \n",
      "Iteration: 8968, Loss:2.33 \n",
      "Iteration: 8969, Loss:2.78 \n",
      "Iteration: 8970, Loss:2.74 \n",
      "Iteration: 8971, Loss:2.51 \n",
      "Iteration: 8972, Loss:2.76 \n",
      "Iteration: 8973, Loss:2.46 \n",
      "Iteration: 8974, Loss:2.48 \n",
      "Iteration: 8975, Loss:2.57 \n",
      "Iteration: 8976, Loss:2.40 \n",
      "Iteration: 8977, Loss:1.88 \n",
      "Iteration: 8978, Loss:2.48 \n",
      "Iteration: 8979, Loss:2.61 \n",
      "Iteration: 8980, Loss:2.69 \n",
      "Iteration: 8981, Loss:2.41 \n",
      "Iteration: 8982, Loss:2.47 \n",
      "Iteration: 8983, Loss:2.51 \n",
      "Iteration: 8984, Loss:2.65 \n",
      "Iteration: 8985, Loss:2.92 \n",
      "Iteration: 8986, Loss:2.85 \n",
      "Iteration: 8987, Loss:2.76 \n",
      "Iteration: 8988, Loss:2.72 \n",
      "Iteration: 8989, Loss:2.69 \n",
      "Iteration: 8990, Loss:2.18 \n",
      "Iteration: 8991, Loss:2.86 \n",
      "Iteration: 8992, Loss:2.63 \n",
      "Iteration: 8993, Loss:2.25 \n",
      "Iteration: 8994, Loss:2.27 \n",
      "Iteration: 8995, Loss:2.01 \n",
      "Iteration: 8996, Loss:2.76 \n",
      "Iteration: 8997, Loss:2.70 \n",
      "Iteration: 8998, Loss:2.81 \n",
      "Iteration: 8999, Loss:2.41 \n",
      "Iteration: 9000, Loss:2.74 \n",
      "Iteration: 9001, Loss:2.88 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_9000.ckpt\n",
      "Iteration: 9002, Loss:2.66 \n",
      "Iteration: 9003, Loss:2.15 \n",
      "Iteration: 9004, Loss:2.85 \n",
      "Iteration: 9005, Loss:2.92 \n",
      "Iteration: 9006, Loss:2.78 \n",
      "Iteration: 9007, Loss:2.16 \n",
      "Iteration: 9008, Loss:2.78 \n",
      "Iteration: 9009, Loss:2.30 \n",
      "Iteration: 9010, Loss:2.76 \n",
      "Iteration: 9011, Loss:2.69 \n",
      "Iteration: 9012, Loss:2.31 \n",
      "Iteration: 9013, Loss:2.51 \n",
      "Iteration: 9014, Loss:2.35 \n",
      "Iteration: 9015, Loss:2.41 \n",
      "Iteration: 9016, Loss:2.68 \n",
      "Iteration: 9017, Loss:2.63 \n",
      "Iteration: 9018, Loss:2.48 \n",
      "Iteration: 9019, Loss:2.33 \n",
      "Iteration: 9020, Loss:2.07 \n",
      "Iteration: 9021, Loss:2.87 \n",
      "Iteration: 9022, Loss:2.70 \n",
      "Iteration: 9023, Loss:2.71 \n",
      "Iteration: 9024, Loss:2.41 \n",
      "Iteration: 9025, Loss:2.78 \n",
      "Iteration: 9026, Loss:2.68 \n",
      "Iteration: 9027, Loss:2.27 \n",
      "Iteration: 9028, Loss:2.60 \n",
      "Iteration: 9029, Loss:2.56 \n",
      "Iteration: 9030, Loss:2.30 \n",
      "Iteration: 9031, Loss:2.32 \n",
      "Iteration: 9032, Loss:2.83 \n",
      "Iteration: 9033, Loss:2.33 \n",
      "Iteration: 9034, Loss:2.24 \n",
      "Iteration: 9035, Loss:2.82 \n",
      "Iteration: 9036, Loss:2.43 \n",
      "Iteration: 9037, Loss:2.49 \n",
      "Iteration: 9038, Loss:2.48 \n",
      "Iteration: 9039, Loss:2.09 \n",
      "Iteration: 9040, Loss:2.24 \n",
      "Iteration: 9041, Loss:2.78 \n",
      "Iteration: 9042, Loss:2.98 \n",
      "Iteration: 9043, Loss:2.49 \n",
      "Iteration: 9044, Loss:2.91 \n",
      "Iteration: 9045, Loss:2.06 \n",
      "Iteration: 9046, Loss:2.62 \n",
      "Iteration: 9047, Loss:2.20 \n",
      "Iteration: 9048, Loss:2.83 \n",
      "Iteration: 9049, Loss:2.26 \n",
      "Iteration: 9050, Loss:2.67 \n",
      "Iteration: 9051, Loss:2.68 \n",
      "Iteration: 9052, Loss:2.42 \n",
      "Iteration: 9053, Loss:2.76 \n",
      "Iteration: 9054, Loss:2.62 \n",
      "Iteration: 9055, Loss:2.40 \n",
      "Iteration: 9056, Loss:2.22 \n",
      "Iteration: 9057, Loss:2.50 \n",
      "Iteration: 9058, Loss:2.49 \n",
      "Iteration: 9059, Loss:2.80 \n",
      "Iteration: 9060, Loss:2.64 \n",
      "Iteration: 9061, Loss:2.09 \n",
      "Iteration: 9062, Loss:2.33 \n",
      "Iteration: 9063, Loss:2.48 \n",
      "Iteration: 9064, Loss:2.46 \n",
      "Iteration: 9065, Loss:2.06 \n",
      "Iteration: 9066, Loss:2.49 \n",
      "Iteration: 9067, Loss:2.46 \n",
      "Iteration: 9068, Loss:2.43 \n",
      "Iteration: 9069, Loss:2.92 \n",
      "Iteration: 9070, Loss:2.21 \n",
      "Iteration: 9071, Loss:3.22 \n",
      "Iteration: 9072, Loss:2.84 \n",
      "Iteration: 9073, Loss:2.76 \n",
      "Iteration: 9074, Loss:1.91 \n",
      "Iteration: 9075, Loss:2.25 \n",
      "Iteration: 9076, Loss:2.82 \n",
      "Iteration: 9077, Loss:2.56 \n",
      "Iteration: 9078, Loss:2.90 \n",
      "Iteration: 9079, Loss:2.44 \n",
      "Iteration: 9080, Loss:2.69 \n",
      "Iteration: 9081, Loss:3.05 \n",
      "Iteration: 9082, Loss:2.61 \n",
      "Iteration: 9083, Loss:2.91 \n",
      "Iteration: 9084, Loss:2.45 \n",
      "Iteration: 9085, Loss:2.94 \n",
      "Iteration: 9086, Loss:2.55 \n",
      "Iteration: 9087, Loss:2.11 \n",
      "Iteration: 9088, Loss:2.26 \n",
      "Iteration: 9089, Loss:2.68 \n",
      "Iteration: 9090, Loss:2.25 \n",
      "Iteration: 9091, Loss:2.38 \n",
      "Iteration: 9092, Loss:2.58 \n",
      "Iteration: 9093, Loss:2.61 \n",
      "Iteration: 9094, Loss:2.71 \n",
      "Iteration: 9095, Loss:2.03 \n",
      "Iteration: 9096, Loss:2.24 \n",
      "Iteration: 9097, Loss:2.47 \n",
      "Iteration: 9098, Loss:2.54 \n",
      "Iteration: 9099, Loss:2.51 \n",
      "Iteration: 9100, Loss:2.53 \n",
      "Iteration: 9101, Loss:2.45 \n",
      "Iteration: 9102, Loss:2.67 \n",
      "Iteration: 9103, Loss:2.65 \n",
      "Iteration: 9104, Loss:2.72 \n",
      "Iteration: 9105, Loss:2.43 \n",
      "Iteration: 9106, Loss:2.54 \n",
      "Iteration: 9107, Loss:2.22 \n",
      "Iteration: 9108, Loss:2.36 \n",
      "Iteration: 9109, Loss:2.31 \n",
      "Iteration: 9110, Loss:2.41 \n",
      "Iteration: 9111, Loss:2.55 \n",
      "Iteration: 9112, Loss:2.85 \n",
      "Iteration: 9113, Loss:2.42 \n",
      "Iteration: 9114, Loss:2.47 \n",
      "Iteration: 9115, Loss:2.55 \n",
      "Iteration: 9116, Loss:2.86 \n",
      "Iteration: 9117, Loss:2.78 \n",
      "Iteration: 9118, Loss:2.78 \n",
      "Iteration: 9119, Loss:3.01 \n",
      "Iteration: 9120, Loss:2.81 \n",
      "Iteration: 9121, Loss:2.72 \n",
      "Iteration: 9122, Loss:2.30 \n",
      "Iteration: 9123, Loss:2.80 \n",
      "Iteration: 9124, Loss:2.72 \n",
      "Iteration: 9125, Loss:2.20 \n",
      "Iteration: 9126, Loss:2.55 \n",
      "Iteration: 9127, Loss:2.38 \n",
      "Iteration: 9128, Loss:2.63 \n",
      "Iteration: 9129, Loss:2.40 \n",
      "Iteration: 9130, Loss:2.44 \n",
      "Iteration: 9131, Loss:2.67 \n",
      "Iteration: 9132, Loss:2.52 \n",
      "Iteration: 9133, Loss:2.82 \n",
      "Iteration: 9134, Loss:2.67 \n",
      "Iteration: 9135, Loss:2.85 \n",
      "Iteration: 9136, Loss:2.52 \n",
      "Iteration: 9137, Loss:2.21 \n",
      "Iteration: 9138, Loss:2.30 \n",
      "Iteration: 9139, Loss:2.18 \n",
      "Iteration: 9140, Loss:2.62 \n",
      "Iteration: 9141, Loss:2.34 \n",
      "Iteration: 9142, Loss:2.68 \n",
      "Iteration: 9143, Loss:2.69 \n",
      "Iteration: 9144, Loss:2.74 \n",
      "Iteration: 9145, Loss:2.52 \n",
      "Iteration: 9146, Loss:2.55 \n",
      "Iteration: 9147, Loss:2.97 \n",
      "Iteration: 9148, Loss:2.62 \n",
      "Iteration: 9149, Loss:2.62 \n",
      "Iteration: 9150, Loss:2.44 \n",
      "Iteration: 9151, Loss:2.26 \n",
      "Iteration: 9152, Loss:2.32 \n",
      "Iteration: 9153, Loss:2.65 \n",
      "Iteration: 9154, Loss:2.61 \n",
      "Iteration: 9155, Loss:2.59 \n",
      "Iteration: 9156, Loss:2.88 \n",
      "Iteration: 9157, Loss:2.87 \n",
      "Iteration: 9158, Loss:2.64 \n",
      "Iteration: 9159, Loss:2.38 \n",
      "Iteration: 9160, Loss:2.14 \n",
      "Iteration: 9161, Loss:2.70 \n",
      "Iteration: 9162, Loss:2.60 \n",
      "Iteration: 9163, Loss:2.60 \n",
      "Iteration: 9164, Loss:2.06 \n",
      "Iteration: 9165, Loss:2.48 \n",
      "Iteration: 9166, Loss:2.67 \n",
      "Iteration: 9167, Loss:2.26 \n",
      "Iteration: 9168, Loss:2.61 \n",
      "Iteration: 9169, Loss:2.11 \n",
      "Iteration: 9170, Loss:2.05 \n",
      "Iteration: 9171, Loss:2.77 \n",
      "Iteration: 9172, Loss:2.40 \n",
      "Iteration: 9173, Loss:2.36 \n",
      "Iteration: 9174, Loss:2.59 \n",
      "Iteration: 9175, Loss:2.51 \n",
      "Iteration: 9176, Loss:2.92 \n",
      "Iteration: 9177, Loss:2.62 \n",
      "Iteration: 9178, Loss:2.46 \n",
      "Iteration: 9179, Loss:2.78 \n",
      "Iteration: 9180, Loss:2.81 \n",
      "Iteration: 9181, Loss:2.94 \n",
      "Iteration: 9182, Loss:2.61 \n",
      "Iteration: 9183, Loss:2.63 \n",
      "Iteration: 9184, Loss:2.15 \n",
      "Iteration: 9185, Loss:2.64 \n",
      "Iteration: 9186, Loss:2.39 \n",
      "Iteration: 9187, Loss:2.59 \n",
      "Iteration: 9188, Loss:2.82 \n",
      "Iteration: 9189, Loss:2.28 \n",
      "Iteration: 9190, Loss:2.78 \n",
      "Iteration: 9191, Loss:3.13 \n",
      "Iteration: 9192, Loss:2.44 \n",
      "Iteration: 9193, Loss:2.61 \n",
      "Iteration: 9194, Loss:2.41 \n",
      "Iteration: 9195, Loss:2.76 \n",
      "Iteration: 9196, Loss:2.22 \n",
      "Iteration: 9197, Loss:2.72 \n",
      "Iteration: 9198, Loss:2.55 \n",
      "Iteration: 9199, Loss:2.75 \n",
      "Iteration: 9200, Loss:2.72 \n",
      "Iteration: 9201, Loss:2.83 \n",
      "Iteration: 9202, Loss:2.71 \n",
      "Iteration: 9203, Loss:2.43 \n",
      "Iteration: 9204, Loss:2.92 \n",
      "Iteration: 9205, Loss:2.53 \n",
      "Iteration: 9206, Loss:2.59 \n",
      "Iteration: 9207, Loss:2.51 \n",
      "Iteration: 9208, Loss:2.33 \n",
      "Iteration: 9209, Loss:2.54 \n",
      "Iteration: 9210, Loss:2.46 \n",
      "Iteration: 9211, Loss:2.22 \n",
      "Iteration: 9212, Loss:2.48 \n",
      "Iteration: 9213, Loss:2.79 \n",
      "Iteration: 9214, Loss:2.62 \n",
      "Iteration: 9215, Loss:2.24 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9216, Loss:2.85 \n",
      "Iteration: 9217, Loss:2.70 \n",
      "Iteration: 9218, Loss:2.74 \n",
      "Iteration: 9219, Loss:2.45 \n",
      "Iteration: 9220, Loss:2.16 \n",
      "Iteration: 9221, Loss:2.83 \n",
      "Iteration: 9222, Loss:2.04 \n",
      "Iteration: 9223, Loss:2.66 \n",
      "Iteration: 9224, Loss:2.80 \n",
      "Iteration: 9225, Loss:2.51 \n",
      "Iteration: 9226, Loss:2.61 \n",
      "Iteration: 9227, Loss:2.68 \n",
      "Iteration: 9228, Loss:2.59 \n",
      "Iteration: 9229, Loss:2.79 \n",
      "Iteration: 9230, Loss:2.49 \n",
      "Iteration: 9231, Loss:2.14 \n",
      "Iteration: 9232, Loss:2.18 \n",
      "Iteration: 9233, Loss:2.79 \n",
      "Iteration: 9234, Loss:2.65 \n",
      "Iteration: 9235, Loss:2.75 \n",
      "Iteration: 9236, Loss:2.31 \n",
      "Iteration: 9237, Loss:2.02 \n",
      "Iteration: 9238, Loss:2.38 \n",
      "Iteration: 9239, Loss:2.24 \n",
      "Iteration: 9240, Loss:2.41 \n",
      "Iteration: 9241, Loss:2.58 \n",
      "Iteration: 9242, Loss:2.78 \n",
      "Iteration: 9243, Loss:2.95 \n",
      "Iteration: 9244, Loss:2.98 \n",
      "Iteration: 9245, Loss:2.51 \n",
      "Iteration: 9246, Loss:2.47 \n",
      "Iteration: 9247, Loss:2.74 \n",
      "Iteration: 9248, Loss:2.37 \n",
      "Iteration: 9249, Loss:2.45 \n",
      "Iteration: 9250, Loss:2.80 \n",
      "Iteration: 9251, Loss:2.37 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_9250.ckpt\n",
      "Iteration: 9252, Loss:2.43 \n",
      "Iteration: 9253, Loss:2.73 \n",
      "Iteration: 9254, Loss:2.31 \n",
      "Iteration: 9255, Loss:2.59 \n",
      "Iteration: 9256, Loss:2.30 \n",
      "Iteration: 9257, Loss:2.89 \n",
      "Iteration: 9258, Loss:2.56 \n",
      "Iteration: 9259, Loss:2.24 \n",
      "Iteration: 9260, Loss:2.61 \n",
      "Iteration: 9261, Loss:2.59 \n",
      "Iteration: 9262, Loss:2.04 \n",
      "Iteration: 9263, Loss:2.24 \n",
      "Iteration: 9264, Loss:2.59 \n",
      "Iteration: 9265, Loss:2.63 \n",
      "Iteration: 9266, Loss:2.49 \n",
      "Iteration: 9267, Loss:2.63 \n",
      "Iteration: 9268, Loss:2.40 \n",
      "Iteration: 9269, Loss:2.38 \n",
      "Iteration: 9270, Loss:2.84 \n",
      "Iteration: 9271, Loss:2.41 \n",
      "Iteration: 9272, Loss:2.73 \n",
      "Iteration: 9273, Loss:2.72 \n",
      "Iteration: 9274, Loss:2.92 \n",
      "Iteration: 9275, Loss:2.01 \n",
      "Iteration: 9276, Loss:2.43 \n",
      "Iteration: 9277, Loss:2.56 \n",
      "Iteration: 9278, Loss:2.36 \n",
      "Iteration: 9279, Loss:2.69 \n",
      "Iteration: 9280, Loss:2.49 \n",
      "Iteration: 9281, Loss:2.22 \n",
      "Iteration: 9282, Loss:2.66 \n",
      "Iteration: 9283, Loss:2.52 \n",
      "Iteration: 9284, Loss:2.39 \n",
      "Iteration: 9285, Loss:2.60 \n",
      "Iteration: 9286, Loss:2.06 \n",
      "Iteration: 9287, Loss:2.62 \n",
      "Iteration: 9288, Loss:2.57 \n",
      "Iteration: 9289, Loss:2.75 \n",
      "Iteration: 9290, Loss:2.81 \n",
      "Iteration: 9291, Loss:2.36 \n",
      "Iteration: 9292, Loss:2.17 \n",
      "Iteration: 9293, Loss:2.82 \n",
      "Iteration: 9294, Loss:2.51 \n",
      "Iteration: 9295, Loss:2.32 \n",
      "Iteration: 9296, Loss:2.75 \n",
      "Iteration: 9297, Loss:2.06 \n",
      "Iteration: 9298, Loss:2.73 \n",
      "Iteration: 9299, Loss:2.73 \n",
      "Iteration: 9300, Loss:2.76 \n",
      "Iteration: 9301, Loss:2.35 \n",
      "Iteration: 9302, Loss:2.08 \n",
      "Iteration: 9303, Loss:2.75 \n",
      "Iteration: 9304, Loss:2.43 \n",
      "Iteration: 9305, Loss:2.39 \n",
      "Iteration: 9306, Loss:2.82 \n",
      "Iteration: 9307, Loss:2.65 \n",
      "Iteration: 9308, Loss:2.69 \n",
      "Iteration: 9309, Loss:2.79 \n",
      "Iteration: 9310, Loss:2.85 \n",
      "Iteration: 9311, Loss:2.89 \n",
      "Iteration: 9312, Loss:2.73 \n",
      "Iteration: 9313, Loss:2.43 \n",
      "Iteration: 9314, Loss:2.37 \n",
      "Iteration: 9315, Loss:2.38 \n",
      "Iteration: 9316, Loss:2.61 \n",
      "Iteration: 9317, Loss:2.19 \n",
      "Iteration: 9318, Loss:2.74 \n",
      "Iteration: 9319, Loss:2.66 \n",
      "Iteration: 9320, Loss:2.35 \n",
      "Iteration: 9321, Loss:3.01 \n",
      "Iteration: 9322, Loss:2.62 \n",
      "Iteration: 9323, Loss:2.21 \n",
      "Iteration: 9324, Loss:2.50 \n",
      "Iteration: 9325, Loss:2.71 \n",
      "Iteration: 9326, Loss:2.44 \n",
      "Iteration: 9327, Loss:2.57 \n",
      "Iteration: 9328, Loss:2.64 \n",
      "Iteration: 9329, Loss:2.65 \n",
      "Iteration: 9330, Loss:2.81 \n",
      "Iteration: 9331, Loss:2.60 \n",
      "Iteration: 9332, Loss:2.26 \n",
      "Iteration: 9333, Loss:2.36 \n",
      "Iteration: 9334, Loss:2.77 \n",
      "Iteration: 9335, Loss:2.62 \n",
      "Iteration: 9336, Loss:2.02 \n",
      "Iteration: 9337, Loss:2.62 \n",
      "Iteration: 9338, Loss:2.69 \n",
      "Iteration: 9339, Loss:2.58 \n",
      "Iteration: 9340, Loss:2.76 \n",
      "Iteration: 9341, Loss:2.71 \n",
      "Iteration: 9342, Loss:2.51 \n",
      "Iteration: 9343, Loss:2.73 \n",
      "Iteration: 9344, Loss:2.54 \n",
      "Iteration: 9345, Loss:2.31 \n",
      "Iteration: 9346, Loss:2.28 \n",
      "Iteration: 9347, Loss:2.67 \n",
      "Iteration: 9348, Loss:2.47 \n",
      "Iteration: 9349, Loss:2.08 \n",
      "Iteration: 9350, Loss:2.56 \n",
      "Iteration: 9351, Loss:2.22 \n",
      "Iteration: 9352, Loss:2.70 \n",
      "Iteration: 9353, Loss:2.67 \n",
      "Iteration: 9354, Loss:2.55 \n",
      "Iteration: 9355, Loss:2.78 \n",
      "Iteration: 9356, Loss:2.23 \n",
      "Iteration: 9357, Loss:2.77 \n",
      "Iteration: 9358, Loss:2.58 \n",
      "Iteration: 9359, Loss:2.55 \n",
      "Iteration: 9360, Loss:2.57 \n",
      "Iteration: 9361, Loss:1.87 \n",
      "Iteration: 9362, Loss:2.72 \n",
      "Iteration: 9363, Loss:2.39 \n",
      "Iteration: 9364, Loss:2.84 \n",
      "Iteration: 9365, Loss:2.17 \n",
      "Iteration: 9366, Loss:2.71 \n",
      "Iteration: 9367, Loss:2.70 \n",
      "Iteration: 9368, Loss:2.07 \n",
      "Iteration: 9369, Loss:2.70 \n",
      "Iteration: 9370, Loss:2.09 \n",
      "Iteration: 9371, Loss:2.85 \n",
      "Iteration: 9372, Loss:2.40 \n",
      "Iteration: 9373, Loss:2.41 \n",
      "Iteration: 9374, Loss:2.96 \n",
      "Iteration: 9375, Loss:2.63 \n",
      "Iteration: 9376, Loss:2.52 \n",
      "Iteration: 9377, Loss:2.08 \n",
      "Iteration: 9378, Loss:2.44 \n",
      "Iteration: 9379, Loss:2.77 \n",
      "Iteration: 9380, Loss:2.75 \n",
      "Iteration: 9381, Loss:2.68 \n",
      "Iteration: 9382, Loss:2.44 \n",
      "Iteration: 9383, Loss:2.60 \n",
      "Iteration: 9384, Loss:2.63 \n",
      "Iteration: 9385, Loss:2.45 \n",
      "Iteration: 9386, Loss:2.41 \n",
      "Iteration: 9387, Loss:2.37 \n",
      "Iteration: 9388, Loss:2.70 \n",
      "Iteration: 9389, Loss:2.49 \n",
      "Iteration: 9390, Loss:2.86 \n",
      "Iteration: 9391, Loss:2.78 \n",
      "Iteration: 9392, Loss:2.77 \n",
      "Iteration: 9393, Loss:2.43 \n",
      "Iteration: 9394, Loss:2.53 \n",
      "Iteration: 9395, Loss:2.37 \n",
      "Iteration: 9396, Loss:2.59 \n",
      "Iteration: 9397, Loss:2.06 \n",
      "Iteration: 9398, Loss:2.42 \n",
      "Iteration: 9399, Loss:2.41 \n",
      "Iteration: 9400, Loss:2.24 \n",
      "Iteration: 9401, Loss:2.91 \n",
      "Iteration: 9402, Loss:2.75 \n",
      "Iteration: 9403, Loss:2.34 \n",
      "Iteration: 9404, Loss:2.35 \n",
      "Iteration: 9405, Loss:3.13 \n",
      "Iteration: 9406, Loss:2.71 \n",
      "Iteration: 9407, Loss:2.37 \n",
      "Iteration: 9408, Loss:2.55 \n",
      "Iteration: 9409, Loss:2.79 \n",
      "Iteration: 9410, Loss:2.85 \n",
      "Iteration: 9411, Loss:2.89 \n",
      "Iteration: 9412, Loss:2.38 \n",
      "Iteration: 9413, Loss:2.56 \n",
      "Iteration: 9414, Loss:2.65 \n",
      "Iteration: 9415, Loss:2.21 \n",
      "Iteration: 9416, Loss:2.32 \n",
      "Iteration: 9417, Loss:2.70 \n",
      "Iteration: 9418, Loss:2.44 \n",
      "Iteration: 9419, Loss:2.46 \n",
      "Iteration: 9420, Loss:2.46 \n",
      "Iteration: 9421, Loss:2.30 \n",
      "Iteration: 9422, Loss:2.53 \n",
      "Iteration: 9423, Loss:2.58 \n",
      "Iteration: 9424, Loss:2.28 \n",
      "Iteration: 9425, Loss:2.53 \n",
      "Iteration: 9426, Loss:2.17 \n",
      "Iteration: 9427, Loss:2.42 \n",
      "Iteration: 9428, Loss:2.57 \n",
      "Iteration: 9429, Loss:2.80 \n",
      "Iteration: 9430, Loss:2.47 \n",
      "Iteration: 9431, Loss:2.74 \n",
      "Iteration: 9432, Loss:2.50 \n",
      "Iteration: 9433, Loss:2.53 \n",
      "Iteration: 9434, Loss:2.63 \n",
      "Iteration: 9435, Loss:2.20 \n",
      "Iteration: 9436, Loss:3.16 \n",
      "Iteration: 9437, Loss:2.30 \n",
      "Iteration: 9438, Loss:2.99 \n",
      "Iteration: 9439, Loss:2.60 \n",
      "Iteration: 9440, Loss:2.82 \n",
      "Iteration: 9441, Loss:2.74 \n",
      "Iteration: 9442, Loss:2.37 \n",
      "Iteration: 9443, Loss:2.55 \n",
      "Iteration: 9444, Loss:2.59 \n",
      "Iteration: 9445, Loss:2.23 \n",
      "Iteration: 9446, Loss:2.59 \n",
      "Iteration: 9447, Loss:2.82 \n",
      "Iteration: 9448, Loss:2.44 \n",
      "Iteration: 9449, Loss:2.71 \n",
      "Iteration: 9450, Loss:2.47 \n",
      "Iteration: 9451, Loss:2.67 \n",
      "Iteration: 9452, Loss:2.59 \n",
      "Iteration: 9453, Loss:2.45 \n",
      "Iteration: 9454, Loss:2.37 \n",
      "Iteration: 9455, Loss:2.76 \n",
      "Iteration: 9456, Loss:2.93 \n",
      "Iteration: 9457, Loss:2.63 \n",
      "Iteration: 9458, Loss:2.68 \n",
      "Iteration: 9459, Loss:2.01 \n",
      "Iteration: 9460, Loss:2.55 \n",
      "Iteration: 9461, Loss:2.24 \n",
      "Iteration: 9462, Loss:2.49 \n",
      "Iteration: 9463, Loss:2.62 \n",
      "Iteration: 9464, Loss:2.86 \n",
      "Iteration: 9465, Loss:2.09 \n",
      "Iteration: 9466, Loss:2.77 \n",
      "Iteration: 9467, Loss:2.26 \n",
      "Iteration: 9468, Loss:2.59 \n",
      "Iteration: 9469, Loss:2.54 \n",
      "Iteration: 9470, Loss:2.22 \n",
      "Iteration: 9471, Loss:2.43 \n",
      "Iteration: 9472, Loss:2.44 \n",
      "Iteration: 9473, Loss:2.71 \n",
      "Iteration: 9474, Loss:2.20 \n",
      "Iteration: 9475, Loss:2.60 \n",
      "Iteration: 9476, Loss:2.78 \n",
      "Iteration: 9477, Loss:2.88 \n",
      "Iteration: 9478, Loss:2.78 \n",
      "Iteration: 9479, Loss:2.47 \n",
      "Iteration: 9480, Loss:2.47 \n",
      "Iteration: 9481, Loss:2.82 \n",
      "Iteration: 9482, Loss:2.27 \n",
      "Iteration: 9483, Loss:2.51 \n",
      "Iteration: 9484, Loss:2.61 \n",
      "Iteration: 9485, Loss:2.64 \n",
      "Iteration: 9486, Loss:2.51 \n",
      "Iteration: 9487, Loss:2.36 \n",
      "Iteration: 9488, Loss:2.08 \n",
      "Iteration: 9489, Loss:2.67 \n",
      "Iteration: 9490, Loss:2.63 \n",
      "Iteration: 9491, Loss:2.18 \n",
      "Iteration: 9492, Loss:2.48 \n",
      "Iteration: 9493, Loss:2.79 \n",
      "Iteration: 9494, Loss:2.14 \n",
      "Iteration: 9495, Loss:2.20 \n",
      "Iteration: 9496, Loss:2.53 \n",
      "Iteration: 9497, Loss:2.43 \n",
      "Iteration: 9498, Loss:2.28 \n",
      "Iteration: 9499, Loss:2.43 \n",
      "Iteration: 9500, Loss:2.74 \n",
      "Iteration: 9501, Loss:2.20 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_9500.ckpt\n",
      "Iteration: 9502, Loss:2.35 \n",
      "Iteration: 9503, Loss:2.26 \n",
      "Iteration: 9504, Loss:2.40 \n",
      "Iteration: 9505, Loss:2.46 \n",
      "Iteration: 9506, Loss:2.61 \n",
      "Iteration: 9507, Loss:2.62 \n",
      "Iteration: 9508, Loss:2.70 \n",
      "Iteration: 9509, Loss:2.94 \n",
      "Iteration: 9510, Loss:2.61 \n",
      "Iteration: 9511, Loss:2.74 \n",
      "Iteration: 9512, Loss:2.62 \n",
      "Iteration: 9513, Loss:2.66 \n",
      "Iteration: 9514, Loss:2.28 \n",
      "Iteration: 9515, Loss:2.46 \n",
      "Iteration: 9516, Loss:2.62 \n",
      "Iteration: 9517, Loss:2.71 \n",
      "Iteration: 9518, Loss:2.16 \n",
      "Iteration: 9519, Loss:2.56 \n",
      "Iteration: 9520, Loss:2.31 \n",
      "Iteration: 9521, Loss:2.41 \n",
      "Iteration: 9522, Loss:2.76 \n",
      "Iteration: 9523, Loss:2.77 \n",
      "Iteration: 9524, Loss:2.38 \n",
      "Iteration: 9525, Loss:2.66 \n",
      "Iteration: 9526, Loss:2.77 \n",
      "Iteration: 9527, Loss:2.85 \n",
      "Iteration: 9528, Loss:2.61 \n",
      "Iteration: 9529, Loss:2.90 \n",
      "Iteration: 9530, Loss:2.73 \n",
      "Iteration: 9531, Loss:2.33 \n",
      "Iteration: 9532, Loss:2.40 \n",
      "Iteration: 9533, Loss:2.78 \n",
      "Iteration: 9534, Loss:2.64 \n",
      "Iteration: 9535, Loss:2.58 \n",
      "Iteration: 9536, Loss:2.89 \n",
      "Iteration: 9537, Loss:2.55 \n",
      "Iteration: 9538, Loss:2.30 \n",
      "Iteration: 9539, Loss:2.65 \n",
      "Iteration: 9540, Loss:2.76 \n",
      "Iteration: 9541, Loss:2.34 \n",
      "Iteration: 9542, Loss:2.94 \n",
      "Iteration: 9543, Loss:2.30 \n",
      "Iteration: 9544, Loss:2.85 \n",
      "Iteration: 9545, Loss:2.00 \n",
      "Iteration: 9546, Loss:2.68 \n",
      "Iteration: 9547, Loss:2.77 \n",
      "Iteration: 9548, Loss:2.78 \n",
      "Iteration: 9549, Loss:2.53 \n",
      "Iteration: 9550, Loss:2.24 \n",
      "Iteration: 9551, Loss:2.26 \n",
      "Iteration: 9552, Loss:2.64 \n",
      "Iteration: 9553, Loss:2.55 \n",
      "Iteration: 9554, Loss:2.79 \n",
      "Iteration: 9555, Loss:1.87 \n",
      "Iteration: 9556, Loss:2.67 \n",
      "Iteration: 9557, Loss:2.73 \n",
      "Iteration: 9558, Loss:2.52 \n",
      "Iteration: 9559, Loss:2.44 \n",
      "Iteration: 9560, Loss:2.73 \n",
      "Iteration: 9561, Loss:2.75 \n",
      "Iteration: 9562, Loss:2.70 \n",
      "Iteration: 9563, Loss:2.79 \n",
      "Iteration: 9564, Loss:2.29 \n",
      "Iteration: 9565, Loss:2.65 \n",
      "Iteration: 9566, Loss:2.84 \n",
      "Iteration: 9567, Loss:2.22 \n",
      "Iteration: 9568, Loss:2.22 \n",
      "Iteration: 9569, Loss:2.71 \n",
      "Iteration: 9570, Loss:2.42 \n",
      "Iteration: 9571, Loss:2.43 \n",
      "Iteration: 9572, Loss:2.58 \n",
      "Iteration: 9573, Loss:2.41 \n",
      "Iteration: 9574, Loss:2.67 \n",
      "Iteration: 9575, Loss:2.57 \n",
      "Iteration: 9576, Loss:2.81 \n",
      "Iteration: 9577, Loss:2.89 \n",
      "Iteration: 9578, Loss:2.62 \n",
      "Iteration: 9579, Loss:2.31 \n",
      "Iteration: 9580, Loss:2.39 \n",
      "Iteration: 9581, Loss:2.50 \n",
      "Iteration: 9582, Loss:2.76 \n",
      "Iteration: 9583, Loss:2.61 \n",
      "Iteration: 9584, Loss:2.54 \n",
      "Iteration: 9585, Loss:2.29 \n",
      "Iteration: 9586, Loss:2.92 \n",
      "Iteration: 9587, Loss:2.85 \n",
      "Iteration: 9588, Loss:2.66 \n",
      "Iteration: 9589, Loss:2.69 \n",
      "Iteration: 9590, Loss:2.42 \n",
      "Iteration: 9591, Loss:2.58 \n",
      "Iteration: 9592, Loss:2.82 \n",
      "Iteration: 9593, Loss:2.41 \n",
      "Iteration: 9594, Loss:2.55 \n",
      "Iteration: 9595, Loss:2.82 \n",
      "Iteration: 9596, Loss:2.00 \n",
      "Iteration: 9597, Loss:2.28 \n",
      "Iteration: 9598, Loss:2.70 \n",
      "Iteration: 9599, Loss:2.28 \n",
      "Iteration: 9600, Loss:2.64 \n",
      "Iteration: 9601, Loss:2.90 \n",
      "Iteration: 9602, Loss:2.59 \n",
      "Iteration: 9603, Loss:2.80 \n",
      "Iteration: 9604, Loss:2.83 \n",
      "Iteration: 9605, Loss:2.61 \n",
      "Iteration: 9606, Loss:2.20 \n",
      "Iteration: 9607, Loss:2.58 \n",
      "Iteration: 9608, Loss:2.94 \n",
      "Iteration: 9609, Loss:2.42 \n",
      "Iteration: 9610, Loss:2.24 \n",
      "Iteration: 9611, Loss:2.42 \n",
      "Iteration: 9612, Loss:2.96 \n",
      "Iteration: 9613, Loss:2.51 \n",
      "Iteration: 9614, Loss:1.96 \n",
      "Iteration: 9615, Loss:2.30 \n",
      "Iteration: 9616, Loss:2.71 \n",
      "Iteration: 9617, Loss:2.59 \n",
      "Iteration: 9618, Loss:2.70 \n",
      "Iteration: 9619, Loss:2.66 \n",
      "Iteration: 9620, Loss:2.71 \n",
      "Iteration: 9621, Loss:2.52 \n",
      "Iteration: 9622, Loss:2.18 \n",
      "Iteration: 9623, Loss:2.55 \n",
      "Iteration: 9624, Loss:2.44 \n",
      "Iteration: 9625, Loss:2.71 \n",
      "Iteration: 9626, Loss:2.72 \n",
      "Iteration: 9627, Loss:2.68 \n",
      "Iteration: 9628, Loss:2.68 \n",
      "Iteration: 9629, Loss:2.92 \n",
      "Iteration: 9630, Loss:2.63 \n",
      "Iteration: 9631, Loss:2.64 \n",
      "Iteration: 9632, Loss:2.67 \n",
      "Iteration: 9633, Loss:2.30 \n",
      "Iteration: 9634, Loss:2.33 \n",
      "Iteration: 9635, Loss:2.53 \n",
      "Iteration: 9636, Loss:2.87 \n",
      "Iteration: 9637, Loss:2.76 \n",
      "Iteration: 9638, Loss:2.48 \n",
      "Iteration: 9639, Loss:2.52 \n",
      "Iteration: 9640, Loss:2.32 \n",
      "Iteration: 9641, Loss:2.05 \n",
      "Iteration: 9642, Loss:2.50 \n",
      "Iteration: 9643, Loss:2.48 \n",
      "Iteration: 9644, Loss:2.45 \n",
      "Iteration: 9645, Loss:2.59 \n",
      "Iteration: 9646, Loss:2.56 \n",
      "Iteration: 9647, Loss:2.65 \n",
      "Iteration: 9648, Loss:2.36 \n",
      "Iteration: 9649, Loss:2.78 \n",
      "Iteration: 9650, Loss:2.64 \n",
      "Iteration: 9651, Loss:2.49 \n",
      "Iteration: 9652, Loss:2.43 \n",
      "Iteration: 9653, Loss:2.46 \n",
      "Iteration: 9654, Loss:2.62 \n",
      "Iteration: 9655, Loss:2.52 \n",
      "Iteration: 9656, Loss:2.46 \n",
      "Iteration: 9657, Loss:2.86 \n",
      "Iteration: 9658, Loss:2.59 \n",
      "Iteration: 9659, Loss:2.41 \n",
      "Iteration: 9660, Loss:2.78 \n",
      "Iteration: 9661, Loss:2.67 \n",
      "Iteration: 9662, Loss:2.08 \n",
      "Iteration: 9663, Loss:2.38 \n",
      "Iteration: 9664, Loss:2.58 \n",
      "Iteration: 9665, Loss:1.91 \n",
      "Iteration: 9666, Loss:2.65 \n",
      "Iteration: 9667, Loss:2.51 \n",
      "Iteration: 9668, Loss:2.72 \n",
      "Iteration: 9669, Loss:2.73 \n",
      "Iteration: 9670, Loss:2.65 \n",
      "Iteration: 9671, Loss:2.48 \n",
      "Iteration: 9672, Loss:2.49 \n",
      "Iteration: 9673, Loss:2.82 \n",
      "Iteration: 9674, Loss:2.67 \n",
      "Iteration: 9675, Loss:2.77 \n",
      "Iteration: 9676, Loss:2.21 \n",
      "Iteration: 9677, Loss:2.59 \n",
      "Iteration: 9678, Loss:2.27 \n",
      "Iteration: 9679, Loss:2.52 \n",
      "Iteration: 9680, Loss:2.22 \n",
      "Iteration: 9681, Loss:2.44 \n",
      "Iteration: 9682, Loss:2.88 \n",
      "Iteration: 9683, Loss:2.41 \n",
      "Iteration: 9684, Loss:2.50 \n",
      "Iteration: 9685, Loss:2.37 \n",
      "Iteration: 9686, Loss:2.30 \n",
      "Iteration: 9687, Loss:2.65 \n",
      "Iteration: 9688, Loss:2.89 \n",
      "Iteration: 9689, Loss:2.75 \n",
      "Iteration: 9690, Loss:2.66 \n",
      "Iteration: 9691, Loss:1.95 \n",
      "Iteration: 9692, Loss:2.96 \n",
      "Iteration: 9693, Loss:2.46 \n",
      "Iteration: 9694, Loss:2.38 \n",
      "Iteration: 9695, Loss:2.50 \n",
      "Iteration: 9696, Loss:2.65 \n",
      "Iteration: 9697, Loss:2.30 \n",
      "Iteration: 9698, Loss:2.89 \n",
      "Iteration: 9699, Loss:2.82 \n",
      "Iteration: 9700, Loss:2.02 \n",
      "Iteration: 9701, Loss:3.02 \n",
      "Iteration: 9702, Loss:2.72 \n",
      "Iteration: 9703, Loss:2.47 \n",
      "Iteration: 9704, Loss:2.35 \n",
      "Iteration: 9705, Loss:2.57 \n",
      "Iteration: 9706, Loss:2.32 \n",
      "Iteration: 9707, Loss:2.62 \n",
      "Iteration: 9708, Loss:2.64 \n",
      "Iteration: 9709, Loss:2.27 \n",
      "Iteration: 9710, Loss:2.73 \n",
      "Iteration: 9711, Loss:2.51 \n",
      "Iteration: 9712, Loss:2.69 \n",
      "Iteration: 9713, Loss:2.81 \n",
      "Iteration: 9714, Loss:2.95 \n",
      "Iteration: 9715, Loss:2.53 \n",
      "Iteration: 9716, Loss:2.64 \n",
      "Iteration: 9717, Loss:2.23 \n",
      "Iteration: 9718, Loss:2.72 \n",
      "Iteration: 9719, Loss:2.29 \n",
      "Iteration: 9720, Loss:2.85 \n",
      "Iteration: 9721, Loss:2.25 \n",
      "Iteration: 9722, Loss:2.66 \n",
      "Iteration: 9723, Loss:2.45 \n",
      "Iteration: 9724, Loss:2.79 \n",
      "Iteration: 9725, Loss:2.76 \n",
      "Iteration: 9726, Loss:2.80 \n",
      "Iteration: 9727, Loss:2.61 \n",
      "Iteration: 9728, Loss:2.47 \n",
      "Iteration: 9729, Loss:2.34 \n",
      "Iteration: 9730, Loss:2.47 \n",
      "Iteration: 9731, Loss:2.19 \n",
      "Iteration: 9732, Loss:2.69 \n",
      "Iteration: 9733, Loss:2.75 \n",
      "Iteration: 9734, Loss:2.46 \n",
      "Iteration: 9735, Loss:2.28 \n",
      "Iteration: 9736, Loss:2.45 \n",
      "Iteration: 9737, Loss:2.37 \n",
      "Iteration: 9738, Loss:2.84 \n",
      "Iteration: 9739, Loss:2.69 \n",
      "Iteration: 9740, Loss:2.20 \n",
      "Iteration: 9741, Loss:2.52 \n",
      "Iteration: 9742, Loss:2.49 \n",
      "Iteration: 9743, Loss:2.21 \n",
      "Iteration: 9744, Loss:2.51 \n",
      "Iteration: 9745, Loss:2.61 \n",
      "Iteration: 9746, Loss:2.63 \n",
      "Iteration: 9747, Loss:2.81 \n",
      "Iteration: 9748, Loss:2.73 \n",
      "Iteration: 9749, Loss:2.69 \n",
      "Iteration: 9750, Loss:2.07 \n",
      "Iteration: 9751, Loss:2.36 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_9750.ckpt\n",
      "Iteration: 9752, Loss:2.08 \n",
      "Iteration: 9753, Loss:2.71 \n",
      "Iteration: 9754, Loss:2.80 \n",
      "Iteration: 9755, Loss:2.48 \n",
      "Iteration: 9756, Loss:2.82 \n",
      "Iteration: 9757, Loss:2.83 \n",
      "Iteration: 9758, Loss:2.57 \n",
      "Iteration: 9759, Loss:2.14 \n",
      "Iteration: 9760, Loss:2.60 \n",
      "Iteration: 9761, Loss:2.28 \n",
      "Iteration: 9762, Loss:2.69 \n",
      "Iteration: 9763, Loss:2.59 \n",
      "Iteration: 9764, Loss:2.62 \n",
      "Iteration: 9765, Loss:2.80 \n",
      "Iteration: 9766, Loss:2.70 \n",
      "Iteration: 9767, Loss:2.65 \n",
      "Iteration: 9768, Loss:2.54 \n",
      "Iteration: 9769, Loss:2.81 \n",
      "Iteration: 9770, Loss:2.55 \n",
      "Iteration: 9771, Loss:2.84 \n",
      "Iteration: 9772, Loss:2.75 \n",
      "Iteration: 9773, Loss:2.29 \n",
      "Iteration: 9774, Loss:2.61 \n",
      "Iteration: 9775, Loss:2.34 \n",
      "Iteration: 9776, Loss:2.06 \n",
      "Iteration: 9777, Loss:2.63 \n",
      "Iteration: 9778, Loss:2.22 \n",
      "Iteration: 9779, Loss:2.34 \n",
      "Iteration: 9780, Loss:2.47 \n",
      "Iteration: 9781, Loss:2.33 \n",
      "Iteration: 9782, Loss:2.89 \n",
      "Iteration: 9783, Loss:3.00 \n",
      "Iteration: 9784, Loss:2.34 \n",
      "Iteration: 9785, Loss:2.22 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9786, Loss:2.66 \n",
      "Iteration: 9787, Loss:2.34 \n",
      "Iteration: 9788, Loss:2.86 \n",
      "Iteration: 9789, Loss:2.67 \n",
      "Iteration: 9790, Loss:2.13 \n",
      "Iteration: 9791, Loss:2.27 \n",
      "Iteration: 9792, Loss:2.30 \n",
      "Iteration: 9793, Loss:2.68 \n",
      "Iteration: 9794, Loss:3.00 \n",
      "Iteration: 9795, Loss:2.68 \n",
      "Iteration: 9796, Loss:2.52 \n",
      "Iteration: 9797, Loss:2.28 \n",
      "Iteration: 9798, Loss:2.47 \n",
      "Iteration: 9799, Loss:2.59 \n",
      "Iteration: 9800, Loss:2.39 \n",
      "Iteration: 9801, Loss:2.63 \n",
      "Iteration: 9802, Loss:2.57 \n",
      "Iteration: 9803, Loss:2.77 \n",
      "Iteration: 9804, Loss:2.61 \n",
      "Iteration: 9805, Loss:2.61 \n",
      "Iteration: 9806, Loss:2.68 \n",
      "Iteration: 9807, Loss:2.77 \n",
      "Iteration: 9808, Loss:2.23 \n",
      "Iteration: 9809, Loss:2.60 \n",
      "Iteration: 9810, Loss:2.48 \n",
      "Iteration: 9811, Loss:2.79 \n",
      "Iteration: 9812, Loss:2.40 \n",
      "Iteration: 9813, Loss:2.51 \n",
      "Iteration: 9814, Loss:2.86 \n",
      "Iteration: 9815, Loss:2.35 \n",
      "Iteration: 9816, Loss:2.48 \n",
      "Iteration: 9817, Loss:2.46 \n",
      "Iteration: 9818, Loss:2.30 \n",
      "Iteration: 9819, Loss:2.55 \n",
      "Iteration: 9820, Loss:2.71 \n",
      "Iteration: 9821, Loss:2.58 \n",
      "Iteration: 9822, Loss:2.68 \n",
      "Iteration: 9823, Loss:2.22 \n",
      "Iteration: 9824, Loss:2.57 \n",
      "Iteration: 9825, Loss:2.30 \n",
      "Iteration: 9826, Loss:2.20 \n",
      "Iteration: 9827, Loss:2.50 \n",
      "Iteration: 9828, Loss:2.43 \n",
      "Iteration: 9829, Loss:2.81 \n",
      "Iteration: 9830, Loss:2.33 \n",
      "Iteration: 9831, Loss:2.40 \n",
      "Iteration: 9832, Loss:2.36 \n",
      "Iteration: 9833, Loss:2.68 \n",
      "Iteration: 9834, Loss:2.61 \n",
      "Iteration: 9835, Loss:2.50 \n",
      "Iteration: 9836, Loss:2.16 \n",
      "Iteration: 9837, Loss:2.90 \n",
      "Iteration: 9838, Loss:3.07 \n",
      "Iteration: 9839, Loss:2.27 \n",
      "Iteration: 9840, Loss:2.38 \n",
      "Iteration: 9841, Loss:2.99 \n",
      "Iteration: 9842, Loss:2.31 \n",
      "Iteration: 9843, Loss:2.87 \n",
      "Iteration: 9844, Loss:2.51 \n",
      "Iteration: 9845, Loss:2.79 \n",
      "Iteration: 9846, Loss:2.33 \n",
      "Iteration: 9847, Loss:2.69 \n",
      "Iteration: 9848, Loss:2.85 \n",
      "Iteration: 9849, Loss:2.56 \n",
      "Iteration: 9850, Loss:2.63 \n",
      "Iteration: 9851, Loss:2.62 \n",
      "Iteration: 9852, Loss:2.13 \n",
      "Iteration: 9853, Loss:2.46 \n",
      "Iteration: 9854, Loss:2.78 \n",
      "Iteration: 9855, Loss:2.54 \n",
      "Iteration: 9856, Loss:2.63 \n",
      "Iteration: 9857, Loss:2.47 \n",
      "Iteration: 9858, Loss:2.73 \n",
      "Iteration: 9859, Loss:2.79 \n",
      "Iteration: 9860, Loss:2.48 \n",
      "Iteration: 9861, Loss:2.24 \n",
      "Iteration: 9862, Loss:3.06 \n",
      "Iteration: 9863, Loss:2.68 \n",
      "Iteration: 9864, Loss:2.11 \n",
      "Iteration: 9865, Loss:2.73 \n",
      "Iteration: 9866, Loss:2.39 \n",
      "Iteration: 9867, Loss:2.91 \n",
      "Iteration: 9868, Loss:2.57 \n",
      "Iteration: 9869, Loss:2.92 \n",
      "Iteration: 9870, Loss:2.73 \n",
      "Iteration: 9871, Loss:2.32 \n",
      "Iteration: 9872, Loss:2.64 \n",
      "Iteration: 9873, Loss:2.42 \n",
      "Iteration: 9874, Loss:2.54 \n",
      "Iteration: 9875, Loss:2.59 \n",
      "Iteration: 9876, Loss:2.33 \n",
      "Iteration: 9877, Loss:2.11 \n",
      "Iteration: 9878, Loss:2.67 \n",
      "Iteration: 9879, Loss:2.78 \n",
      "Iteration: 9880, Loss:2.24 \n",
      "Iteration: 9881, Loss:2.70 \n",
      "Iteration: 9882, Loss:2.24 \n",
      "Iteration: 9883, Loss:2.61 \n",
      "Iteration: 9884, Loss:2.50 \n",
      "Iteration: 9885, Loss:2.67 \n",
      "Iteration: 9886, Loss:2.38 \n",
      "Iteration: 9887, Loss:2.67 \n",
      "Iteration: 9888, Loss:2.29 \n",
      "Iteration: 9889, Loss:2.30 \n",
      "Iteration: 9890, Loss:2.62 \n",
      "Iteration: 9891, Loss:2.71 \n",
      "Iteration: 9892, Loss:2.90 \n",
      "Iteration: 9893, Loss:2.20 \n",
      "Iteration: 9894, Loss:2.65 \n",
      "Iteration: 9895, Loss:2.61 \n",
      "Iteration: 9896, Loss:2.35 \n",
      "Iteration: 9897, Loss:2.50 \n",
      "Iteration: 9898, Loss:2.04 \n",
      "Iteration: 9899, Loss:2.49 \n",
      "Iteration: 9900, Loss:2.44 \n",
      "Iteration: 9901, Loss:2.55 \n",
      "Iteration: 9902, Loss:2.30 \n",
      "Iteration: 9903, Loss:2.37 \n",
      "Iteration: 9904, Loss:2.44 \n",
      "Iteration: 9905, Loss:2.58 \n",
      "Iteration: 9906, Loss:2.45 \n",
      "Iteration: 9907, Loss:2.85 \n",
      "Iteration: 9908, Loss:2.51 \n",
      "Iteration: 9909, Loss:2.44 \n",
      "Iteration: 9910, Loss:2.73 \n",
      "Iteration: 9911, Loss:2.37 \n",
      "Iteration: 9912, Loss:2.71 \n",
      "Iteration: 9913, Loss:2.72 \n",
      "Iteration: 9914, Loss:2.83 \n",
      "Iteration: 9915, Loss:2.71 \n",
      "Iteration: 9916, Loss:2.62 \n",
      "Iteration: 9917, Loss:2.57 \n",
      "Iteration: 9918, Loss:2.54 \n",
      "Iteration: 9919, Loss:2.47 \n",
      "Iteration: 9920, Loss:2.36 \n",
      "Iteration: 9921, Loss:2.60 \n",
      "Iteration: 9922, Loss:2.26 \n",
      "Iteration: 9923, Loss:2.44 \n",
      "Iteration: 9924, Loss:2.61 \n",
      "Iteration: 9925, Loss:2.56 \n",
      "Iteration: 9926, Loss:2.27 \n",
      "Iteration: 9927, Loss:2.37 \n",
      "Iteration: 9928, Loss:2.60 \n",
      "Iteration: 9929, Loss:2.57 \n",
      "Iteration: 9930, Loss:2.58 \n",
      "Iteration: 9931, Loss:2.63 \n",
      "Iteration: 9932, Loss:2.59 \n",
      "Iteration: 9933, Loss:2.09 \n",
      "Iteration: 9934, Loss:2.46 \n",
      "Iteration: 9935, Loss:2.36 \n",
      "Iteration: 9936, Loss:2.47 \n",
      "Iteration: 9937, Loss:2.91 \n",
      "Iteration: 9938, Loss:2.54 \n",
      "Iteration: 9939, Loss:2.53 \n",
      "Iteration: 9940, Loss:2.60 \n",
      "Iteration: 9941, Loss:2.70 \n",
      "Iteration: 9942, Loss:2.57 \n",
      "Iteration: 9943, Loss:2.29 \n",
      "Iteration: 9944, Loss:2.56 \n",
      "Iteration: 9945, Loss:2.88 \n",
      "Iteration: 9946, Loss:2.73 \n",
      "Iteration: 9947, Loss:2.82 \n",
      "Iteration: 9948, Loss:2.62 \n",
      "Iteration: 9949, Loss:2.76 \n",
      "Iteration: 9950, Loss:2.34 \n",
      "Iteration: 9951, Loss:2.64 \n",
      "Iteration: 9952, Loss:2.57 \n",
      "Iteration: 9953, Loss:2.52 \n",
      "Iteration: 9954, Loss:2.66 \n",
      "Iteration: 9955, Loss:2.44 \n",
      "Iteration: 9956, Loss:2.23 \n",
      "Iteration: 9957, Loss:2.40 \n",
      "Iteration: 9958, Loss:2.50 \n",
      "Iteration: 9959, Loss:2.86 \n",
      "Iteration: 9960, Loss:2.74 \n",
      "Iteration: 9961, Loss:2.09 \n",
      "Iteration: 9962, Loss:2.69 \n",
      "Iteration: 9963, Loss:2.46 \n",
      "Iteration: 9964, Loss:2.27 \n",
      "Iteration: 9965, Loss:2.62 \n",
      "Iteration: 9966, Loss:2.29 \n",
      "Iteration: 9967, Loss:2.56 \n",
      "Iteration: 9968, Loss:2.79 \n",
      "Iteration: 9969, Loss:2.70 \n",
      "Iteration: 9970, Loss:2.76 \n",
      "Iteration: 9971, Loss:2.67 \n",
      "Iteration: 9972, Loss:2.16 \n",
      "Iteration: 9973, Loss:2.73 \n",
      "Iteration: 9974, Loss:2.17 \n",
      "Iteration: 9975, Loss:2.48 \n",
      "Iteration: 9976, Loss:2.78 \n",
      "Iteration: 9977, Loss:2.50 \n",
      "Iteration: 9978, Loss:2.69 \n",
      "Iteration: 9979, Loss:2.78 \n",
      "Iteration: 9980, Loss:2.46 \n",
      "Iteration: 9981, Loss:2.43 \n",
      "Iteration: 9982, Loss:2.75 \n",
      "Iteration: 9983, Loss:2.70 \n",
      "Iteration: 9984, Loss:2.38 \n",
      "Iteration: 9985, Loss:2.41 \n",
      "Iteration: 9986, Loss:2.29 \n",
      "Iteration: 9987, Loss:2.90 \n",
      "Iteration: 9988, Loss:2.73 \n",
      "Iteration: 9989, Loss:2.73 \n",
      "Iteration: 9990, Loss:2.76 \n",
      "Iteration: 9991, Loss:2.49 \n",
      "Iteration: 9992, Loss:2.57 \n",
      "Iteration: 9993, Loss:2.66 \n",
      "Iteration: 9994, Loss:2.83 \n",
      "Iteration: 9995, Loss:2.66 \n",
      "Iteration: 9996, Loss:2.51 \n",
      "Iteration: 9997, Loss:2.74 \n",
      "Iteration: 9998, Loss:2.29 \n",
      "Iteration: 9999, Loss:2.80 \n",
      "Iteration: 10000, Loss:2.39 \n",
      "Iteration: 10001, Loss:2.41 \n",
      "C:\\Users\\kkb6\\Desktop\\PlayListCurator\\model-training\\checkpoints\\run-1\\run-PlayList-Curator_weights_10000.ckpt\n"
     ]
    }
   ],
   "source": [
    "valid_iters = 100\n",
    "for _ in range(10000):\n",
    "    i += 1\n",
    "    log = i%3 == 0\n",
    "    is_valid = i%100 == 0 or i == 20\n",
    "    \n",
    "    x, labs = a_train.__next__()\n",
    "    x = Variable(torch.from_numpy(x))\n",
    "    labs = Variable(torch.from_numpy(labs))\n",
    "    labs = labs.long()\n",
    "    \n",
    "    output  = net(x)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = criterion(output, labs)\n",
    "    log_value('training_loss',loss,i)\n",
    "    \n",
    "    print(\"Iteration: {}, Loss:{:.2f} \".format(i + 1,loss)) \n",
    "    \n",
    "    loss.backward()\n",
    " \n",
    "    optimizer.step()\n",
    "        \n",
    "    if is_valid:\n",
    "        losses = []\n",
    "        for __ in range(valid_iters):\n",
    "            x, labs = a_valid.__next__()\n",
    "            x = Variable(torch.from_numpy(x))\n",
    "            labs= Variable(torch.from_numpy(labs))\n",
    "            labs = labs.long()\n",
    "            \n",
    "            output  = net(x)\n",
    "            v_loss = criterion(output, labs).detach()\n",
    "    \n",
    "            losses.append(v_loss)\n",
    "\n",
    "        valid_loss = np.mean(losses)\n",
    "        log_value('valid_loss', valid_loss, i)\n",
    "              \n",
    "        if valid_loss < best_valid_loss:\n",
    "            torch.save(net.state_dict(), os.path.join(checkpointdir, tag + '_weights_%d.ckpt'%i))\n",
    "            print(os.path.join(checkpointdir, tag + '_weights_%d.ckpt'%i))\n",
    "            best_valid_loss = valid_loss\n",
    "       \n",
    "    if i % 250 == 0:\n",
    "        torch.save(net.state_dict(), os.path.join(checkpointdir, tag + '_weights_%d.ckpt'%i))\n",
    "        print(os.path.join(checkpointdir, tag + '_weights_%d.ckpt'%i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
